{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded large dataset with 60000 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>generation</th>\n",
       "      <th>is_ai</th>\n",
       "      <th>source_type</th>\n",
       "      <th>n_tokens_ws</th>\n",
       "      <th>n_chars</th>\n",
       "      <th>length_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Simultaneous segmentation of multiple organs f...</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "      <td>271</td>\n",
       "      <td>1816</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10</td>\n",
       "      <td>9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10</td>\n",
       "      <td>9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>A Variational Image Segmentation Model based o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is a fundamental research t...</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "      <td>271</td>\n",
       "      <td>1892</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38d4f731-4259-4770-9669-255b61bf61b2</td>\n",
       "      <td>38d4f731-4259-4770-9669-255b61bf61b2</td>\n",
       "      <td>38d4f731-4259-4770-9669-255b61bf61b2</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Computing Valid p-values for Image Segmentatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Image segmentation is one of the most fundamen...</td>\n",
       "      <td>False</td>\n",
       "      <td>human</td>\n",
       "      <td>183</td>\n",
       "      <td>1283</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "1  9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10  9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10   \n",
       "2  38d4f731-4259-4770-9669-255b61bf61b2  38d4f731-4259-4770-9669-255b61bf61b2   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human      NaN                NaN   \n",
       "1  9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10  human      NaN                NaN   \n",
       "2  38d4f731-4259-4770-9669-255b61bf61b2  human      NaN                NaN   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "1   none  abstracts  A Variational Image Segmentation Model based o...    NaN   \n",
       "2   none  abstracts  Computing Valid p-values for Image Segmentatio...    NaN   \n",
       "\n",
       "                                          generation  is_ai source_type  \\\n",
       "0  Simultaneous segmentation of multiple organs f...  False       human   \n",
       "1  Image segmentation is a fundamental research t...  False       human   \n",
       "2  Image segmentation is one of the most fundamen...  False       human   \n",
       "\n",
       "   n_tokens_ws  n_chars length_bin  \n",
       "0          271     1816       long  \n",
       "1          271     1892       long  \n",
       "2          183     1283     medium  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuration and dataset selection\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths to the three samples\n",
    "SAMPLE_PATHS = {\n",
    "    \"small\":  \"raid_sample_small.csv\",\n",
    "    \"medium\": \"raid_sample_medium.csv\",\n",
    "    \"large\":  \"raid_sample_large.csv\",\n",
    "}\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Set this to one of: \"small\", \"medium\", \"large\"\n",
    "SELECTED_DATASET = \"large\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "TEXT_COL = \"generation\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATHS[SELECTED_DATASET])\n",
    "print(f\"Loaded {SELECTED_DATASET} dataset with {len(df)} rows.\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6afd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy, CMUdict (NLTK), and g2p_en fallback\n",
    "\n",
    "# Ensure CMUdict is available\n",
    "nltk.download('cmudict', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "CMU = cmudict.dict()  # key: lowercase word, value: list of pronunciations (list of ARPAbet tokens)\n",
    "\n",
    "# Load spaCy English model (use 'en_core_web_sm' unless you already have md/lg installed)\n",
    "try:\n",
    "    nlp: Language = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure sentence boundaries are available (parser usually handles this; add sentencizer if needed)\n",
    "if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# g2p_en for OOV coverage\n",
    "G2P = G2p()\n",
    "\n",
    "# ARPAbet vowel bases (stress digits removed when checking)\n",
    "ARPA_VOWELS = {\n",
    "    \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
    "    \"EH\", \"ER\", \"EY\",\n",
    "    \"IH\", \"IY\",\n",
    "    \"OW\", \"OY\",\n",
    "    \"UH\", \"UW\"\n",
    "}\n",
    "\n",
    "# Cache syllable counts for speed\n",
    "_SYLL_CACHE: Dict[str, int] = {}\n",
    "\n",
    "def cmu_syllables(word: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Returns syllable count using CMUdict if available; else None.\n",
    "    Policy: use the first pronunciation variant.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w not in CMU:\n",
    "        return None\n",
    "    phones = CMU[w][0]\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    return max(count, 1)  # at least one for non-empty alphabetic words\n",
    "\n",
    "def g2p_syllables(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns syllable count using neural g2p_en; counts vowel phonemes.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w in _SYLL_CACHE:\n",
    "        return _SYLL_CACHE[w]\n",
    "    phones = G2P(w)\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    # Guard: ensure >=1 for alphabetic tokens\n",
    "    if count == 0 and re.search(r\"[A-Za-z]\", w):\n",
    "        count = 1\n",
    "    _SYLL_CACHE[w] = count\n",
    "    return count\n",
    "\n",
    "def syllables_hybrid(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Hybrid policy: try CMUdict first; if OOV, fall back to g2p_en.\n",
    "    \"\"\"\n",
    "    c = cmu_syllables(word)\n",
    "    if c is not None:\n",
    "        return c\n",
    "    return g2p_syllables(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3020ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation utilities using spaCy + CMUdict with g2p_en fallback\n",
    "\n",
    "def _word_like(tok) -> bool:\n",
    "    \"\"\"\n",
    "    Select lexical tokens (alphabetic, not space).\n",
    "    spaCy's tok.is_alpha ensures letter-only tokens; change if you want alphanumerics.\n",
    "    \"\"\"\n",
    "    return tok.is_alpha and not tok.is_space\n",
    "\n",
    "def _alnum_char_count(token_text: str) -> int:\n",
    "    \"\"\"Count alphanumeric characters for ARI; excludes whitespace and punctuation.\"\"\"\n",
    "    return sum(ch.isalnum() for ch in token_text)\n",
    "\n",
    "def features_from_doc(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - avg_word_length\n",
    "      - type_token_ratio\n",
    "      - stopword_ratio\n",
    "      - punctuation_ratio       (punct chars / non-space chars)\n",
    "      - avg_sentence_length     (words per sentence)\n",
    "      - sentence_length_std     (std of sentence word counts)\n",
    "      - flesch_reading_ease\n",
    "      - gunning_fog\n",
    "      - smog_index\n",
    "      - automated_readability_index\n",
    "    \"\"\"\n",
    "    # Sentences\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    n_sents = max(len(sents), 1)\n",
    "\n",
    "    # Token groups\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    punct_toks = [t for t in doc if t.is_punct]\n",
    "    nonspace_toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "    W = len(word_toks)\n",
    "\n",
    "    # Characters for ARI and punctuation ratio\n",
    "    chars_alnum = sum(_alnum_char_count(t.text) for t in nonspace_toks)\n",
    "    punct_chars = sum(len(t.text) for t in punct_toks)\n",
    "    nonspace_chars = sum(len(t.text) for t in nonspace_toks)\n",
    "\n",
    "    # Sentence-level word counts\n",
    "    sent_word_counts = [sum(1 for t in s if _word_like(t)) for s in sents]\n",
    "    avg_sentence_length = float(np.mean(sent_word_counts)) if sent_word_counts else 0.0\n",
    "    sentence_length_std  = float(np.std(sent_word_counts, ddof=0)) if len(sent_word_counts) > 1 else 0.0\n",
    "\n",
    "    # Word-level lengths\n",
    "    word_lengths = [len(t.text) for t in word_toks]\n",
    "    avg_word_length = float(np.mean(word_lengths)) if word_lengths else 0.0\n",
    "\n",
    "    # Type-token ratio (lowercased forms)\n",
    "    vocab = {t.text.lower() for t in word_toks}\n",
    "    type_token_ratio = (len(vocab) / W) if W > 0 else 0.0\n",
    "\n",
    "    # Stopword ratio via spaCy stop flags\n",
    "    stop_count = sum(1 for t in word_toks if t.is_stop)\n",
    "    stopword_ratio = (stop_count / W) if W > 0 else 0.0\n",
    "\n",
    "    # Punctuation ratio over non-space characters\n",
    "    punctuation_ratio = (punct_chars / nonspace_chars) if nonspace_chars > 0 else 0.0\n",
    "\n",
    "    # Syllables (hybrid)\n",
    "    syll_per_word = [syllables_hybrid(t.text) for t in word_toks] if W > 0 else []\n",
    "    syll_total = int(np.sum(syll_per_word)) if syll_per_word else 0\n",
    "    polysyllables = int(np.sum([syl >= 3 for syl in syll_per_word])) if syll_per_word else 0\n",
    "    complex_words = polysyllables  # standard: >= 3 syllables\n",
    "\n",
    "    # Rates for readability\n",
    "    words_per_sentence = (W / n_sents) if n_sents > 0 else 0.0\n",
    "    syllables_per_word = (syll_total / W) if W > 0 else 0.0\n",
    "    chars_per_word_ari = (chars_alnum / W) if W > 0 else 0.0\n",
    "\n",
    "    # Readability indices\n",
    "    # Flesch Reading Ease\n",
    "    flesch = 206.835 - 1.015 * words_per_sentence - 84.6 * syllables_per_word\n",
    "\n",
    "    # Gunning Fog\n",
    "    fog = 0.4 * (words_per_sentence + 100.0 * (complex_words / W if W > 0 else 0.0))\n",
    "\n",
    "    # SMOG\n",
    "    smog = (1.043 * math.sqrt(polysyllables * (30.0 / n_sents)) + 3.1291) if (polysyllables > 0 and n_sents > 0) else 0.0\n",
    "\n",
    "    # Automated Readability Index\n",
    "    ari = 4.71 * chars_per_word_ari + 0.5 * words_per_sentence - 21.43\n",
    "\n",
    "    return {\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"punctuation_ratio\": punctuation_ratio,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"sentence_length_std\": sentence_length_std,\n",
    "        \"flesch_reading_ease\": flesch,\n",
    "        \"gunning_fog\": fog,\n",
    "        \"smog_index\": smog,\n",
    "        \"automated_readability_index\": ari,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa1f44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature columns:\n",
      "['avg_word_length', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>stopword_ratio</th>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <th>sentence_length_std</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>fbc8a5ea-90fa-47b8-8fa7-73dd954f1524</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Combo Loss: Handling Input and Output Imbalanc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.434783</td>\n",
       "      <td>0.576087</td>\n",
       "      <td>0.387681</td>\n",
       "      <td>0.023933</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>7.337575</td>\n",
       "      <td>21.881870</td>\n",
       "      <td>20.750145</td>\n",
       "      <td>17.916177</td>\n",
       "      <td>18.053152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10</td>\n",
       "      <td>9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10</td>\n",
       "      <td>9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>A Variational Image Segmentation Model based o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.691756</td>\n",
       "      <td>0.512545</td>\n",
       "      <td>0.415771</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>19.928571</td>\n",
       "      <td>7.075756</td>\n",
       "      <td>18.317177</td>\n",
       "      <td>18.294009</td>\n",
       "      <td>16.084391</td>\n",
       "      <td>15.342458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38d4f731-4259-4770-9669-255b61bf61b2</td>\n",
       "      <td>38d4f731-4259-4770-9669-255b61bf61b2</td>\n",
       "      <td>38d4f731-4259-4770-9669-255b61bf61b2</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>abstracts</td>\n",
       "      <td>Computing Valid p-values for Image Segmentatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.739362</td>\n",
       "      <td>0.542553</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.019982</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>12.882500</td>\n",
       "      <td>21.314894</td>\n",
       "      <td>18.243606</td>\n",
       "      <td>17.352394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524   \n",
       "1  9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10  9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10   \n",
       "2  38d4f731-4259-4770-9669-255b61bf61b2  38d4f731-4259-4770-9669-255b61bf61b2   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  fbc8a5ea-90fa-47b8-8fa7-73dd954f1524  human      NaN                NaN   \n",
       "1  9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10  human      NaN                NaN   \n",
       "2  38d4f731-4259-4770-9669-255b61bf61b2  human      NaN                NaN   \n",
       "\n",
       "  attack     domain                                              title prompt  \\\n",
       "0   none  abstracts  Combo Loss: Handling Input and Output Imbalanc...    NaN   \n",
       "1   none  abstracts  A Variational Image Segmentation Model based o...    NaN   \n",
       "2   none  abstracts  Computing Valid p-values for Image Segmentatio...    NaN   \n",
       "\n",
       "   ... avg_word_length  type_token_ratio stopword_ratio  punctuation_ratio  \\\n",
       "0  ...        5.434783          0.576087       0.387681           0.023933   \n",
       "1  ...        5.691756          0.512545       0.415771           0.020962   \n",
       "2  ...        5.739362          0.542553       0.446809           0.019982   \n",
       "\n",
       "   avg_sentence_length sentence_length_std  flesch_reading_ease  gunning_fog  \\\n",
       "0            27.600000            7.337575            21.881870    20.750145   \n",
       "1            19.928571            7.075756            18.317177    18.294009   \n",
       "2            23.500000            9.000000            12.882500    21.314894   \n",
       "\n",
       "   smog_index  automated_readability_index  \n",
       "0   17.916177                    18.053152  \n",
       "1   16.084391                    15.342458  \n",
       "2   18.243606                    17.352394  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application NLP.pipe based\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# 1 for deterministic ordering in some environments; -1  all available cores\n",
    "N_PROCESS = -1\n",
    "\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "feature_rows: List[Dict[str, float]] = []\n",
    "for doc in nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESS):\n",
    "    feature_rows.append(features_from_doc(doc))\n",
    "\n",
    "feat_df = pd.DataFrame(feature_rows)\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Computed feature columns:\")\n",
    "print(list(feat_df.columns))\n",
    "df_with_features.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01c5cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched dataset: raid_sample_large_with_features_PREPOS.csv  (rows: 60000)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_word_length</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>4.395837</td>\n",
       "      <td>1.548118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.586753</td>\n",
       "      <td>4.679701</td>\n",
       "      <td>5.567568</td>\n",
       "      <td>103.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.543659</td>\n",
       "      <td>0.237435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.081917</td>\n",
       "      <td>0.570597</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopword_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.401944</td>\n",
       "      <td>0.205058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.462687</td>\n",
       "      <td>0.604297</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>0.018124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005291</td>\n",
       "      <td>0.026568</td>\n",
       "      <td>0.046552</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>22.572604</td>\n",
       "      <td>28.120557</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.666667</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>510.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_length_std</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>9.283595</td>\n",
       "      <td>13.115418</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.819091</td>\n",
       "      <td>15.881937</td>\n",
       "      <td>507.187233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>63.494831</td>\n",
       "      <td>55.253984</td>\n",
       "      <td>-451.815</td>\n",
       "      <td>19.530901</td>\n",
       "      <td>59.350730</td>\n",
       "      <td>94.516103</td>\n",
       "      <td>206.835000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>13.960141</td>\n",
       "      <td>12.196909</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.152914</td>\n",
       "      <td>12.755556</td>\n",
       "      <td>20.907391</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>11.330193</td>\n",
       "      <td>5.831059</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.604125</td>\n",
       "      <td>11.491704</td>\n",
       "      <td>17.122413</td>\n",
       "      <td>81.249730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>11.222168</td>\n",
       "      <td>28.318284</td>\n",
       "      <td>-21.430</td>\n",
       "      <td>2.223114</td>\n",
       "      <td>10.911729</td>\n",
       "      <td>19.190989</td>\n",
       "      <td>4543.060000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count       mean        std      min  \\\n",
       "avg_word_length              60000.0   4.395837   1.548118    0.000   \n",
       "type_token_ratio             60000.0   0.543659   0.237435    0.000   \n",
       "stopword_ratio               60000.0   0.401944   0.205058    0.000   \n",
       "punctuation_ratio            60000.0   0.027598   0.018124    0.000   \n",
       "avg_sentence_length          60000.0  22.572604  28.120557    0.000   \n",
       "sentence_length_std          60000.0   9.283595  13.115418    0.000   \n",
       "flesch_reading_ease          60000.0  63.494831  55.253984 -451.815   \n",
       "gunning_fog                  60000.0  13.960141  12.196909    0.000   \n",
       "smog_index                   60000.0  11.330193   5.831059    0.000   \n",
       "automated_readability_index  60000.0  11.222168  28.318284  -21.430   \n",
       "\n",
       "                                   10%        50%        90%          max  \n",
       "avg_word_length               3.586753   4.679701   5.567568   103.444444  \n",
       "type_token_ratio              0.081917   0.570597   0.807692     1.000000  \n",
       "stopword_ratio                0.000000   0.462687   0.604297     1.000000  \n",
       "punctuation_ratio             0.005291   0.026568   0.046552     1.000000  \n",
       "avg_sentence_length           8.666667  19.166667  30.500000   510.000000  \n",
       "sentence_length_std           0.000000   6.819091  15.881937   507.187233  \n",
       "flesch_reading_ease          19.530901  59.350730  94.516103   206.835000  \n",
       "gunning_fog                   5.152914  12.755556  20.907391   204.000000  \n",
       "smog_index                    4.604125  11.491704  17.122413    81.249730  \n",
       "automated_readability_index   2.223114  10.911729  19.190989  4543.060000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5: Save enriched dataset and basic descriptive statistics\n",
    "\n",
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_PREPOS.csv\"\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS}  (rows: {len(df_with_features)})\")\n",
    "\n",
    "cols_to_describe = [\n",
    "    \"avg_word_length\",\n",
    "    \"type_token_ratio\",\n",
    "    \"stopword_ratio\",\n",
    "    \"punctuation_ratio\",\n",
    "    \"avg_sentence_length\",\n",
    "    \"sentence_length_std\",\n",
    "    \"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\n",
    "    \"smog_index\",\n",
    "    \"automated_readability_index\",\n",
    "]\n",
    "display(df_with_features[cols_to_describe].describe(percentiles=[0.1, 0.5, 0.9]).T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
