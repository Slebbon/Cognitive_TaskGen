{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded large dataset with 60000 rows.\n",
      "df_columns: Index(['id', 'adv_source_id', 'source_id', 'model', 'decoding',\n",
      "       'repetition_penalty', 'attack', 'domain', 'title', 'prompt',\n",
      "       'generation', 'is_ai', 'source_type', 'generation_raw', 'had_urls',\n",
      "       'had_html', 'had_code', 'had_table', 'n_chars', 'n_tok', 'alpha_ratio',\n",
      "       'digit_ratio', 'punct_ratio', 'avg_word_length', 'std_word_length',\n",
      "       'entropy_bits', 'entropy_norm', 'is_text_like', 'not_text_reason',\n",
      "       'n_tokens_ws', 'length_bin'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>std_word_length</th>\n",
       "      <th>entropy_bits</th>\n",
       "      <th>entropy_norm</th>\n",
       "      <th>is_text_like</th>\n",
       "      <th>not_text_reason</th>\n",
       "      <th>n_tokens_ws</th>\n",
       "      <th>length_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a274811-4809-4a88-96a2-9af63c06bae9</td>\n",
       "      <td>e4f35f18-1a33-4cb3-bf63-d4402ded0dbc</td>\n",
       "      <td>79c87b70-fb77-4a80-881a-2a77db2b17bc</td>\n",
       "      <td>mistral</td>\n",
       "      <td>greedy</td>\n",
       "      <td>no</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>books</td>\n",
       "      <td>Each Man's Son</td>\n",
       "      <td>The following is the full text of a plot summa...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023848</td>\n",
       "      <td>0.035230</td>\n",
       "      <td>4.400621</td>\n",
       "      <td>2.422283</td>\n",
       "      <td>4.525679</td>\n",
       "      <td>0.782805</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>322</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3273ddc0-2211-4116-90bb-1ce99bf01e6c</td>\n",
       "      <td>2d7d0873-d4f7-47d1-b971-cbdaff88d5bc</td>\n",
       "      <td>2d7d0873-d4f7-47d1-b971-cbdaff88d5bc</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_deletion</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Salad of Roasted Beets and Arugula with Blue C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015239</td>\n",
       "      <td>0.034833</td>\n",
       "      <td>4.523404</td>\n",
       "      <td>2.000661</td>\n",
       "      <td>4.417504</td>\n",
       "      <td>0.782710</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46c83315-fb96-4c22-91ac-b77e7edb42bc</td>\n",
       "      <td>4f1cf565-8c27-4c20-ac22-824146821c03</td>\n",
       "      <td>143a1df5-a01a-476d-a3fb-c59f2ec78fff</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>greedy</td>\n",
       "      <td>no</td>\n",
       "      <td>whitespace</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Guiltfree Chocolate Cheesecake</td>\n",
       "      <td>The following is the full text of a recipe for...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045175</td>\n",
       "      <td>0.055441</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.089545</td>\n",
       "      <td>4.555896</td>\n",
       "      <td>0.824811</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  8a274811-4809-4a88-96a2-9af63c06bae9  e4f35f18-1a33-4cb3-bf63-d4402ded0dbc   \n",
       "1  3273ddc0-2211-4116-90bb-1ce99bf01e6c  2d7d0873-d4f7-47d1-b971-cbdaff88d5bc   \n",
       "2  46c83315-fb96-4c22-91ac-b77e7edb42bc  4f1cf565-8c27-4c20-ac22-824146821c03   \n",
       "\n",
       "                              source_id    model decoding repetition_penalty  \\\n",
       "0  79c87b70-fb77-4a80-881a-2a77db2b17bc  mistral   greedy                 no   \n",
       "1  2d7d0873-d4f7-47d1-b971-cbdaff88d5bc    human      NaN                NaN   \n",
       "2  143a1df5-a01a-476d-a3fb-c59f2ec78fff     gpt3   greedy                 no   \n",
       "\n",
       "             attack   domain  \\\n",
       "0  zero_width_space    books   \n",
       "1  article_deletion  recipes   \n",
       "2        whitespace  recipes   \n",
       "\n",
       "                                               title  \\\n",
       "0                                     Each Man's Son   \n",
       "1  Salad of Roasted Beets and Arugula with Blue C...   \n",
       "2                    Guiltfree Chocolate Cheesecake    \n",
       "\n",
       "                                              prompt  ... digit_ratio  \\\n",
       "0  The following is the full text of a plot summa...  ...    0.023848   \n",
       "1                                                NaN  ...    0.015239   \n",
       "2  The following is the full text of a recipe for...  ...    0.045175   \n",
       "\n",
       "   punct_ratio avg_word_length std_word_length  entropy_bits  entropy_norm  \\\n",
       "0     0.035230        4.400621        2.422283      4.525679      0.782805   \n",
       "1     0.034833        4.523404        2.000661      4.417504      0.782710   \n",
       "2     0.055441        5.000000        2.089545      4.555896      0.824811   \n",
       "\n",
       "   is_text_like  not_text_reason  n_tokens_ws  length_bin  \n",
       "0          True              NaN          322        long  \n",
       "1          True              NaN          248        long  \n",
       "2          True              NaN           84       short  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuration and dataset selection\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths to the three samples\n",
    "SAMPLE_PATHS = {\n",
    "    \"small\":  \"raid_sample_small.csv\",\n",
    "    \"medium\": \"raid_sample_medium.csv\",\n",
    "    \"large\":  \"raid_sample_large.csv\",\n",
    "}\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Set this to one of: \"small\", \"medium\", \"large\"\n",
    "SELECTED_DATASET = \"large\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "TEXT_COL = \"generation\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATHS[SELECTED_DATASET])\n",
    "print(f\"Loaded {SELECTED_DATASET} dataset with {len(df)} rows.\")\n",
    "print(f\"df_columns: {df.columns}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6afd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy, CMUdict (NLTK), and g2p_en fallback\n",
    "\n",
    "# Ensure CMUdict is available\n",
    "nltk.download('cmudict', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "CMU = cmudict.dict()  # key: lowercase word, value: list of pronunciations (list of ARPAbet tokens)\n",
    "\n",
    "# Load spaCy English model (use 'en_core_web_sm' unless you already have md/lg installed)\n",
    "try:\n",
    "    nlp: Language = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure sentence boundaries are available (parser usually handles this; add sentencizer if needed)\n",
    "if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# g2p_en for OOV coverage\n",
    "G2P = G2p()\n",
    "\n",
    "# ARPAbet vowel bases (stress digits removed when checking)\n",
    "ARPA_VOWELS = {\n",
    "    \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
    "    \"EH\", \"ER\", \"EY\",\n",
    "    \"IH\", \"IY\",\n",
    "    \"OW\", \"OY\",\n",
    "    \"UH\", \"UW\"\n",
    "}\n",
    "\n",
    "# Cache syllable counts for speed\n",
    "_SYLL_CACHE: Dict[str, int] = {}\n",
    "\n",
    "def cmu_syllables(word: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Returns syllable count using CMUdict if available; else None.\n",
    "    Policy: use the first pronunciation variant.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w not in CMU:\n",
    "        return None\n",
    "    phones = CMU[w][0]\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    return max(count, 1)  # at least one for non-empty alphabetic words\n",
    "\n",
    "def g2p_syllables(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns syllable count using neural g2p_en; counts vowel phonemes.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w in _SYLL_CACHE:\n",
    "        return _SYLL_CACHE[w]\n",
    "    phones = G2P(w)\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    # Guard: ensure >=1 for alphabetic tokens\n",
    "    if count == 0 and re.search(r\"[A-Za-z]\", w):\n",
    "        count = 1\n",
    "    _SYLL_CACHE[w] = count\n",
    "    return count\n",
    "\n",
    "def syllables_hybrid(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Hybrid policy: try CMUdict first; if OOV, fall back to g2p_en.\n",
    "    \"\"\"\n",
    "    c = cmu_syllables(word)\n",
    "    if c is not None:\n",
    "        return c\n",
    "    return g2p_syllables(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3020ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation utilities using spaCy + CMUdict with g2p_en fallback\n",
    "\n",
    "import zlib\n",
    "\n",
    "# permissive fallback segmentation for pathological cases\n",
    "_FALLBACK_SPLIT = re.compile(r'(?<=[\\.!?])\\s+|[\\r\\n]+|(?<=;)\\s+|(?<=:)\\s+')\n",
    "\n",
    "def resegmentize_if_needed(text: str, nlp: Language, asl_hi: int = 100, min_tokens: int = 120, doc=None):\n",
    "    \"\"\"Return fallback sentence strings when avg sentence length is extreme.\"\"\"\n",
    "    doc = doc or nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    n_sent = len(sents)\n",
    "    n_tok = len(doc)\n",
    "    avg_len = (n_tok / max(n_sent, 1)) if n_tok else 0\n",
    "\n",
    "    if (n_tok >= min_tokens) and (n_sent <= 2 or avg_len >= asl_hi):\n",
    "        parts = [s.strip() for s in _FALLBACK_SPLIT.split(text) if s and not s.isspace()]\n",
    "        return parts\n",
    "    return None\n",
    "\n",
    "def prepare_doc(text: str, nlp: Language, skip_reseg: bool = False) -> Tuple[Doc, bool]:\n",
    "    \"\"\"\n",
    "    Parse text with spaCy and optionally resegment when heuristics trigger.\n",
    "    Set skip_reseg=True when you need accurate dependency trees.\n",
    "    \"\"\"\n",
    "    primary_doc = nlp(text)\n",
    "    \n",
    "    if skip_reseg:\n",
    "        return primary_doc, False\n",
    "    \n",
    "    fallback = resegmentize_if_needed(text, nlp, doc=primary_doc)\n",
    "    if fallback is None:\n",
    "        return primary_doc, False\n",
    "    \n",
    "    fixed_text = \". \".join(fallback)\n",
    "    return nlp(fixed_text), True\n",
    "\n",
    "def can_compute_readability(n_tokens: int, n_sents: int) -> bool:\n",
    "    return (n_tokens >= 100) and (n_sents >= 3)\n",
    "\n",
    "def safe_readability(tokens: int, sentences: int, syllable_counts: List[int], chars_per_word: float, complex_words: int, polysyllables: int) -> Dict[str, float]:\n",
    "    \"\"\"Safely compute readability metrics, falling back to NaN when unstable.\"\"\"\n",
    "    out = {\n",
    "        \"flesch_reading_ease\": np.nan,\n",
    "        \"gunning_fog\": np.nan,\n",
    "        \"smog_index\": np.nan,\n",
    "        \"automated_readability_index\": np.nan,\n",
    "    }\n",
    "    if not can_compute_readability(tokens, sentences):\n",
    "        return out\n",
    "\n",
    "    words = max(tokens, 1)\n",
    "    sents = max(sentences, 1)\n",
    "    syllables = max(int(np.sum(syllable_counts)), 1)\n",
    "    chars_per_word = float(chars_per_word) if chars_per_word else 0.0\n",
    "    complex_words = max(complex_words, 0)\n",
    "    polysyllables = max(polysyllables, 0)\n",
    "\n",
    "    out[\"flesch_reading_ease\"] = 206.835 - 1.015 * (words / sents) - 84.6 * (syllables / words)\n",
    "    out[\"gunning_fog\"] = 0.4 * ((words / sents) + 100.0 * (complex_words / words))\n",
    "    out[\"smog_index\"] = (1.043 * math.sqrt(30.0 * (polysyllables / sents)) + 3.1291) if polysyllables > 0 else np.nan\n",
    "    out[\"automated_readability_index\"] = 4.71 * chars_per_word + 0.5 * (words / sents) - 21.43\n",
    "    return out\n",
    "\n",
    "def _word_like(tok) -> bool:\n",
    "    \"\"\"Select lexical tokens (alphabetic, not space).\"\"\"\n",
    "    return tok.is_alpha and not tok.is_space\n",
    "\n",
    "def _alnum_char_count(token_text: str) -> int:\n",
    "    \"\"\"Count alphanumeric characters for ARI; excludes whitespace and punctuation.\"\"\"\n",
    "    return sum(ch.isalnum() for ch in token_text)\n",
    "\n",
    "def features_from_doc(doc: Doc, text: str, *, resegmented: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"Compute core lexical and readability-driven metrics with guards.\"\"\"\n",
    "    if doc.has_annotation(\"SENT_START\"):\n",
    "        sents = list(doc.sents)\n",
    "    else:\n",
    "        sents = list(doc.sents) if hasattr(doc, \"sents\") else [doc]\n",
    "    if not sents:\n",
    "        sents = [doc]\n",
    "    n_sents = len(sents)\n",
    "\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    punct_toks = [t for t in doc if t.is_punct]\n",
    "    nonspace_toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "    n_tokens = len(word_toks)\n",
    "    sent_word_counts = [sum(1 for t in sent if _word_like(t)) for sent in sents]\n",
    "    avg_sentence_length = float(np.mean(sent_word_counts)) if sent_word_counts else np.nan\n",
    "    sentence_length_std = float(np.std(sent_word_counts, ddof=0)) if len(sent_word_counts) > 1 else np.nan\n",
    "\n",
    "    word_lengths = [len(t.text) for t in word_toks] \n",
    "    avg_word_length = float(np.mean(word_lengths)) if word_lengths else np.nan\n",
    "\n",
    "    vocab = {t.text.lower() for t in word_toks}\n",
    "    type_token_ratio = (len(vocab) / n_tokens) if n_tokens > 0 else np.nan\n",
    "\n",
    "    stop_count = sum(1 for t in word_toks if t.is_stop)\n",
    "    stopword_ratio = (stop_count / n_tokens) if n_tokens > 0 else np.nan\n",
    "\n",
    "    chars_alnum = sum(_alnum_char_count(t.text) for t in nonspace_toks)\n",
    "    punct_chars = sum(len(t.text) for t in punct_toks)\n",
    "    nonspace_chars = sum(len(t.text) for t in nonspace_toks)\n",
    "    punctuation_ratio = (punct_chars / nonspace_chars) if nonspace_chars > 0 else np.nan\n",
    "\n",
    "    syllable_counts = [syllables_hybrid(t.text) for t in word_toks]\n",
    "    polysyllables = sum(1 for syl in syllable_counts if syl >= 3)\n",
    "    complex_words = polysyllables\n",
    "    chars_per_word = (chars_alnum / n_tokens) if n_tokens > 0 else 0.0\n",
    "\n",
    "    readability = safe_readability(\n",
    "        tokens=n_tokens,\n",
    "        sentences=n_sents,\n",
    "        syllable_counts=syllable_counts,\n",
    "        chars_per_word=chars_per_word,\n",
    "        complex_words=complex_words,\n",
    "        polysyllables=polysyllables,\n",
    "    )\n",
    "\n",
    "    features = {\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"punctuation_ratio\": punctuation_ratio,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"sentence_length_std\": sentence_length_std,\n",
    "        \"n_tokens_doc\": float(n_tokens),\n",
    "        \"n_sentences_doc\": float(n_sents),\n",
    "        \"resegmented\": bool(resegmented),\n",
    "    }\n",
    "    features.update(readability)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74acf093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram and burstiness utility functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# N-gram feature extraction utilities\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def ngram_diversity(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate n-gram diversity (unique n-grams / total n-grams).\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def ngram_entropy(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of n-gram distribution.\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_burstiness(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate burstiness coefficient based on word frequency distribution.\n",
    "    Burstiness = (sigma - mu) / (sigma + mu)\n",
    "    where mu is mean frequency and sigma is standard deviation.\n",
    "    Returns NaN if statistics are undefined.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "\n",
    "    word_counts = Counter(tokens)\n",
    "    frequencies = list(word_counts.values())\n",
    "\n",
    "    if len(frequencies) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(frequencies)\n",
    "    sigma = np.std(frequencies, ddof=0)\n",
    "\n",
    "    if mu + sigma == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return (sigma - mu) / (sigma + mu)\n",
    "\n",
    "def safe_ngram_stats(tokens: List[str], n: int = 2, min_ngrams: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Return diversity and entropy for n-grams when sample size is sufficient.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return {\"diversity\": np.nan, \"entropy\": np.nan}\n",
    "\n",
    "    grams = extract_ngrams(tokens, n)\n",
    "    if len(grams) < min_ngrams:\n",
    "        return {\"diversity\": np.nan, \"entropy\": np.nan}\n",
    "\n",
    "    counts = Counter(grams)\n",
    "    diversity = len(counts) / len(grams)\n",
    "    probs = np.array(list(counts.values()), dtype=float) / len(grams)\n",
    "    entropy = float(-np.sum(probs * np.log2(probs)))\n",
    "    return {\"diversity\": diversity, \"entropy\": entropy}\n",
    "\n",
    "print(\"N-gram and burstiness utility functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9638c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Character-level feature extraction\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def character_ngram_features(text: str, n: int = 3) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract character n-gram diversity and entropy.\n",
    "    Returns diversity ratio and entropy for character n-grams.\n",
    "    \"\"\"\n",
    "    if len(text) < n:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    char_ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "    if not char_ngrams:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    diversity = len(set(char_ngrams)) / len(char_ngrams)\n",
    "\n",
    "    counts = Counter(char_ngrams)\n",
    "    total = len(char_ngrams)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "\n",
    "    return diversity, entropy\n",
    "\n",
    "def compression_features(text: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute compression ratio and bits-per-character using zlib.\"\"\"\n",
    "    encoded = text.encode(\"utf-8\")\n",
    "    raw_len = len(encoded)\n",
    "    if raw_len == 0:\n",
    "        return {\"compression_ratio\": np.nan, \"bits_per_char\": np.nan}\n",
    "\n",
    "    compressed = zlib.compress(encoded, level=6)\n",
    "    ratio = len(compressed) / raw_len\n",
    "    bits_per_char = 8.0 * len(compressed) / raw_len\n",
    "    return {\"compression_ratio\": ratio, \"bits_per_char\": bits_per_char}\n",
    "\n",
    "def compression_ratio(text: str) -> float:\n",
    "    \"\"\"Backward-compatible helper returning only the compression ratio.\"\"\"\n",
    "    return compression_features(text)[\"compression_ratio\"]\n",
    "\n",
    "def character_statistics(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract surface-level character statistics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            \"uppercase_ratio\": np.nan,\n",
    "            \"digit_ratio\": np.nan,\n",
    "            \"whitespace_ratio\": np.nan,\n",
    "            \"unique_char_count\": np.nan,\n",
    "        }\n",
    "\n",
    "    total_chars = len(text)\n",
    "\n",
    "    return {\n",
    "        \"uppercase_ratio\": sum(1 for c in text if c.isupper()) / total_chars,\n",
    "        \"digit_ratio\": sum(1 for c in text if c.isdigit()) / total_chars,\n",
    "        \"whitespace_ratio\": sum(1 for c in text if c.isspace()) / total_chars,\n",
    "        \"unique_char_count\": float(len(set(text))),\n",
    "    }\n",
    "\n",
    "print(\"Character-level feature functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74b9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc, Token\n",
    "\n",
    "def _root_chain_depth(token: Token, max_steps: int) -> int:\n",
    "    \"\"\"\n",
    "    Length of the head chain from `token` up to the sentence/doc root.\n",
    "    Robust to cycles and malformed heads by bounding steps and tracking indices.\n",
    "    \"\"\"\n",
    "    depth = 0\n",
    "    cur = token\n",
    "    visited_idx = set()\n",
    "\n",
    "    # Use token indices (doc-relative) for identity; this is stable.\n",
    "    while cur.head.i != cur.i:\n",
    "        if cur.i in visited_idx:\n",
    "            # Cycle detected: bail out with 0 depth for this token\n",
    "            return 0\n",
    "        visited_idx.add(cur.i)\n",
    "\n",
    "        depth += 1\n",
    "        if depth > max_steps:\n",
    "            # Malformed graph (excessive chain): cap and exit\n",
    "            return max_steps\n",
    "\n",
    "        cur = cur.head\n",
    "    return depth\n",
    "\n",
    "def sent_max_depth(sent) -> int:\n",
    "    \"\"\"\n",
    "    Longest head-chain root->leaf depth within a sentence span.\n",
    "    Traversal is over the full doc-level heads; we only *measure* per sentence.\n",
    "    \"\"\"\n",
    "    if len(sent) == 0:\n",
    "        return 0\n",
    "\n",
    "    # A conservative upper bound: number of tokens in the *doc*\n",
    "    # (not just in the sentence), to safely handle cross-sentence heads.\n",
    "    max_steps = len(sent.doc) + 5\n",
    "\n",
    "    return max((_root_chain_depth(tok, max_steps) for tok in sent), default=0)\n",
    "\n",
    "def doc_depth_stats(doc: Doc) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Average and maximum tree depth across sentences in the original Doc.\n",
    "    Falls back gracefully for empty docs.\n",
    "    \"\"\"\n",
    "    sentences = list(doc.sents) if doc.has_annotation(\"SENT_START\") or hasattr(doc, \"sents\") else [doc]\n",
    "    if not sentences:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    depths = [sent_max_depth(sent) for sent in sentences if len(sent) > 0]\n",
    "    if not depths:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    return float(np.mean(depths)), float(np.max(depths))\n",
    "\n",
    "def dependency_tree_features(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract robust dependency-tree structural features from the original Doc.\n",
    "    Avoids Span.as_doc(), uses doc-level indices and bounds traversal.\n",
    "    \"\"\"\n",
    "    sentences = list(doc.sents) if doc.has_annotation(\"SENT_START\") or hasattr(doc, \"sents\") else [doc]\n",
    "    if not sentences:\n",
    "        sentences = [doc]\n",
    "\n",
    "    avg_depth, max_depth = doc_depth_stats(doc)\n",
    "\n",
    "    per_sentence_distances = []\n",
    "    left_deps = 0\n",
    "    right_deps = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        distances = []\n",
    "        for token in sent:\n",
    "            # Skip roots\n",
    "            if token.head.i == token.i:\n",
    "                continue\n",
    "            # Distance computed in doc coordinates is fine:\n",
    "            d = abs(token.i - token.head.i)\n",
    "            distances.append(d)\n",
    "            if token.i < token.head.i:\n",
    "                left_deps += 1\n",
    "            else:\n",
    "                right_deps += 1\n",
    "        if distances:\n",
    "            per_sentence_distances.append(float(np.mean(distances)))\n",
    "\n",
    "    avg_dep_distance = float(np.mean(per_sentence_distances)) if per_sentence_distances else 0.0\n",
    "    total_deps = left_deps + right_deps\n",
    "    left_ratio = (left_deps / total_deps) if total_deps else 0.0\n",
    "    right_ratio = (right_deps / total_deps) if total_deps else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_tree_depth\": avg_depth,\n",
    "        \"max_tree_depth\": max_depth,\n",
    "        \"avg_dependency_distance\": avg_dep_distance,\n",
    "        \"left_dependency_ratio\": left_ratio,\n",
    "        \"right_dependency_ratio\": right_ratio,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89de6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Optional\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# Token normalization helpers\n",
    "# ---------------------------\n",
    "\n",
    "def _normalize_tokens(\n",
    "    tokens: Iterable,\n",
    "    *,\n",
    "    lower: bool = True,\n",
    "    alpha_only: bool = True,\n",
    "    min_len: int = 2,\n",
    "    use_lemma: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize a sequence of tokens (strings or spaCy tokens).\n",
    "    - lower: lowercases\n",
    "    - alpha_only: keep tokens with .isalpha() (falls back to regex if string)\n",
    "    - min_len: drop tokens shorter than this after normalization\n",
    "    - use_lemma: if spaCy tokens are provided, use token.lemma_\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        s = t\n",
    "        # spaCy Token support\n",
    "        if hasattr(t, \"lemma_\") and use_lemma:\n",
    "            s = t.lemma_\n",
    "        elif hasattr(t, \"text\"):\n",
    "            s = t.text\n",
    "\n",
    "        if not isinstance(s, str):\n",
    "            s = str(s)\n",
    "\n",
    "        if lower:\n",
    "            s = s.lower()\n",
    "\n",
    "        if alpha_only:\n",
    "            # use spaCy attribute if available, else a regex fallback\n",
    "            if hasattr(t, \"is_alpha\"):\n",
    "                if not t.is_alpha:\n",
    "                    continue\n",
    "            else:\n",
    "                if not re.match(r\"^[a-zA-Z]+$\", s):\n",
    "                    continue\n",
    "\n",
    "        if len(s) < min_len:\n",
    "            continue\n",
    "\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Hapax ratio (stable variant)\n",
    "# ---------------------------\n",
    "\n",
    "def hapax_legomena_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Ratio of hapax tokens to TOTAL TOKENS (your original definition).\n",
    "    More common in lexicography is hapax / types; we keep your denominator,\n",
    "    but you may prefer hapax / unique_types for length-robustness.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(tokens)\n",
    "    hapax = sum(1 for c in cnt.values() if c == 1)\n",
    "    return hapax / float(len(tokens))\n",
    "\n",
    "\n",
    "def hapax_type_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Optional: hapax to TYPES ratio (often more stable than hapax/token).\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(tokens)\n",
    "    types = len(cnt)\n",
    "    if types == 0:\n",
    "        return float(\"nan\")\n",
    "    hapax = sum(1 for c in cnt.values() if c == 1)\n",
    "    return hapax / float(types)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Yule's K (guarded)\n",
    "# ---------------------------\n",
    "\n",
    "def yules_k(tokens: List[str], *, min_tokens: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Yule's K = 10^4 * (sum v^2 * V_v - N) / N^2\n",
    "    Guarded against short texts; returns NaN if N < min_tokens.\n",
    "    \"\"\"\n",
    "    N = len(tokens)\n",
    "    if N < min_tokens:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    cnt = Counter(tokens)\n",
    "    spectrum = Counter(cnt.values())  # V_v\n",
    "    # sum v^2 * V_v\n",
    "    s2 = sum((v * v) * Vv for v, Vv in spectrum.items())\n",
    "\n",
    "    # use float64\n",
    "    Nf = float(N)\n",
    "    K = 10000.0 * (s2 - Nf) / (Nf * Nf)\n",
    "    return float(K)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MTLD (standard, robust)\n",
    "# ---------------------------\n",
    "\n",
    "def _mtld_one_pass(tokens: List[str], threshold: float, min_segment: int) -> float:\n",
    "    \"\"\"\n",
    "    One-direction MTLD pass (standard algorithm):\n",
    "    Accumulate a segment until TTR falls below threshold; count a factor and reset.\n",
    "    The final partial segment contributes a fractional factor.\n",
    "    Returns the number of factors observed.\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    token_count = 0\n",
    "    factor_count = 0.0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / float(token_count)\n",
    "\n",
    "        # Only allow a factor to close if we have a minimally meaningful segment\n",
    "        if (ttr < threshold) and (token_count >= min_segment):\n",
    "            factor_count += 1.0\n",
    "            types.clear()\n",
    "            token_count = 0\n",
    "\n",
    "    # partial segment contribution\n",
    "    if token_count > 0:\n",
    "        # If ttr is already above threshold, this adds <1 factor,\n",
    "        # otherwise adds a smaller fraction.\n",
    "        ttr = len(types) / float(token_count)\n",
    "        if ttr != 1.0:  # avoid division by zero in degenerate case\n",
    "            factor_count += (1.0 - ttr) / (1.0 - threshold)\n",
    "        else:\n",
    "            # maximally diverse partial segment: count a tiny fraction\n",
    "            factor_count += 0.0\n",
    "\n",
    "    return factor_count\n",
    "\n",
    "\n",
    "def mtld(\n",
    "    tokens: List[str],\n",
    "    threshold: float = 0.72,\n",
    "    *,\n",
    "    min_tokens: int = 200,\n",
    "    min_segment: int = 50\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Measure of Textual Lexical Diversity (MTLD), forward/backward average.\n",
    "    Guard-rails:\n",
    "      - require at least `min_tokens` tokens, else NaN\n",
    "      - clamp threshold to [0.60, 0.80]\n",
    "      - enforce `min_segment` for factor completion\n",
    "    \"\"\"\n",
    "    n = len(tokens)\n",
    "    if n < min_tokens:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    threshold = max(0.60, min(0.80, float(threshold)))\n",
    "\n",
    "    f = _mtld_one_pass(tokens, threshold, min_segment)\n",
    "    b = _mtld_one_pass(list(reversed(tokens)), threshold, min_segment)\n",
    "\n",
    "    # If both are zero (pathological), return NaN rather than n/0\n",
    "    vals = [x for x in (f, b) if x > 0.0]\n",
    "    if not vals:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    mean_factors = float(np.mean(vals))\n",
    "    return n / mean_factors\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregator with normalization\n",
    "# ---------------------------\n",
    "\n",
    "def vocabulary_sophistication_features(\n",
    "    tokens: List,\n",
    "    *,\n",
    "    normalize: str = \"lower\",   # {\"none\",\"lower\",\"lemma\"}\n",
    "    alpha_only: bool = True,\n",
    "    min_len: int = 2,\n",
    "    use_lemma_if_spacy: Optional[bool] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - Accepts raw strings or spaCy Tokens.\n",
    "    - Normalizes then computes robust metrics with NaN for undersized texts.\n",
    "    - Adds hapax/type as a more length-stable complement (does not replace your original).\n",
    "    \"\"\"\n",
    "    if normalize not in {\"none\", \"lower\", \"lemma\"}:\n",
    "        raise ValueError(\"normalize must be one of {'none','lower','lemma'}\")\n",
    "\n",
    "    if use_lemma_if_spacy is None:\n",
    "        use_lemma_if_spacy = (normalize == \"lemma\")\n",
    "\n",
    "    toks = _normalize_tokens(\n",
    "        tokens,\n",
    "        lower=(normalize == \"lower\"),\n",
    "        alpha_only=alpha_only,\n",
    "        min_len=min_len,\n",
    "        use_lemma=use_lemma_if_spacy\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"hapax_legomena_ratio\": hapax_legomena_ratio(toks),\n",
    "        \"hapax_type_ratio\":     hapax_type_ratio(toks),\n",
    "        \"yules_k\":              yules_k(toks, min_tokens=100),\n",
    "        \"mtld\":                 mtld(toks, threshold=0.72, min_tokens=200, min_segment=50),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dfb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation pattern functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Punctuation pattern analysis\n",
    "\n",
    "def punctuation_patterns(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detailed punctuation pattern features beyond simple ratio.\n",
    "    \"\"\"\n",
    "    all_tokens = [t for t in doc if not t.is_space]\n",
    "    punct_tokens = [t for t in doc if t.is_punct]\n",
    "    \n",
    "    if not all_tokens:\n",
    "        return {\n",
    "            \"comma_ratio\": 0.0,\n",
    "            \"period_ratio\": 0.0,\n",
    "            \"question_ratio\": 0.0,\n",
    "            \"exclamation_ratio\": 0.0,\n",
    "            \"semicolon_ratio\": 0.0,\n",
    "            \"colon_ratio\": 0.0,\n",
    "            \"quote_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total = len(all_tokens)\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punct_text = ''.join([t.text for t in punct_tokens])\n",
    "    \n",
    "    return {\n",
    "        \"comma_ratio\": punct_text.count(',') / total,\n",
    "        \"period_ratio\": punct_text.count('.') / total,\n",
    "        \"question_ratio\": punct_text.count('?') / total,\n",
    "        \"exclamation_ratio\": punct_text.count('!') / total,\n",
    "        \"semicolon_ratio\": punct_text.count(';') / total,\n",
    "        \"colon_ratio\": punct_text.count(':') / total,\n",
    "        \"quote_ratio\": (punct_text.count('\"') + punct_text.count(\"'\")) / total,\n",
    "    }\n",
    "\n",
    "print(\"Punctuation pattern functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_features(text: str, doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract sentiment and emotional tone features.\n",
    "    Uses TextBlob for polarity and subjectivity.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"sentiment_subjectivity\": 0.0,\n",
    "            \"sentiment_polarity_variance\": 0.0,\n",
    "            \"positive_word_ratio\": 0.0,\n",
    "            \"negative_word_ratio\": 0.0,\n",
    "            \"neutral_sentence_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Overall document sentiment\n",
    "    blob = TextBlob(text)\n",
    "    features = {\n",
    "        \"sentiment_polarity\": blob.sentiment.polarity,  # -1 (negative) to 1 (positive)\n",
    "        \"sentiment_subjectivity\": blob.sentiment.subjectivity,  # 0 (objective) to 1 (subjective)\n",
    "    }\n",
    "    \n",
    "    # Sentence-level sentiment variance\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    sent_polarities = []\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_blob = TextBlob(sent.text)\n",
    "        polarity = sent_blob.sentiment.polarity\n",
    "        sent_polarities.append(polarity)\n",
    "        \n",
    "        # Count neutral sentences (polarity close to 0)\n",
    "        if abs(polarity) < 0.1:\n",
    "            neutral_count += 1\n",
    "    \n",
    "    features[\"sentiment_polarity_variance\"] = float(np.var(sent_polarities)) if len(sent_polarities) > 1 else 0.0\n",
    "    features[\"neutral_sentence_ratio\"] = neutral_count / len(sents) if sents else 0.0\n",
    "    \n",
    "    # Positive/negative word ratios using spaCy tokens\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    if word_toks:\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for token in word_toks:\n",
    "            word_blob = TextBlob(token.text.lower())\n",
    "            polarity = word_blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                positive_count += 1\n",
    "            elif polarity < -0.1:\n",
    "                negative_count += 1\n",
    "        \n",
    "        features[\"positive_word_ratio\"] = positive_count / len(word_toks)\n",
    "        features[\"negative_word_ratio\"] = negative_count / len(word_toks)\n",
    "    else:\n",
    "        features[\"positive_word_ratio\"] = 0.0\n",
    "        features[\"negative_word_ratio\"] = 0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1f44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature columns:\n",
      "['avg_word_length', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'digit_ratio', 'whitespace_ratio', 'unique_char_count', 'compression_ratio', 'bits_per_char', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'hapax_type_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio', 'had_urls', 'had_html', 'had_code', 'had_table']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <th>had_urls</th>\n",
       "      <th>had_html</th>\n",
       "      <th>had_code</th>\n",
       "      <th>had_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a274811-4809-4a88-96a2-9af63c06bae9</td>\n",
       "      <td>e4f35f18-1a33-4cb3-bf63-d4402ded0dbc</td>\n",
       "      <td>79c87b70-fb77-4a80-881a-2a77db2b17bc</td>\n",
       "      <td>mistral</td>\n",
       "      <td>greedy</td>\n",
       "      <td>no</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>books</td>\n",
       "      <td>Each Man's Son</td>\n",
       "      <td>The following is the full text of a plot summa...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.101742</td>\n",
       "      <td>0.260947</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3273ddc0-2211-4116-90bb-1ce99bf01e6c</td>\n",
       "      <td>2d7d0873-d4f7-47d1-b971-cbdaff88d5bc</td>\n",
       "      <td>2d7d0873-d4f7-47d1-b971-cbdaff88d5bc</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article_deletion</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Salad of Roasted Beets and Arugula with Blue C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.048611</td>\n",
       "      <td>0.367063</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.042735</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46c83315-fb96-4c22-91ac-b77e7edb42bc</td>\n",
       "      <td>4f1cf565-8c27-4c20-ac22-824146821c03</td>\n",
       "      <td>143a1df5-a01a-476d-a3fb-c59f2ec78fff</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>greedy</td>\n",
       "      <td>no</td>\n",
       "      <td>whitespace</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Guiltfree Chocolate Cheesecake</td>\n",
       "      <td>The following is the full text of a recipe for...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132857</td>\n",
       "      <td>0.355714</td>\n",
       "      <td>0.024519</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.070423</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  8a274811-4809-4a88-96a2-9af63c06bae9  e4f35f18-1a33-4cb3-bf63-d4402ded0dbc   \n",
       "1  3273ddc0-2211-4116-90bb-1ce99bf01e6c  2d7d0873-d4f7-47d1-b971-cbdaff88d5bc   \n",
       "2  46c83315-fb96-4c22-91ac-b77e7edb42bc  4f1cf565-8c27-4c20-ac22-824146821c03   \n",
       "\n",
       "                              source_id    model decoding repetition_penalty  \\\n",
       "0  79c87b70-fb77-4a80-881a-2a77db2b17bc  mistral   greedy                 no   \n",
       "1  2d7d0873-d4f7-47d1-b971-cbdaff88d5bc    human      NaN                NaN   \n",
       "2  143a1df5-a01a-476d-a3fb-c59f2ec78fff     gpt3   greedy                 no   \n",
       "\n",
       "             attack   domain  \\\n",
       "0  zero_width_space    books   \n",
       "1  article_deletion  recipes   \n",
       "2        whitespace  recipes   \n",
       "\n",
       "                                               title  \\\n",
       "0                                     Each Man's Son   \n",
       "1  Salad of Roasted Beets and Arugula with Blue C...   \n",
       "2                    Guiltfree Chocolate Cheesecake    \n",
       "\n",
       "                                              prompt  ... sentiment_polarity  \\\n",
       "0  The following is the full text of a plot summa...  ...           0.101742   \n",
       "1                                                NaN  ...           0.048611   \n",
       "2  The following is the full text of a recipe for...  ...           0.132857   \n",
       "\n",
       "   sentiment_subjectivity sentiment_polarity_variance neutral_sentence_ratio  \\\n",
       "0                0.260947                    0.008746               0.538462   \n",
       "1                0.367063                    0.013839               0.545455   \n",
       "2                0.355714                    0.024519               0.727273   \n",
       "\n",
       "   positive_word_ratio  negative_word_ratio  had_urls  had_html  had_code  \\\n",
       "0             0.035714             0.012987     False     False     False   \n",
       "1             0.042735             0.038462     False     False     False   \n",
       "2             0.070423             0.028169     False     False     False   \n",
       "\n",
       "   had_table  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "\n",
       "[3 rows x 85 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application doc processing with segmentation safeguards\n",
    "\n",
    "def window_tokens(doc: Doc, max_tokens: int = 500) -> List[str]:\n",
    "    \"\"\"Lowercase alphabetic tokens clipped to a comparison window.\"\"\"\n",
    "    return [t.text.lower() for t in doc if _word_like(t)][:max_tokens]\n",
    "\n",
    "feature_rows: List[Dict[str, float]] = []\n",
    "for idx, row in df.reset_index(drop=True).iterrows():\n",
    "    text = str(row.get(TEXT_COL, \"\"))\n",
    "    \n",
    "    # Use resegmented doc for readability and most features\n",
    "    doc, resegmented = prepare_doc(text, nlp)\n",
    "    doc_features = dict(features_from_doc(doc, text, resegmented=resegmented))\n",
    "\n",
    "    tok_win = window_tokens(doc, max_tokens=500)\n",
    "    unigram = safe_ngram_stats(tok_win, n=1, min_ngrams=100)\n",
    "    bigram = safe_ngram_stats(tok_win, n=2, min_ngrams=100)\n",
    "    trigram = safe_ngram_stats(tok_win, n=3, min_ngrams=100)\n",
    "    doc_features[\"unigram_diversity\"] = unigram[\"diversity\"]\n",
    "    doc_features[\"bigram_diversity\"] = bigram[\"diversity\"]\n",
    "    doc_features[\"trigram_diversity\"] = trigram[\"diversity\"]\n",
    "    doc_features[\"bigram_entropy\"] = bigram[\"entropy\"]\n",
    "    doc_features[\"trigram_entropy\"] = trigram[\"entropy\"]\n",
    "    doc_features[\"token_burstiness\"] = calculate_burstiness(tok_win) if len(tok_win) >= 2 else np.nan\n",
    "\n",
    "    char_diversity, char_entropy = character_ngram_features(text, n=3)\n",
    "    doc_features[\"char_trigram_diversity\"] = char_diversity\n",
    "    doc_features[\"char_trigram_entropy\"] = char_entropy\n",
    "    doc_features.update(character_statistics(text))\n",
    "    doc_features.update(compression_features(text))\n",
    "\n",
    "    # CRITICAL: Use clean parse for dependency features to avoid resegmentation artifacts\n",
    "    doc_clean, _ = prepare_doc(text, nlp, skip_reseg=True)\n",
    "    dep_feats = dependency_tree_features(doc_clean)\n",
    "    doc_features.update(dep_feats)\n",
    "    max_tree_depth = dep_feats.get(\"max_tree_depth\")\n",
    "    try:\n",
    "        depth_nan = math.isnan(max_tree_depth)\n",
    "    except (TypeError, ValueError):\n",
    "        depth_nan = False\n",
    "    depth_ok = depth_nan or max_tree_depth is None\n",
    "    if not depth_ok:\n",
    "        try:\n",
    "            depth_ok = max_tree_depth <= 50\n",
    "        except TypeError:\n",
    "            depth_ok = False\n",
    "    doc_features[\"depth_check_passed\"] = bool(depth_ok)\n",
    "\n",
    "    vocab_feats = vocabulary_sophistication_features(tok_win)\n",
    "    doc_features.update(vocab_feats)\n",
    "\n",
    "    doc_features.update(punctuation_patterns(doc))\n",
    "    doc_features.update(sentiment_features(text, doc))\n",
    "\n",
    "    for flag in [\"had_urls\", \"had_html\", \"had_code\", \"had_table\"]:\n",
    "        if flag in row.index:\n",
    "            doc_features[flag] = row[flag]\n",
    "\n",
    "    feature_rows.append(doc_features)\n",
    "\n",
    "feat_df = pd.DataFrame(feature_rows)\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Computed feature columns:\")\n",
    "print(list(feat_df.columns))\n",
    "df_with_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c5cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched dataset: raid_sample_large_with_features_PREPOS.csv  (rows: 60000)\n",
      "Feature columns saved (48 total):\n",
      "['type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'whitespace_ratio', 'unique_char_count', 'compression_ratio', 'bits_per_char', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'hapax_type_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>59999.0</td>\n",
       "      <td>0.587220</td>\n",
       "      <td>0.143939</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.449405</td>\n",
       "      <td>0.580556</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopword_ratio</th>\n",
       "      <td>59999.0</td>\n",
       "      <td>0.442409</td>\n",
       "      <td>0.165985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.231809</td>\n",
       "      <td>0.474320</td>\n",
       "      <td>0.608303</td>\n",
       "      <td>0.989848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.029878</td>\n",
       "      <td>0.012764</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.028169</td>\n",
       "      <td>0.045172</td>\n",
       "      <td>0.313095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>22.519281</td>\n",
       "      <td>20.385584</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.596875</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>465.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_length_std</th>\n",
       "      <td>59337.0</td>\n",
       "      <td>10.254223</td>\n",
       "      <td>10.978179</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.192881</td>\n",
       "      <td>7.925103</td>\n",
       "      <td>16.102420</td>\n",
       "      <td>233.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_doc</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>254.759100</td>\n",
       "      <td>242.829985</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>111.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>12615.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_sentences_doc</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>13.666100</td>\n",
       "      <td>12.448411</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>805.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>55123.0</td>\n",
       "      <td>54.750313</td>\n",
       "      <td>24.305058</td>\n",
       "      <td>-167.910708</td>\n",
       "      <td>21.496652</td>\n",
       "      <td>58.315472</td>\n",
       "      <td>82.195726</td>\n",
       "      <td>194.240294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>55123.0</td>\n",
       "      <td>13.681865</td>\n",
       "      <td>5.659952</td>\n",
       "      <td>1.152766</td>\n",
       "      <td>7.406159</td>\n",
       "      <td>13.087584</td>\n",
       "      <td>20.405993</td>\n",
       "      <td>69.365657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>54886.0</td>\n",
       "      <td>12.165220</td>\n",
       "      <td>3.967437</td>\n",
       "      <td>3.666510</td>\n",
       "      <td>7.447530</td>\n",
       "      <td>11.855464</td>\n",
       "      <td>17.122413</td>\n",
       "      <td>49.068485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>55123.0</td>\n",
       "      <td>11.853153</td>\n",
       "      <td>6.126844</td>\n",
       "      <td>-9.797680</td>\n",
       "      <td>5.741039</td>\n",
       "      <td>11.101261</td>\n",
       "      <td>18.109722</td>\n",
       "      <td>82.172177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_diversity</th>\n",
       "      <td>55878.0</td>\n",
       "      <td>0.577171</td>\n",
       "      <td>0.134185</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.456912</td>\n",
       "      <td>0.573171</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_diversity</th>\n",
       "      <td>55741.0</td>\n",
       "      <td>0.894303</td>\n",
       "      <td>0.135863</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>0.822630</td>\n",
       "      <td>0.924138</td>\n",
       "      <td>0.981675</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_diversity</th>\n",
       "      <td>55638.0</td>\n",
       "      <td>0.952368</td>\n",
       "      <td>0.131545</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.984962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_entropy</th>\n",
       "      <td>55741.0</td>\n",
       "      <td>7.524664</td>\n",
       "      <td>0.802617</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.758752</td>\n",
       "      <td>7.592457</td>\n",
       "      <td>8.442943</td>\n",
       "      <td>8.958888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_entropy</th>\n",
       "      <td>55638.0</td>\n",
       "      <td>7.662145</td>\n",
       "      <td>0.807565</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.857981</td>\n",
       "      <td>7.742541</td>\n",
       "      <td>8.581364</td>\n",
       "      <td>8.960002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_burstiness</th>\n",
       "      <td>59992.0</td>\n",
       "      <td>-0.007655</td>\n",
       "      <td>0.206372</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.233536</td>\n",
       "      <td>0.024921</td>\n",
       "      <td>0.203058</td>\n",
       "      <td>0.755133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_diversity</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.524792</td>\n",
       "      <td>0.125520</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.393808</td>\n",
       "      <td>0.524874</td>\n",
       "      <td>0.668304</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_entropy</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>8.874403</td>\n",
       "      <td>0.649990</td>\n",
       "      <td>2.365083</td>\n",
       "      <td>8.290397</td>\n",
       "      <td>8.954099</td>\n",
       "      <td>9.516813</td>\n",
       "      <td>10.759561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.026932</td>\n",
       "      <td>0.020546</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>0.023746</td>\n",
       "      <td>0.045660</td>\n",
       "      <td>0.801292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.169997</td>\n",
       "      <td>0.017860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145735</td>\n",
       "      <td>0.171109</td>\n",
       "      <td>0.190850</td>\n",
       "      <td>0.331959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_char_count</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>46.596267</td>\n",
       "      <td>10.210872</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>221.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.507723</td>\n",
       "      <td>0.087634</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.438690</td>\n",
       "      <td>0.512996</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bits_per_char</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>4.061782</td>\n",
       "      <td>0.701069</td>\n",
       "      <td>0.136724</td>\n",
       "      <td>3.509517</td>\n",
       "      <td>4.103971</td>\n",
       "      <td>4.684001</td>\n",
       "      <td>10.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_tree_depth</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>5.921419</td>\n",
       "      <td>4.840174</td>\n",
       "      <td>0.287958</td>\n",
       "      <td>3.705882</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>7.777778</td>\n",
       "      <td>367.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tree_depth</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>9.824617</td>\n",
       "      <td>6.573506</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>470.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>3.072595</td>\n",
       "      <td>1.090981</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.549316</td>\n",
       "      <td>2.987275</td>\n",
       "      <td>3.533624</td>\n",
       "      <td>109.344304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left_dependency_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.479068</td>\n",
       "      <td>0.067091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404651</td>\n",
       "      <td>0.473786</td>\n",
       "      <td>0.560570</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_dependency_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.520932</td>\n",
       "      <td>0.067091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.439430</td>\n",
       "      <td>0.526214</td>\n",
       "      <td>0.595349</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_legomena_ratio</th>\n",
       "      <td>58627.0</td>\n",
       "      <td>0.464544</td>\n",
       "      <td>0.190073</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284006</td>\n",
       "      <td>0.438525</td>\n",
       "      <td>0.688537</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_type_ratio</th>\n",
       "      <td>58627.0</td>\n",
       "      <td>0.727213</td>\n",
       "      <td>0.142479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596026</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yules_k</th>\n",
       "      <td>50745.0</td>\n",
       "      <td>158.827099</td>\n",
       "      <td>281.440224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>68.703711</td>\n",
       "      <td>121.875000</td>\n",
       "      <td>215.578911</td>\n",
       "      <td>9979.079498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtld</th>\n",
       "      <td>30217.0</td>\n",
       "      <td>189.156146</td>\n",
       "      <td>1000.621077</td>\n",
       "      <td>32.307692</td>\n",
       "      <td>55.777478</td>\n",
       "      <td>84.844828</td>\n",
       "      <td>154.261576</td>\n",
       "      <td>47528.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comma_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.046260</td>\n",
       "      <td>0.029530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013042</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.078603</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.042645</td>\n",
       "      <td>0.022150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022901</td>\n",
       "      <td>0.040865</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.005138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.263473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003205</td>\n",
       "      <td>0.149701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.001855</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.193370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.006725</td>\n",
       "      <td>0.013694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023148</td>\n",
       "      <td>0.295455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.093540</td>\n",
       "      <td>0.127433</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.030561</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0.248162</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.422977</td>\n",
       "      <td>0.171981</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.598675</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.040739</td>\n",
       "      <td>0.038813</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.031471</td>\n",
       "      <td>0.090292</td>\n",
       "      <td>0.729601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.548710</td>\n",
       "      <td>0.245906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.035182</td>\n",
       "      <td>0.028074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.993763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <td>60000.0</td>\n",
       "      <td>0.016530</td>\n",
       "      <td>0.016713</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014085</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count        mean          std         min  \\\n",
       "type_token_ratio             59999.0    0.587220     0.143939    0.002092   \n",
       "stopword_ratio               59999.0    0.442409     0.165985    0.000000   \n",
       "punctuation_ratio            60000.0    0.029878     0.012764    0.000000   \n",
       "avg_sentence_length          60000.0   22.519281    20.385584    0.000000   \n",
       "sentence_length_std          59337.0   10.254223    10.978179    0.000000   \n",
       "n_tokens_doc                 60000.0  254.759100   242.829985    0.000000   \n",
       "n_sentences_doc              60000.0   13.666100    12.448411    1.000000   \n",
       "flesch_reading_ease          55123.0   54.750313    24.305058 -167.910708   \n",
       "gunning_fog                  55123.0   13.681865     5.659952    1.152766   \n",
       "smog_index                   54886.0   12.165220     3.967437    3.666510   \n",
       "automated_readability_index  55123.0   11.853153     6.126844   -9.797680   \n",
       "unigram_diversity            55878.0    0.577171     0.134185    0.002092   \n",
       "bigram_diversity             55741.0    0.894303     0.135863    0.002096   \n",
       "trigram_diversity            55638.0    0.952368     0.131545    0.002101   \n",
       "bigram_entropy               55741.0    7.524664     0.802617   -0.000000   \n",
       "trigram_entropy              55638.0    7.662145     0.807565   -0.000000   \n",
       "token_burstiness             59992.0   -0.007655     0.206372   -1.000000   \n",
       "char_trigram_diversity       60000.0    0.524792     0.125520    0.003956   \n",
       "char_trigram_entropy         60000.0    8.874403     0.649990    2.365083   \n",
       "uppercase_ratio              60000.0    0.026932     0.020546    0.000000   \n",
       "whitespace_ratio             60000.0    0.169997     0.017860    0.000000   \n",
       "unique_char_count            60000.0   46.596267    10.210872    6.000000   \n",
       "compression_ratio            60000.0    0.507723     0.087634    0.017090   \n",
       "bits_per_char                60000.0    4.061782     0.701069    0.136724   \n",
       "avg_tree_depth               60000.0    5.921419     4.840174    0.287958   \n",
       "max_tree_depth               60000.0    9.824617     6.573506    1.000000   \n",
       "avg_dependency_distance      60000.0    3.072595     1.090981    1.000000   \n",
       "left_dependency_ratio        60000.0    0.479068     0.067091    0.000000   \n",
       "right_dependency_ratio       60000.0    0.520932     0.067091    0.000000   \n",
       "hapax_legomena_ratio         58627.0    0.464544     0.190073    0.000000   \n",
       "hapax_type_ratio             58627.0    0.727213     0.142479    0.000000   \n",
       "yules_k                      50745.0  158.827099   281.440224    0.000000   \n",
       "mtld                         30217.0  189.156146  1000.621077   32.307692   \n",
       "comma_ratio                  60000.0    0.046260     0.029530    0.000000   \n",
       "period_ratio                 60000.0    0.042645     0.022150    0.000000   \n",
       "question_ratio               60000.0    0.001483     0.005138    0.000000   \n",
       "exclamation_ratio            60000.0    0.001145     0.004812    0.000000   \n",
       "semicolon_ratio              60000.0    0.001415     0.004793    0.000000   \n",
       "colon_ratio                  60000.0    0.001855     0.004701    0.000000   \n",
       "quote_ratio                  60000.0    0.006725     0.013694    0.000000   \n",
       "sentiment_polarity           60000.0    0.093540     0.127433   -1.000000   \n",
       "sentiment_subjectivity       60000.0    0.422977     0.171981    0.000000   \n",
       "sentiment_polarity_variance  60000.0    0.040739     0.038813    0.000000   \n",
       "neutral_sentence_ratio       60000.0    0.548710     0.245906    0.000000   \n",
       "positive_word_ratio          60000.0    0.035182     0.028074    0.000000   \n",
       "negative_word_ratio          60000.0    0.016530     0.016713    0.000000   \n",
       "\n",
       "                                    10%         50%         90%           max  \n",
       "type_token_ratio               0.449405    0.580556    0.750000      1.000000  \n",
       "stopword_ratio                 0.231809    0.474320    0.608303      0.989848  \n",
       "punctuation_ratio              0.016949    0.028169    0.045172      0.313095  \n",
       "avg_sentence_length           12.000000   19.596875   30.000000    465.000000  \n",
       "sentence_length_std            4.192881    7.925103   16.102420    233.000000  \n",
       "n_tokens_doc                 111.000000  222.000000  397.000000  12615.000000  \n",
       "n_sentences_doc                5.000000   11.000000   24.000000    805.000000  \n",
       "flesch_reading_ease           21.496652   58.315472   82.195726    194.240294  \n",
       "gunning_fog                    7.406159   13.087584   20.405993     69.365657  \n",
       "smog_index                     7.447530   11.855464   17.122413     49.068485  \n",
       "automated_readability_index    5.741039   11.101261   18.109722     82.172177  \n",
       "unigram_diversity              0.456912    0.573171    0.714286      1.000000  \n",
       "bigram_diversity               0.822630    0.924138    0.981675      1.000000  \n",
       "trigram_diversity              0.924528    0.984962    1.000000      1.000000  \n",
       "bigram_entropy                 6.758752    7.592457    8.442943      8.958888  \n",
       "trigram_entropy                6.857981    7.742541    8.581364      8.960002  \n",
       "token_burstiness              -0.233536    0.024921    0.203058      0.755133  \n",
       "char_trigram_diversity         0.393808    0.524874    0.668304      1.000000  \n",
       "char_trigram_entropy           8.290397    8.954099    9.516813     10.759561  \n",
       "uppercase_ratio                0.010735    0.023746    0.045660      0.801292  \n",
       "whitespace_ratio               0.145735    0.171109    0.190850      0.331959  \n",
       "unique_char_count             34.000000   46.000000   60.000000    221.000000  \n",
       "compression_ratio              0.438690    0.512996    0.585500      1.266667  \n",
       "bits_per_char                  3.509517    4.103971    4.684001     10.133333  \n",
       "avg_tree_depth                 3.705882    5.583333    7.777778    367.000000  \n",
       "max_tree_depth                 6.000000    9.000000   13.000000    470.000000  \n",
       "avg_dependency_distance        2.549316    2.987275    3.533624    109.344304  \n",
       "left_dependency_ratio          0.404651    0.473786    0.560570      1.000000  \n",
       "right_dependency_ratio         0.439430    0.526214    0.595349      1.000000  \n",
       "hapax_legomena_ratio           0.284006    0.438525    0.688537      1.000000  \n",
       "hapax_type_ratio               0.596026    0.738095    0.864198      1.000000  \n",
       "yules_k                       68.703711  121.875000  215.578911   9979.079498  \n",
       "mtld                          55.777478   84.844828  154.261576  47528.320000  \n",
       "comma_ratio                    0.013042    0.043103    0.078603      0.500000  \n",
       "period_ratio                   0.022901    0.040865    0.063291      1.333333  \n",
       "question_ratio                 0.000000    0.000000    0.005025      0.263473  \n",
       "exclamation_ratio              0.000000    0.000000    0.003205      0.149701  \n",
       "semicolon_ratio                0.000000    0.000000    0.004509      0.180000  \n",
       "colon_ratio                    0.000000    0.000000    0.006494      0.193370  \n",
       "quote_ratio                    0.000000    0.000000    0.023148      0.295455  \n",
       "sentiment_polarity            -0.030561    0.085714    0.248162      1.000000  \n",
       "sentiment_subjectivity         0.200000    0.450000    0.598675      1.000000  \n",
       "sentiment_polarity_variance    0.000425    0.031471    0.090292      0.729601  \n",
       "neutral_sentence_ratio         0.250000    0.538462    1.000000      1.000000  \n",
       "positive_word_ratio            0.000000    0.032258    0.064516      0.993763  \n",
       "negative_word_ratio            0.000000    0.014085    0.034884      1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Save enriched dataset and basic descriptive statistics\n",
    "\n",
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_PREPOS.csv\"\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS}  (rows: {len(df_with_features)})\")\n",
    "\n",
    "feature_cols = [col for col in df_with_features.columns if col not in df.columns]\n",
    "print(f\"Feature columns saved ({len(feature_cols)} total):\")\n",
    "print(feature_cols)\n",
    "\n",
    "display(df_with_features[feature_cols].describe(percentiles=[0.1, 0.5, 0.9]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125c533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b190a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length min: 0.0 max: 465.0\n",
      "flesch_reading_ease min: -167.9107078507078 max: 194.24029411764707\n",
      "gunning_fog min: 1.1527659574468085 max: 69.36565656565656\n",
      "automated_readability_index min: -9.797680491551457 max: 82.17217701641684\n",
      "mtld min: 32.30769230769231 max: 47528.32000000002\n",
      "yules_k min: 0.0 max: 9979.07949790795\n",
      "max_tree_depth min: 1.0 max: 470.0\n"
     ]
    }
   ],
   "source": [
    "for col in [\"avg_sentence_length\",\"flesch_reading_ease\",\"gunning_fog\",\n",
    "            \"automated_readability_index\",\"mtld\",\"yules_k\",\"max_tree_depth\"]:\n",
    "    if col in df.columns:\n",
    "        print(col, \"min:\", df[col].min(), \"max:\", df[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b72a3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed thresholds:\n",
      "  asl_hi: 94.66666666666667\n",
      "  sls_hi: 61.5\n",
      "  fog_hi: 37.22068402366864\n",
      "  ari_hi: 41.545073474497\n",
      "  fre_lo: -29.083826048329737\n",
      "  mtld_hi: 4733.439999999991\n",
      "  yk_hi: 1754.3724034324473\n",
      "  depth_max_hi: 27.0\n",
      "  depth_avg_hi: 17.0\n",
      "  depdist_hi: 5.721523089248039\n",
      "  comp_hi: 1.0\n",
      "  upper_hi: 0.0942173674928059\n",
      "  uniq_hi: 73.0\n",
      "  ws_lo: 0.1233140655105973\n",
      "  ws_hi: 0.21633103545652826\n",
      "\n",
      "Cause counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>markup_or_code_noise</th>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependency_depth_computation_bug</th>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_metric_instability_or_length_effects</th>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_segmentation_failure</th>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              count\n",
       "cause                                              \n",
       "markup_or_code_noise                           1141\n",
       "dependency_depth_computation_bug                654\n",
       "lexical_metric_instability_or_length_effects    405\n",
       "sentence_segmentation_failure                   229"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flag counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <td>3614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_noise</th>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_instability</th>\n",
       "      <td>405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth_implausible</th>\n",
       "      <td>372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_outlier</th>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_overhead</th>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count\n",
       "severity                   3614\n",
       "markup_noise               1115\n",
       "seg_len_extreme            1022\n",
       "lexical_instability         405\n",
       "depth_implausible           372\n",
       "readability_outlier         357\n",
       "dep_distance_implausible    300\n",
       "compression_overhead         41\n",
       "ngram_edge_effects            2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top offenders (with original feature values):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <th>readability_outlier</th>\n",
       "      <th>lexical_instability</th>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <th>depth_implausible</th>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <th>compression_overhead</th>\n",
       "      <th>markup_noise</th>\n",
       "      <th>suspected_causes</th>\n",
       "      <th>severity</th>\n",
       "      <th>...</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>unique_char_count</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>bigram_entropy</th>\n",
       "      <th>trigram_entropy</th>\n",
       "      <th>bigram_diversity</th>\n",
       "      <th>trigram_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3183</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.322482</td>\n",
       "      <td>0.065202</td>\n",
       "      <td>0.046671</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.258751</td>\n",
       "      <td>2.520852</td>\n",
       "      <td>2.807304</td>\n",
       "      <td>0.015915</td>\n",
       "      <td>0.018617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15858</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.865823</td>\n",
       "      <td>0.517350</td>\n",
       "      <td>0.003471</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.122121</td>\n",
       "      <td>8.599913</td>\n",
       "      <td>8.596190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36435</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>470.0</td>\n",
       "      <td>1.118143</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.200500</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.200083</td>\n",
       "      <td>0.086196</td>\n",
       "      <td>0.086351</td>\n",
       "      <td>0.010417</td>\n",
       "      <td>0.010438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38321</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10.571429</td>\n",
       "      <td>0.094817</td>\n",
       "      <td>0.160556</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.027813</td>\n",
       "      <td>0.263122</td>\n",
       "      <td>0.264832</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.039370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45103</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.028494</td>\n",
       "      <td>0.166893</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.165536</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46486</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>5.724209</td>\n",
       "      <td>0.419024</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.112421</td>\n",
       "      <td>8.066089</td>\n",
       "      <td>8.060696</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49582</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>92.0</td>\n",
       "      <td>5.825203</td>\n",
       "      <td>0.483241</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.109037</td>\n",
       "      <td>8.022368</td>\n",
       "      <td>8.016808</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51083</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20.534674</td>\n",
       "      <td>0.162883</td>\n",
       "      <td>0.114018</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.163376</td>\n",
       "      <td>2.716205</td>\n",
       "      <td>2.727452</td>\n",
       "      <td>0.154440</td>\n",
       "      <td>0.158915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7115</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.697481</td>\n",
       "      <td>0.185150</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.172932</td>\n",
       "      <td>4.417189</td>\n",
       "      <td>5.048976</td>\n",
       "      <td>0.332226</td>\n",
       "      <td>0.396667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13451</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.398577</td>\n",
       "      <td>0.039560</td>\n",
       "      <td>0.118681</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.176557</td>\n",
       "      <td>2.613044</td>\n",
       "      <td>2.613073</td>\n",
       "      <td>0.029046</td>\n",
       "      <td>0.029167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>49.662116</td>\n",
       "      <td>0.023643</td>\n",
       "      <td>0.157980</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.157442</td>\n",
       "      <td>1.584946</td>\n",
       "      <td>1.584946</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.010274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26074</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.254747</td>\n",
       "      <td>0.504972</td>\n",
       "      <td>0.011809</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.115289</td>\n",
       "      <td>8.531381</td>\n",
       "      <td>8.527477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13318</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5.926407</td>\n",
       "      <td>0.127469</td>\n",
       "      <td>0.007181</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.203770</td>\n",
       "      <td>3.797012</td>\n",
       "      <td>3.846402</td>\n",
       "      <td>0.179641</td>\n",
       "      <td>0.192771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9640</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>5.052589</td>\n",
       "      <td>0.535903</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.136260</td>\n",
       "      <td>8.463524</td>\n",
       "      <td>8.459432</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>2.625352</td>\n",
       "      <td>0.131895</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.119038</td>\n",
       "      <td>4.006582</td>\n",
       "      <td>4.114368</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.304813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16168</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.665072</td>\n",
       "      <td>0.034557</td>\n",
       "      <td>0.100792</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.150468</td>\n",
       "      <td>1.832373</td>\n",
       "      <td>1.939085</td>\n",
       "      <td>0.028708</td>\n",
       "      <td>0.033654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2.890011</td>\n",
       "      <td>0.160690</td>\n",
       "      <td>0.010345</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.030345</td>\n",
       "      <td>4.295535</td>\n",
       "      <td>4.333473</td>\n",
       "      <td>0.290503</td>\n",
       "      <td>0.297753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12462</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.597866</td>\n",
       "      <td>0.524238</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.122311</td>\n",
       "      <td>8.573647</td>\n",
       "      <td>8.569856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23158</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>123.0</td>\n",
       "      <td>1.502041</td>\n",
       "      <td>0.021902</td>\n",
       "      <td>0.153942</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.153317</td>\n",
       "      <td>0.999988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.008163</td>\n",
       "      <td>0.008197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21855</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6.338357</td>\n",
       "      <td>0.562011</td>\n",
       "      <td>0.007019</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.132535</td>\n",
       "      <td>8.312883</td>\n",
       "      <td>8.308339</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_len_extreme  readability_outlier  lexical_instability  \\\n",
       "3183              True                 True                 True   \n",
       "15858             True                False                 True   \n",
       "36435             True                 True                 True   \n",
       "38321             True                False                 True   \n",
       "45103             True                False                 True   \n",
       "46486             True                 True                False   \n",
       "49582             True                 True                False   \n",
       "51083             True                 True                 True   \n",
       "7115              True                False                 True   \n",
       "13451             True                False                 True   \n",
       "8346              True                False                 True   \n",
       "26074             True                 True                False   \n",
       "13318             True                False                 True   \n",
       "9640              True                 True                 True   \n",
       "1940              True                False                 True   \n",
       "16168             True                False                 True   \n",
       "4473              True                 True                False   \n",
       "12462             True                 True                 True   \n",
       "23158             True                False                 True   \n",
       "21855             True                 True                 True   \n",
       "\n",
       "       ngram_edge_effects  depth_implausible  dep_distance_implausible  \\\n",
       "3183                False              False                      True   \n",
       "15858               False               True                      True   \n",
       "36435               False               True                     False   \n",
       "38321               False               True                      True   \n",
       "45103                True               True                     False   \n",
       "46486               False               True                      True   \n",
       "49582               False               True                      True   \n",
       "51083               False              False                      True   \n",
       "7115                False               True                     False   \n",
       "13451               False               True                     False   \n",
       "8346                False              False                      True   \n",
       "26074               False              False                      True   \n",
       "13318               False               True                      True   \n",
       "9640                False               True                     False   \n",
       "1940                False               True                     False   \n",
       "16168               False               True                     False   \n",
       "4473                False               True                     False   \n",
       "12462               False              False                     False   \n",
       "23158               False               True                     False   \n",
       "21855               False              False                      True   \n",
       "\n",
       "       compression_overhead  markup_noise  \\\n",
       "3183                  False          True   \n",
       "15858                 False          True   \n",
       "36435                 False          True   \n",
       "38321                 False          True   \n",
       "45103                 False          True   \n",
       "46486                 False          True   \n",
       "49582                 False          True   \n",
       "51083                 False          True   \n",
       "7115                  False          True   \n",
       "13451                 False          True   \n",
       "8346                  False          True   \n",
       "26074                 False          True   \n",
       "13318                 False         False   \n",
       "9640                  False         False   \n",
       "1940                  False          True   \n",
       "16168                 False          True   \n",
       "4473                  False          True   \n",
       "12462                 False          True   \n",
       "23158                 False          True   \n",
       "21855                 False         False   \n",
       "\n",
       "                                        suspected_causes  severity  ...  \\\n",
       "3183   [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "15858  [dependency_depth_computation_bug, lexical_met...         5  ...   \n",
       "36435  [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "38321  [dependency_depth_computation_bug, lexical_met...         5  ...   \n",
       "45103  [dependency_depth_computation_bug, lexical_met...         5  ...   \n",
       "46486  [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "49582  [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "51083  [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "7115   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "13451  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "8346   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "26074  [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "13318  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "9640   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "1940   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "16168  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "4473   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "12462  [sentence_segmentation_failure, lexical_metric...         4  ...   \n",
       "23158  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "21855  [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "\n",
       "       max_tree_depth  avg_dependency_distance  compression_ratio  \\\n",
       "3183              8.0                 8.322482           0.065202   \n",
       "15858            40.0                 6.865823           0.517350   \n",
       "36435           470.0                 1.118143           0.017090   \n",
       "38321            19.0                10.571429           0.094817   \n",
       "45103           122.0                 1.000000           0.028494   \n",
       "46486            43.0                 5.724209           0.419024   \n",
       "49582            92.0                 5.825203           0.483241   \n",
       "51083             7.0                20.534674           0.162883   \n",
       "7115             30.0                 2.697481           0.185150   \n",
       "13451            42.0                 3.398577           0.039560   \n",
       "8346              3.0                49.662116           0.023643   \n",
       "26074            16.0                 6.254747           0.504972   \n",
       "13318            26.0                 5.926407           0.127469   \n",
       "9640             24.0                 5.052589           0.535903   \n",
       "1940             90.0                 2.625352           0.131895   \n",
       "16168            70.0                 1.665072           0.034557   \n",
       "4473            142.0                 2.890011           0.160690   \n",
       "12462            12.0                 3.597866           0.524238   \n",
       "23158           123.0                 1.502041           0.021902   \n",
       "21855            22.0                 6.338357           0.562011   \n",
       "\n",
       "       uppercase_ratio  unique_char_count  whitespace_ratio  bigram_entropy  \\\n",
       "3183          0.046671               17.0          0.258751        2.520852   \n",
       "15858         0.003471               41.0          0.122121        8.599913   \n",
       "36435         0.200500               11.0          0.200083        0.086196   \n",
       "38321         0.160556               15.0          0.027813        0.263122   \n",
       "45103         0.166893                6.0          0.165536       -0.000000   \n",
       "46486         0.003368               40.0          0.112421        8.066089   \n",
       "49582         0.002121               35.0          0.109037        8.022368   \n",
       "51083         0.114018               42.0          0.163376        2.716205   \n",
       "7115          0.119831               47.0          0.172932        4.417189   \n",
       "13451         0.118681               18.0          0.176557        2.613044   \n",
       "8346          0.157980               14.0          0.157442        1.584946   \n",
       "26074         0.011809               49.0          0.115289        8.531381   \n",
       "13318         0.007181               35.0          0.203770        3.797012   \n",
       "9640          0.000757               41.0          0.136260        8.463524   \n",
       "1940          0.005392               32.0          0.119038        4.006582   \n",
       "16168         0.100792               15.0          0.150468        1.832373   \n",
       "4473          0.010345               44.0          0.030345        4.295535   \n",
       "12462         0.003531               41.0          0.122311        8.573647   \n",
       "23158         0.153942               11.0          0.153317        0.999988   \n",
       "21855         0.007019               60.0          0.132535        8.312883   \n",
       "\n",
       "       trigram_entropy  bigram_diversity  trigram_diversity  \n",
       "3183          2.807304          0.015915           0.018617  \n",
       "15858         8.596190          1.000000           1.000000  \n",
       "36435         0.086351          0.010417           0.010438  \n",
       "38321         0.264832          0.039062           0.039370  \n",
       "45103        -0.000000          0.008197           0.008264  \n",
       "46486         8.060696          1.000000           1.000000  \n",
       "49582         8.016808          1.000000           1.000000  \n",
       "51083         2.727452          0.154440           0.158915  \n",
       "7115          5.048976          0.332226           0.396667  \n",
       "13451         2.613073          0.029046           0.029167  \n",
       "8346          1.584946          0.010239           0.010274  \n",
       "26074         8.527477          1.000000           1.000000  \n",
       "13318         3.846402          0.179641           0.192771  \n",
       "9640          8.459432          1.000000           1.000000  \n",
       "1940          4.114368          0.280000           0.304813  \n",
       "16168         1.939085          0.028708           0.033654  \n",
       "4473          4.333473          0.290503           0.297753  \n",
       "12462         8.569856          1.000000           1.000000  \n",
       "23158         1.000000          0.008163           0.008197  \n",
       "21855         8.308339          1.000000           1.000000  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"bigram_entropy\",\"trigram_entropy\",\"bigram_diversity\",\"trigram_diversity\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\"whitespace_ratio\"\n",
    "]\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols=NUMERIC_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Coerce known feature columns to numeric (if present).\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _q(df, col, q):\n",
    "    return float(np.nanquantile(df[col].values, q)) if col in df else np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# Thresholds and Diagnostics\n",
    "# -------------------------------\n",
    "def compute_diagnostic_thresholds(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Data-driven thresholds using quantiles only.\"\"\"\n",
    "    thr = {}\n",
    "    thr[\"asl_hi\"]       = _q(df, \"avg_sentence_length\", 0.99)\n",
    "    thr[\"sls_hi\"]       = _q(df, \"sentence_length_std\", 0.99)\n",
    "    thr[\"fog_hi\"]       = _q(df, \"gunning_fog\", 0.995)\n",
    "    thr[\"ari_hi\"]       = _q(df, \"automated_readability_index\", 0.995)\n",
    "    thr[\"fre_lo\"]       = _q(df, \"flesch_reading_ease\", 0.005)\n",
    "    thr[\"mtld_hi\"]      = _q(df, \"mtld\", 0.995)\n",
    "    thr[\"yk_hi\"]        = _q(df, \"yules_k\", 0.995)\n",
    "    thr[\"depth_max_hi\"] = _q(df, \"max_tree_depth\", 0.995)\n",
    "    thr[\"depth_avg_hi\"] = _q(df, \"avg_tree_depth\", 0.995)\n",
    "    thr[\"depdist_hi\"]   = _q(df, \"avg_dependency_distance\", 0.995)\n",
    "    thr[\"comp_hi\"]      = 1.0  # Compression > 1.0 = expansion\n",
    "    thr[\"upper_hi\"]     = _q(df, \"uppercase_ratio\", 0.995)\n",
    "    thr[\"uniq_hi\"]      = _q(df, \"unique_char_count\", 0.995)\n",
    "    thr[\"ws_lo\"]        = _q(df, \"whitespace_ratio\", 0.005)\n",
    "    thr[\"ws_hi\"]        = _q(df, \"whitespace_ratio\", 0.995)\n",
    "    return thr\n",
    "\n",
    "def diagnose_feature_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return diagnostics dataframe with boolean flags and suspected causes list.\"\"\"\n",
    "    # Ensure numeric so comparisons fire correctly\n",
    "    df = ensure_numeric(df)\n",
    "\n",
    "    thr = compute_diagnostic_thresholds(df)\n",
    "    D = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Flag definitions\n",
    "    D[\"seg_len_extreme\"] = (\n",
    "        (df.get(\"avg_sentence_length\", np.nan) > thr[\"asl_hi\"]) |\n",
    "        (df.get(\"sentence_length_std\", np.nan) > thr[\"sls_hi\"])\n",
    "    )\n",
    "    D[\"readability_outlier\"] = (\n",
    "        (df.get(\"flesch_reading_ease\", np.nan) < thr[\"fre_lo\"]) |\n",
    "        (df.get(\"gunning_fog\", np.nan) > thr[\"fog_hi\"]) |\n",
    "        (df.get(\"automated_readability_index\", np.nan) > thr[\"ari_hi\"])\n",
    "    )\n",
    "    D[\"lexical_instability\"] = (\n",
    "        (df.get(\"mtld\", np.nan) > thr[\"mtld_hi\"]) |\n",
    "        (df.get(\"yules_k\", np.nan) > thr[\"yk_hi\"])\n",
    "    )\n",
    "    D[\"ngram_edge_effects\"] = (\n",
    "    (df.get(\"bigram_entropy\", np.nan) == 0) |\n",
    "    (df.get(\"trigram_entropy\", np.nan) == 0) |\n",
    "    ((df.get(\"n_tokens_ws\", 1000) < 50) & (df.get(\"trigram_diversity\", 0) == 1.0)) |\n",
    "    ((df.get(\"n_tokens_ws\", 1000) < 50) & (df.get(\"bigram_diversity\", 0) == 1.0))\n",
    "    )\n",
    "    D[\"depth_implausible\"] = (\n",
    "        (df.get(\"max_tree_depth\", np.nan) > thr[\"depth_max_hi\"]) |\n",
    "        (df.get(\"avg_tree_depth\", np.nan) > thr[\"depth_avg_hi\"])\n",
    "    )\n",
    "    D[\"dep_distance_implausible\"] = (df.get(\"avg_dependency_distance\", np.nan) > thr[\"depdist_hi\"])\n",
    "    D[\"compression_overhead\"] = (df.get(\"compression_ratio\", np.nan) > thr[\"comp_hi\"])\n",
    "    D[\"markup_noise\"] = (\n",
    "    (df.get(\"uppercase_ratio\", np.nan) > thr[\"upper_hi\"]) |\n",
    "    (df.get(\"unique_char_count\", np.nan) > thr[\"uniq_hi\"]) |\n",
    "    (df.get(\"whitespace_ratio\", np.nan) < thr[\"ws_lo\"]) |\n",
    "    (df.get(\"whitespace_ratio\", np.nan) > thr[\"ws_hi\"])\n",
    "    )\n",
    "\n",
    "    # Suspected causes column (pre-allocate + .at)\n",
    "    D[\"suspected_causes\"] = pd.Series([[] for _ in range(len(D))], index=D.index, dtype=object)\n",
    "    for i in D.index:\n",
    "        c = []\n",
    "        if bool(D.at[i, \"depth_implausible\"]) or bool(D.at[i, \"dep_distance_implausible\"]):\n",
    "            c.append(\"dependency_depth_computation_bug\")\n",
    "        if bool(D.at[i, \"seg_len_extreme\"]) and bool(D.at[i, \"readability_outlier\"]):\n",
    "            c.append(\"sentence_segmentation_failure\")\n",
    "        if bool(D.at[i, \"lexical_instability\"]) or bool(D.at[i, \"ngram_edge_effects\"]):\n",
    "            c.append(\"lexical_metric_instability_or_length_effects\")\n",
    "        if bool(D.at[i, \"compression_overhead\"]) or bool(D.at[i, \"markup_noise\"]):\n",
    "            c.append(\"markup_or_code_noise\")\n",
    "        D.at[i, \"suspected_causes\"] = c\n",
    "\n",
    "    # Severity score\n",
    "    flag_cols = [c for c in D.columns if c != \"suspected_causes\"]\n",
    "    for c in flag_cols:\n",
    "        D[c] = D[c].fillna(False).astype(bool)\n",
    "    D[\"severity\"] = D[flag_cols].sum(axis=1).astype(int)\n",
    "\n",
    "    # Attach thresholds for later inspection\n",
    "    D.attrs[\"thresholds\"] = thr\n",
    "    return D\n",
    "\n",
    "def build_diagnostic_report(diag: pd.DataFrame, top_k: int = 15):\n",
    "    \"\"\"Summarize counts by cause and return top offending rows by severity.\"\"\"\n",
    "    cause_counts = (\n",
    "        diag[\"suspected_causes\"].explode().value_counts(dropna=True)\n",
    "        .rename_axis(\"cause\").to_frame(\"count\")\n",
    "    )\n",
    "    flag_counts = (\n",
    "        diag.drop(columns=[\"suspected_causes\"])\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .to_frame(\"count\")\n",
    "    )\n",
    "    top_offenders = diag.sort_values(\"severity\", ascending=False).head(top_k)\n",
    "    return {\"cause_counts\": cause_counts, \"flag_counts\": flag_counts, \"top_offenders\": top_offenders}\n",
    "\n",
    "# -------------------------------\n",
    "# Runner / Example usage\n",
    "# -------------------------------\n",
    "\n",
    "# Assume you already have `df` with your features\n",
    "# If your features were just computed and may be strings, the coercion inside\n",
    "# diagnose_feature_outliers will handle them; coercing here is optional:\n",
    "# df = ensure_numeric(df)\n",
    "\n",
    "diag = diagnose_feature_outliers(df)\n",
    "rep  = build_diagnostic_report(diag, top_k=20)\n",
    "\n",
    "print(\"Computed thresholds:\")\n",
    "for k, v in diag.attrs.get(\"thresholds\", {}).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nCause counts:\")\n",
    "display(rep[\"cause_counts\"])\n",
    "\n",
    "print(\"\\nFlag counts:\")\n",
    "display(rep[\"flag_counts\"])\n",
    "\n",
    "print(\"\\nTop offenders (with original feature values):\")\n",
    "cols_to_show = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\n",
    "    \"whitespace_ratio\",\"bigram_entropy\",\"trigram_entropy\",\n",
    "    \"bigram_diversity\",\"trigram_diversity\"\n",
    "]\n",
    "existing_cols = [c for c in cols_to_show if c in df.columns]\n",
    "display(rep[\"top_offenders\"].join(df[existing_cols], how=\"left\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c0ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FEATURES_TO_CAP = [\n",
    "    'avg_sentence_length','sentence_length_std','flesch_reading_ease',\n",
    "    'gunning_fog','automated_readability_index','mtld','yules_k',\n",
    "    'avg_dependency_distance','max_tree_depth','compression_ratio',\n",
    "    'bigram_entropy','trigram_entropy','bigram_diversity','trigram_diversity'\n",
    "]\n",
    "\n",
    "# Natural/practical bounds to prevent absurd caps\n",
    "BOUNDED = {\n",
    "    'bigram_diversity': (0.0, 1.0),\n",
    "    'trigram_diversity': (0.0, 1.0),\n",
    "    'compression_ratio': (0.0, np.inf),\n",
    "    'flesch_reading_ease': (-100.0, 150.0),   # practical working range\n",
    "}\n",
    "\n",
    "def _finite(s: pd.Series) -> pd.Series:\n",
    "    return s[np.isfinite(s.values)]\n",
    "\n",
    "def calculate_percentile_caps(df: pd.DataFrame, lower_pct=1, upper_pct=99) -> dict:\n",
    "    caps = {}\n",
    "    for feat in FEATURES_TO_CAP:\n",
    "        if feat not in df.columns: \n",
    "            continue\n",
    "        s = _finite(df[feat].dropna())\n",
    "        if s.empty:\n",
    "            continue\n",
    "        lo = float(np.percentile(s, lower_pct))\n",
    "        hi = float(np.percentile(s, upper_pct))\n",
    "        if feat in BOUNDED:\n",
    "            blo, bhi = BOUNDED[feat]\n",
    "            lo = max(lo, blo)\n",
    "            hi = min(hi, bhi)\n",
    "        if lo > hi:  # degenerate case\n",
    "            lo, hi = hi, lo\n",
    "        caps[feat] = (lo, hi)\n",
    "        print(f\"{feat}: [{lo:.2f}, {hi:.2f}]\")\n",
    "    return caps\n",
    "\n",
    "def cap_extreme_features(df: pd.DataFrame, caps: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['features_capped'] = 0  # keep your counter if you like\n",
    "\n",
    "    for feat, (lo, hi) in caps.items():\n",
    "        if feat not in df.columns:\n",
    "            continue\n",
    "        col = df[feat]\n",
    "        before = col.copy()\n",
    "        # clip in-place while preserving NaNs\n",
    "        df[feat] = col.clip(lower=lo, upper=hi)\n",
    "        changed = (before != df[feat]) & df[feat].notna() & before.notna()\n",
    "        if changed.any():\n",
    "            print(f\"Capped {feat}: {int(changed.sum())} values\")\n",
    "            df['features_capped'] += changed.astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee2f54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length: [7.81, 94.67]\n",
      "sentence_length_std: [1.49, 61.50]\n",
      "flesch_reading_ease: [-11.71, 96.66]\n",
      "gunning_fog: [4.67, 32.09]\n",
      "automated_readability_index: [2.06, 35.25]\n",
      "mtld: [42.88, 2619.86]\n",
      "yules_k: [2.70, 1112.71]\n",
      "avg_dependency_distance: [2.14, 4.82]\n",
      "max_tree_depth: [5.00, 20.00]\n",
      "compression_ratio: [0.11, 0.70]\n",
      "bigram_entropy: [4.26, 8.84]\n",
      "trigram_entropy: [4.52, 8.95]\n",
      "bigram_diversity: [0.13, 1.00]\n",
      "trigram_diversity: [0.14, 1.00]\n",
      "Capped avg_sentence_length: 1193 values\n",
      "Capped sentence_length_std: 1185 values\n",
      "Capped flesch_reading_ease: 1104 values\n",
      "Capped gunning_fog: 1104 values\n",
      "Capped automated_readability_index: 1104 values\n",
      "Capped mtld: 605 values\n",
      "Capped yules_k: 1016 values\n",
      "Capped avg_dependency_distance: 1200 values\n",
      "Capped max_tree_depth: 1110 values\n",
      "Capped compression_ratio: 1200 values\n",
      "Capped bigram_entropy: 1116 values\n",
      "Capped trigram_entropy: 910 values\n",
      "Capped bigram_diversity: 558 values\n",
      "Capped trigram_diversity: 557 values\n"
     ]
    }
   ],
   "source": [
    "actual_caps = calculate_percentile_caps(df_with_features, lower_pct=1, upper_pct=99)\n",
    "df_with_features = cap_extreme_features(df_with_features, actual_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5e1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3.0 standard deviations as threshold\n",
      "Parse issues: 922 texts\n",
      "Readability anomalies: 1068 texts\n",
      "Lexical anomalies: 2525 texts\n",
      "Quality score distribution:\n",
      "quality_score\n",
      "0      175\n",
      "1      776\n",
      "2     2438\n",
      "3    56611\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "READABILITY_FEATS = ['flesch_reading_ease','gunning_fog','automated_readability_index']\n",
    "LEXICAL_FEATS     = ['mtld','yules_k','type_token_ratio']\n",
    "PARSE_FEATS       = ['avg_dependency_distance']  # plus tree-depth presence checks\n",
    "\n",
    "# Features where z-score should be computed on log1p to tame skew (values stay untouched)\n",
    "LOG_FOR_Z = {'mtld','yules_k','avg_dependency_distance','sentence_length_std','max_tree_depth'}\n",
    "\n",
    "def _z_on(series: pd.Series, log_if_needed: bool) -> pd.Series:\n",
    "    s = series.astype(float).replace([np.inf,-np.inf], np.nan)\n",
    "    if log_if_needed:\n",
    "        nonan = s.dropna()\n",
    "        if (nonan >= 0).all():\n",
    "            s = np.log1p(s)\n",
    "    # classical population z-score (ddof=0); fill NaNs with mean to avoid bias in stats.zscore\n",
    "    m = s.mean()\n",
    "    sd = s.std(ddof=0)\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - m) / sd\n",
    "\n",
    "def add_quality_flags_statistical(df: pd.DataFrame, n_std: float = 3.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Compute z-scores (abs) transiently; do not keep columns\n",
    "    z_map = {}\n",
    "\n",
    "    for feat in set(READABILITY_FEATS + LEXICAL_FEATS + PARSE_FEATS):\n",
    "        if feat in df.columns:\n",
    "            z = _z_on(df[feat], log_if_needed=(feat in LOG_FOR_Z)).abs()\n",
    "            z_map[feat] = z\n",
    "\n",
    "    df['parse_quality_issue'] = (\n",
    "        df['max_tree_depth'].isna() |\n",
    "        df['avg_tree_depth'].isna() |\n",
    "        (z_map.get('avg_dependency_distance', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['readability_anomaly'] = (\n",
    "        (z_map.get('flesch_reading_ease', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('gunning_fog', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('automated_readability_index', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['lexical_anomaly'] = (\n",
    "        (z_map.get('mtld', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('yules_k', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('type_token_ratio', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['quality_score'] = (3 - (\n",
    "        df['parse_quality_issue'].astype(int) +\n",
    "        df['readability_anomaly'].astype(int) +\n",
    "        df['lexical_anomaly'].astype(int)\n",
    "    )).clip(lower=0, upper=3)\n",
    "\n",
    "    print(f\"Using {n_std} standard deviations as threshold\")\n",
    "    print(f\"Parse issues: {int(df['parse_quality_issue'].sum())} texts\")\n",
    "    print(f\"Readability anomalies: {int(df['readability_anomaly'].sum())} texts\")\n",
    "    print(f\"Lexical anomalies: {int(df['lexical_anomaly'].sum())} texts\")\n",
    "    print(f\"Quality score distribution:\\n{df['quality_score'].value_counts().sort_index()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_features = add_quality_flags_statistical(df_with_features, n_std=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a3c1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Impute] Dropping duplicate columns: ['avg_word_length', 'digit_ratio', 'had_urls', 'had_html', 'had_code', 'had_table']\n",
      "\n",
      "[Impute] Dropping 2 features with >40% missing:\n",
      "  - not_text_reason: 100.0% missing\n",
      "  - mtld: 49.6% missing\n",
      "\n",
      "[Impute] Filled 53783 missing values across 16 features\n",
      "Top 10 imputed features:\n",
      "yules_k                        9255\n",
      "smog_index                     5114\n",
      "gunning_fog                    4877\n",
      "flesch_reading_ease            4877\n",
      "automated_readability_index    4877\n",
      "trigram_diversity              4362\n",
      "trigram_entropy                4362\n",
      "bigram_diversity               4259\n",
      "bigram_entropy                 4259\n",
      "unigram_diversity              4122\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def impute_after_capping(df: pd.DataFrame, max_missing_pct: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing values with group medians, dropping features with excessive missingness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    max_missing_pct : float\n",
    "        Maximum proportion of missing values allowed (default 0.4 = 40%)\n",
    "        Features exceeding this threshold are dropped before imputation\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop duplicate columns (keep first)\n",
    "    if df.columns.duplicated().any():\n",
    "        dup_mask = df.columns.duplicated()\n",
    "        print(f\"[Impute] Dropping duplicate columns: {df.columns[dup_mask].tolist()}\")\n",
    "        df = df.loc[:, ~dup_mask]\n",
    "\n",
    "    # Numeric features except target\n",
    "    num_feats = [c for c in df.select_dtypes(include=[np.number]).columns if c != \"is_ai\"]\n",
    "    if not num_feats:\n",
    "        return df\n",
    "\n",
    "    # Check missingness and drop high-missing features FIRST\n",
    "    missing_pct = df[num_feats].isna().mean()\n",
    "    high_missing = missing_pct[missing_pct > max_missing_pct].sort_values(ascending=False)\n",
    "    \n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\n[Impute] Dropping {len(high_missing)} features with >{max_missing_pct*100:.0f}% missing:\")\n",
    "        for feat, pct in high_missing.items():\n",
    "            print(f\"  - {feat}: {pct*100:.1f}% missing\")\n",
    "        \n",
    "        # Remove high-missing features\n",
    "        num_feats = [f for f in num_feats if f not in high_missing.index]\n",
    "        df.drop(columns=high_missing.index, inplace=True)\n",
    "    \n",
    "    if not num_feats:\n",
    "        print(\"[Impute] No features remaining after missingness filter\")\n",
    "        return df\n",
    "\n",
    "    # Group keys for stratified imputation\n",
    "    group_keys = []\n",
    "    if \"source_type\" in df.columns:\n",
    "        group_keys.append(\"source_type\")\n",
    "    if \"n_tokens_doc\" in df.columns:\n",
    "        bins = [0, 100, 250, 500, 10_000]\n",
    "        labels = [\"S\", \"M\", \"L\", \"XL\"]\n",
    "        df[\"__len_bin__\"] = pd.cut(df[\"n_tokens_doc\"], bins=bins, right=False, labels=labels)\n",
    "        group_keys.append(\"__len_bin__\")\n",
    "\n",
    "    before_missing = df[num_feats].isna().sum()\n",
    "    values = df[num_feats].to_numpy(dtype=float)\n",
    "    mask = np.isnan(values)\n",
    "\n",
    "    # Group-wise median imputation\n",
    "    if group_keys:\n",
    "        med_df = df.groupby(group_keys, dropna=False, observed=False)[num_feats].transform(\"median\")\n",
    "        med_vals = med_df.to_numpy(dtype=float)\n",
    "        values = np.where(mask, med_vals, values)\n",
    "\n",
    "    # Global per-column median fallback\n",
    "    still_nan = np.isnan(values)\n",
    "    if still_nan.any():\n",
    "        # Check for columns that are entirely NaN after group fill\n",
    "        all_nan_cols = np.where(np.isnan(values).all(axis=0))[0]\n",
    "        if len(all_nan_cols):\n",
    "            drop_cols = [num_feats[i] for i in all_nan_cols]\n",
    "            print(f\"[Impute] Dropping {len(drop_cols)} all-NaN features: {drop_cols}\")\n",
    "            \n",
    "            keep_idx = [i for i in range(values.shape[1]) if i not in all_nan_cols]\n",
    "            values = values[:, keep_idx]\n",
    "            num_feats = [num_feats[i] for i in keep_idx]\n",
    "            df.drop(columns=drop_cols, inplace=True)\n",
    "            still_nan = np.isnan(values)\n",
    "\n",
    "        if still_nan.any():\n",
    "            col_medians = np.nanmedian(values, axis=0)\n",
    "            row_idx, col_idx = np.where(still_nan)\n",
    "            values[row_idx, col_idx] = col_medians[col_idx]\n",
    "\n",
    "    # Write back\n",
    "    df.loc[:, num_feats] = values\n",
    "    df.drop(columns=[\"__len_bin__\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Report imputation summary\n",
    "    after_missing = df[num_feats].isna().sum()\n",
    "    imputed_counts = (before_missing.reindex(num_feats).fillna(0).astype(int)\n",
    "                      - after_missing.reindex(num_feats).fillna(0).astype(int))\n",
    "    total_imputed = int(imputed_counts.sum())\n",
    "    \n",
    "    if total_imputed:\n",
    "        print(f\"\\n[Impute] Filled {total_imputed} missing values across {(imputed_counts > 0).sum()} features\")\n",
    "        top_imputed = imputed_counts[imputed_counts > 0].sort_values(ascending=False).head(10)\n",
    "        print(\"Top 10 imputed features:\")\n",
    "        print(top_imputed.to_string())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_with_features = impute_after_capping(df_with_features, max_missing_pct=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9480101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Correlation] Found 23 highly correlated pairs (>0.85):\n",
      "                  feature_1              feature_2  correlation\n",
      "     right_dependency_ratio  left_dependency_ratio     1.000000\n",
      "               n_tokens_doc                  n_tok     0.999695\n",
      "                n_tokens_ws                  n_tok     0.998853\n",
      "               n_tokens_doc            n_tokens_ws     0.998847\n",
      "                      n_tok                n_chars     0.993493\n",
      "                n_tokens_ws                n_chars     0.993412\n",
      "               n_tokens_doc                n_chars     0.993291\n",
      "            trigram_entropy         bigram_entropy     0.990213\n",
      "              bits_per_char      compression_ratio     0.985311\n",
      "          trigram_diversity       bigram_diversity     0.967851\n",
      "                 smog_index    flesch_reading_ease     0.939617\n",
      "                gunning_fog    flesch_reading_ease     0.933114\n",
      "                 smog_index            gunning_fog     0.931207\n",
      "           whitespace_ratio        avg_word_length     0.924752\n",
      "automated_readability_index            gunning_fog     0.910420\n",
      "          unigram_diversity       type_token_ratio     0.905217\n",
      "          compression_ratio char_trigram_diversity     0.903729\n",
      "              bits_per_char char_trigram_diversity     0.899692\n",
      "           hapax_type_ratio   hapax_legomena_ratio     0.876604\n",
      "            n_sentences_doc            n_tokens_ws     0.862988\n",
      "     char_trigram_diversity       type_token_ratio     0.862020\n",
      "            n_sentences_doc           n_tokens_doc     0.856282\n",
      "            n_sentences_doc                  n_tok     0.855371\n",
      "  Drop 'left_dependency_ratio', keep 'right_dependency_ratio' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_tok', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_tokens_ws', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_chars', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'bigram_entropy', keep 'trigram_entropy' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'compression_ratio', keep 'bits_per_char' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'bigram_diversity', keep 'trigram_diversity' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'flesch_reading_ease', keep 'smog_index' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'gunning_fog', keep 'smog_index' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'avg_word_length', keep 'whitespace_ratio' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'type_token_ratio', keep 'unigram_diversity' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'char_trigram_diversity', keep 'bits_per_char' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'hapax_legomena_ratio', keep 'hapax_type_ratio' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_tokens_doc', keep 'n_sentences_doc' (missing: 0.0% vs 0.0%)\n",
      "\n",
      "[Correlation] Dropped 14 redundant features\n",
      "  Remaining features: 43\n"
     ]
    }
   ],
   "source": [
    "def drop_correlated_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str] = None,\n",
    "    threshold: float = 0.85,\n",
    "    method: str = 'pearson',\n",
    "    keep_strategy: str = 'lower_missing'\n",
    ") -> Tuple[pd.DataFrame, List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Identify and drop highly correlated features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature_cols : List[str], optional\n",
    "        List of feature columns to check. If None, uses all numeric columns except 'is_ai'\n",
    "    threshold : float\n",
    "        Correlation threshold (default 0.85). Pairs above this are considered redundant\n",
    "    method : str\n",
    "        Correlation method: 'pearson', 'spearman', or 'kendall'\n",
    "    keep_strategy : str\n",
    "        Which feature to keep from correlated pairs:\n",
    "        - 'lower_missing': Keep feature with less missing data\n",
    "        - 'higher_variance': Keep feature with higher variance\n",
    "        - 'first': Keep the first feature alphabetically\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_filtered : pd.DataFrame\n",
    "        Dataframe with redundant features removed\n",
    "    dropped_features : List[str]\n",
    "        List of dropped feature names\n",
    "    corr_pairs : pd.DataFrame\n",
    "        DataFrame showing correlated pairs and correlation values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify feature columns\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                       if c != 'is_ai']\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[feature_cols].corr(method=method).abs()\n",
    "    \n",
    "    # Get upper triangle (avoid double-counting pairs)\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find correlated pairs\n",
    "    correlated_pairs = []\n",
    "    for col in upper_tri.columns:\n",
    "        high_corr = upper_tri[col][upper_tri[col] > threshold]\n",
    "        for idx, corr_val in high_corr.items():\n",
    "            correlated_pairs.append({\n",
    "                'feature_1': col,\n",
    "                'feature_2': idx,\n",
    "                'correlation': corr_val\n",
    "            })\n",
    "    \n",
    "    if not correlated_pairs:\n",
    "        print(f\"[Correlation] No feature pairs with correlation > {threshold}\")\n",
    "        return df, [], pd.DataFrame()\n",
    "    \n",
    "    # Create dataframe of correlated pairs\n",
    "    corr_df = pd.DataFrame(correlated_pairs).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\n[Correlation] Found {len(corr_df)} highly correlated pairs (>{threshold}):\")\n",
    "    print(corr_df.to_string(index=False))\n",
    "    \n",
    "    # Determine which features to drop\n",
    "    to_drop: Set[str] = set()\n",
    "    \n",
    "    for _, row in corr_df.iterrows():\n",
    "        feat1, feat2 = row['feature_1'], row['feature_2']\n",
    "        \n",
    "        # Skip if either already marked for dropping\n",
    "        if feat1 in to_drop or feat2 in to_drop:\n",
    "            continue\n",
    "        \n",
    "        # Decide which to keep based on strategy\n",
    "        if keep_strategy == 'lower_missing':\n",
    "            miss1 = df[feat1].isna().mean()\n",
    "            miss2 = df[feat2].isna().mean()\n",
    "            drop_feat = feat1 if miss1 > miss2 else feat2\n",
    "            keep_feat = feat2 if miss1 > miss2 else feat1\n",
    "            reason = f\"missing: {miss1:.1%} vs {miss2:.1%}\"\n",
    "            \n",
    "        elif keep_strategy == 'higher_variance':\n",
    "            var1 = df[feat1].var()\n",
    "            var2 = df[feat2].var()\n",
    "            drop_feat = feat1 if var1 < var2 else feat2\n",
    "            keep_feat = feat2 if var1 < var2 else feat1\n",
    "            reason = f\"variance: {var1:.2f} vs {var2:.2f}\"\n",
    "            \n",
    "        else:  # 'first'\n",
    "            drop_feat = max(feat1, feat2)  # Drop lexicographically later\n",
    "            keep_feat = min(feat1, feat2)\n",
    "            reason = \"alphabetical\"\n",
    "        \n",
    "        to_drop.add(drop_feat)\n",
    "        print(f\"  Drop '{drop_feat}', keep '{keep_feat}' ({reason})\")\n",
    "    \n",
    "    # Drop features\n",
    "    dropped_list = sorted(to_drop)\n",
    "    df_filtered = df.drop(columns=dropped_list)\n",
    "    \n",
    "    print(f\"\\n[Correlation] Dropped {len(dropped_list)} redundant features\")\n",
    "    print(f\"  Remaining features: {len([c for c in df_filtered.columns if c in feature_cols])}\")\n",
    "    \n",
    "    return df_filtered, dropped_list, corr_df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "df_with_features, dropped_features, correlation_pairs = drop_correlated_features(\n",
    "    df_with_features,\n",
    "    threshold=0.85,\n",
    "    keep_strategy='lower_missing'\n",
    ")\n",
    "\n",
    "# Optional: Visualize correlation matrix before/after\n",
    "def plot_correlation_heatmap(df, feature_cols=None, title=\"Feature Correlations\"):\n",
    "    \"\"\"Plot correlation heatmap for visual inspection\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                       if c != 'is_ai']\n",
    "    \n",
    "    corr = df[feature_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "    sns.heatmap(corr, mask=mask, annot=False, cmap='coolwarm', \n",
    "                center=0, vmin=-1, vmax=1, square=True,\n",
    "                linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize before/after (optional)\n",
    "# plot_correlation_heatmap(df_original, title=\"Before Dropping Correlated Features\")\n",
    "# plot_correlation_heatmap(df_with_features, title=\"After Dropping Correlated Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5695e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Strategic Drop] Removing 7 redundant features:\n",
      "\n",
      "  âœ— right_dependency_ratio\n",
      "    â†’ Perfect inverse of left_dependency_ratio\n",
      "\n",
      "  âœ— trigram_entropy\n",
      "    â†’ Redundant with trigram_diversity (0.969 correlation)\n",
      "\n",
      "  âœ— bits_per_char\n",
      "    â†’ Redundant with compression_ratio (0.990 correlation)\n",
      "\n",
      "  âœ— smog_index\n",
      "    â†’ Redundant with flesch_reading_ease (0.937 correlation)\n",
      "\n",
      "  âœ— automated_readability_index\n",
      "    â†’ Redundant with gunning_fog (0.903 correlation)\n",
      "\n",
      "  âœ— unigram_diversity\n",
      "    â†’ Redundant with type_token_ratio (0.908 correlation)\n",
      "\n",
      "  âœ— hapax_type_ratio\n",
      "    â†’ Redundant with hapax_legomena_ratio (0.881 correlation)\n",
      "\n",
      "[Success] No correlations > 0.85 remain\n",
      "\n",
      "[Final] 36 features retained\n"
     ]
    }
   ],
   "source": [
    "# >>>> Tailored to my code\n",
    "\n",
    "def drop_correlated_features_strategic(df: pd.DataFrame, threshold: float = 0.85) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Drop correlated features with domain-informed decisions about which to keep.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Manual decisions based on your correlation output\n",
    "    drops = {\n",
    "        # Perfect/near-perfect duplicates - keep the most interpretable\n",
    "        'right_dependency_ratio': 'Perfect inverse of left_dependency_ratio',\n",
    "        'n_tok': 'Duplicate of n_tokens_ws (0.998 correlation)',\n",
    "        'n_chars': 'Redundant with n_tokens_ws (0.987 correlation)',\n",
    "        \n",
    "        # N-gram entropy/diversity - keep diversity (more interpretable)\n",
    "        'bigram_entropy': 'Redundant with bigram_diversity (0.991 correlation)',\n",
    "        'trigram_entropy': 'Redundant with trigram_diversity (0.969 correlation)',\n",
    "        \n",
    "        # Compression metrics - keep ratio (more standard)\n",
    "        'bits_per_char': 'Redundant with compression_ratio (0.990 correlation)',\n",
    "        \n",
    "        # Readability indices - keep Flesch (most widely used)\n",
    "        'smog_index': 'Redundant with flesch_reading_ease (0.937 correlation)',\n",
    "        'gunning_fog': 'Redundant with flesch_reading_ease (0.934 correlation)',\n",
    "        'automated_readability_index': 'Redundant with gunning_fog (0.903 correlation)',\n",
    "        \n",
    "        # Lexical diversity - keep type_token_ratio (simpler)\n",
    "        'unigram_diversity': 'Redundant with type_token_ratio (0.908 correlation)',\n",
    "        'hapax_type_ratio': 'Redundant with hapax_legomena_ratio (0.881 correlation)',\n",
    "        \n",
    "        # Character/compression overlap - keep char_trigram_diversity\n",
    "        # (compression_ratio already kept, so this reduces triple redundancy)\n",
    "    }\n",
    "    \n",
    "    # Check which features actually exist\n",
    "    existing_drops = {k: v for k, v in drops.items() if k in df.columns}\n",
    "    \n",
    "    print(f\"[Strategic Drop] Removing {len(existing_drops)} redundant features:\\n\")\n",
    "    for feat, reason in existing_drops.items():\n",
    "        print(f\"  âœ— {feat}\")\n",
    "        print(f\"    â†’ {reason}\\n\")\n",
    "    \n",
    "    df_filtered = df.drop(columns=list(existing_drops.keys()))\n",
    "    \n",
    "    # Verify no high correlations remain\n",
    "    feature_cols = [c for c in df_filtered.select_dtypes(include=[np.number]).columns \n",
    "                   if c != 'is_ai']\n",
    "    corr_matrix = df_filtered[feature_cols].corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    remaining_high = []\n",
    "    for col in upper_tri.columns:\n",
    "        high_corr = upper_tri[col][upper_tri[col] > threshold]\n",
    "        if not high_corr.empty:\n",
    "            for idx, val in high_corr.items():\n",
    "                remaining_high.append((col, idx, val))\n",
    "    \n",
    "    if remaining_high:\n",
    "        print(f\"[Warning] {len(remaining_high)} pairs still exceed threshold:\")\n",
    "        for f1, f2, corr in sorted(remaining_high, key=lambda x: x[2], reverse=True)[:5]:\n",
    "            print(f\"  {f1} <-> {f2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"[Success] No correlations > {threshold} remain\")\n",
    "    \n",
    "    print(f\"\\n[Final] {len(feature_cols)} features retained\")\n",
    "    \n",
    "    return df_filtered, existing_drops\n",
    "\n",
    "# Apply strategic dropping\n",
    "df_with_features, drop_log = drop_correlated_features_strategic(df_with_features, threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5581ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quality distribution by source_type:\n",
      "quality_score   0    1    2      3\n",
      "model                             \n",
      "chatgpt         0    2    7   2719\n",
      "cohere          0   10   36   2682\n",
      "cohere-chat     0    2   26   2700\n",
      "gpt2           10   80  452   2186\n",
      "gpt3            0    7   44   2677\n",
      "gpt4            0    0    3   2725\n",
      "human           3   63  396  29538\n",
      "llama-chat      0    1   71   2656\n",
      "mistral         1   54  294   2378\n",
      "mistral-chat    0    6   21   2700\n",
      "mpt            92  339  877   1417\n",
      "mpt-chat       69  212  211   2233\n",
      "Saved enriched dataset: raid_sample_large_with_features_CLEANED.csv (rows: 60000)\n"
     ]
    }
   ],
   "source": [
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_CLEANED.csv\"\n",
    "\n",
    "# Ensure balanced quality across train/test when you split later\n",
    "print(f\"\\nQuality distribution by source_type:\")\n",
    "print(df_with_features.groupby(['model', 'quality_score']).size().unstack(fill_value=0))\n",
    "\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS} (rows: {len(df_with_features)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
