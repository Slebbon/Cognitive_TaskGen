{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded medium dataset with 12000 rows.\n",
      "df_columns: Index(['id', 'adv_source_id', 'source_id', 'model', 'decoding',\n",
      "       'repetition_penalty', 'attack', 'domain', 'title', 'prompt',\n",
      "       'generation', 'is_ai', 'source_type', 'generation_raw', 'had_urls',\n",
      "       'had_html', 'had_code', 'had_table', 'n_chars', 'n_tok', 'alpha_ratio',\n",
      "       'digit_ratio', 'punct_ratio', 'avg_word_length', 'std_word_length',\n",
      "       'entropy_bits', 'entropy_norm', 'is_text_like', 'not_text_reason',\n",
      "       'n_tokens_ws', 'length_bin'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>std_word_length</th>\n",
       "      <th>entropy_bits</th>\n",
       "      <th>entropy_norm</th>\n",
       "      <th>is_text_like</th>\n",
       "      <th>not_text_reason</th>\n",
       "      <th>n_tokens_ws</th>\n",
       "      <th>length_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31dac42c-d620-48ef-9f4e-30a072594c4a</td>\n",
       "      <td>cd90a5fb-6aee-4a78-a32d-e34016718deb</td>\n",
       "      <td>0499bbab-4652-479a-8409-95f5ee077c0a</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>number</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Or From That Sea Of Time</td>\n",
       "      <td>The following is the full text of a poem title...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>1.610466</td>\n",
       "      <td>4.247145</td>\n",
       "      <td>0.828019</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec269ad5-b494-4269-aff4-9d2b4023db56</td>\n",
       "      <td>5890f7ff-7153-49ba-b4f0-11cb0fc502fa</td>\n",
       "      <td>3a482121-a841-4cc5-b071-058785c10525</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>upper_lower</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Petit Fours</td>\n",
       "      <td>Write a recipe for \"Petit Fours\".</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>5.390572</td>\n",
       "      <td>2.193266</td>\n",
       "      <td>4.566332</td>\n",
       "      <td>0.750121</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>313</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86644bab-0ba0-4b90-bba6-344df0e58ea7</td>\n",
       "      <td>e6d6a10d-6a5b-4801-b160-68ec66db6f9c</td>\n",
       "      <td>e16d6eac-e79b-439d-8f6c-673dcebaecad</td>\n",
       "      <td>cohere-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>no</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Formal specification</td>\n",
       "      <td>Write the body of a Wikipedia article titled \"...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.015865</td>\n",
       "      <td>5.599291</td>\n",
       "      <td>3.701504</td>\n",
       "      <td>4.285282</td>\n",
       "      <td>0.763224</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  31dac42c-d620-48ef-9f4e-30a072594c4a  cd90a5fb-6aee-4a78-a32d-e34016718deb   \n",
       "1  ec269ad5-b494-4269-aff4-9d2b4023db56  5890f7ff-7153-49ba-b4f0-11cb0fc502fa   \n",
       "2  86644bab-0ba0-4b90-bba6-344df0e58ea7  e6d6a10d-6a5b-4801-b160-68ec66db6f9c   \n",
       "\n",
       "                              source_id        model  decoding  \\\n",
       "0  0499bbab-4652-479a-8409-95f5ee077c0a         gpt2  sampling   \n",
       "1  3a482121-a841-4cc5-b071-058785c10525     mpt-chat  sampling   \n",
       "2  e16d6eac-e79b-439d-8f6c-673dcebaecad  cohere-chat  sampling   \n",
       "\n",
       "  repetition_penalty            attack   domain                     title  \\\n",
       "0                yes            number   poetry  Or From That Sea Of Time   \n",
       "1                yes       upper_lower  recipes               Petit Fours   \n",
       "2                 no  zero_width_space     wiki      Formal specification   \n",
       "\n",
       "                                              prompt  ... digit_ratio  \\\n",
       "0  The following is the full text of a poem title...  ...    0.000000   \n",
       "1                  Write a recipe for \"Petit Fours\".  ...    0.012588   \n",
       "2  Write the body of a Wikipedia article titled \"...  ...    0.002115   \n",
       "\n",
       "   punct_ratio avg_word_length std_word_length  entropy_bits  entropy_norm  \\\n",
       "0     0.015625        4.080000        1.610466      4.247145      0.828019   \n",
       "1     0.024169        5.390572        2.193266      4.566332      0.750121   \n",
       "2     0.015865        5.599291        3.701504      4.285282      0.763224   \n",
       "\n",
       "   is_text_like  not_text_reason  n_tokens_ws  length_bin  \n",
       "0          True              NaN           49       short  \n",
       "1          True              NaN          313        long  \n",
       "2          True              NaN          280        long  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuration and dataset selection\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths to the three samples\n",
    "SAMPLE_PATHS = {\n",
    "    \"small\":  \"raid_sample_small.csv\",\n",
    "    \"medium\": \"raid_sample_medium.csv\",\n",
    "    \"large\":  \"raid_sample_large.csv\",\n",
    "}\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Set this to one of: \"small\", \"medium\", \"large\"\n",
    "SELECTED_DATASET = \"medium\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "TEXT_COL = \"generation\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATHS[SELECTED_DATASET])\n",
    "print(f\"Loaded {SELECTED_DATASET} dataset with {len(df)} rows.\")\n",
    "print(f\"df_columns: {df.columns}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6afd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy, CMUdict (NLTK), and g2p_en fallback\n",
    "\n",
    "# Ensure CMUdict is available\n",
    "nltk.download('cmudict', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "CMU = cmudict.dict()  # key: lowercase word, value: list of pronunciations (list of ARPAbet tokens)\n",
    "\n",
    "# Load spaCy English model (use 'en_core_web_sm' unless you already have md/lg installed)\n",
    "try:\n",
    "    nlp: Language = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure sentence boundaries are available (parser usually handles this; add sentencizer if needed)\n",
    "if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# g2p_en for OOV coverage\n",
    "G2P = G2p()\n",
    "\n",
    "# ARPAbet vowel bases (stress digits removed when checking)\n",
    "ARPA_VOWELS = {\n",
    "    \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
    "    \"EH\", \"ER\", \"EY\",\n",
    "    \"IH\", \"IY\",\n",
    "    \"OW\", \"OY\",\n",
    "    \"UH\", \"UW\"\n",
    "}\n",
    "\n",
    "# Cache syllable counts for speed\n",
    "_SYLL_CACHE: Dict[str, int] = {}\n",
    "\n",
    "def cmu_syllables(word: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Returns syllable count using CMUdict if available; else None.\n",
    "    Policy: use the first pronunciation variant.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w not in CMU:\n",
    "        return None\n",
    "    phones = CMU[w][0]\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    return max(count, 1)  # at least one for non-empty alphabetic words\n",
    "\n",
    "def g2p_syllables(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns syllable count using neural g2p_en; counts vowel phonemes.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w in _SYLL_CACHE:\n",
    "        return _SYLL_CACHE[w]\n",
    "    phones = G2P(w)\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    # Guard: ensure >=1 for alphabetic tokens\n",
    "    if count == 0 and re.search(r\"[A-Za-z]\", w):\n",
    "        count = 1\n",
    "    _SYLL_CACHE[w] = count\n",
    "    return count\n",
    "\n",
    "def syllables_hybrid(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Hybrid policy: try CMUdict first; if OOV, fall back to g2p_en.\n",
    "    \"\"\"\n",
    "    c = cmu_syllables(word)\n",
    "    if c is not None:\n",
    "        return c\n",
    "    return g2p_syllables(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3020ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation utilities using spaCy + CMUdict with g2p_en fallback\n",
    "\n",
    "import zlib\n",
    "\n",
    "# permissive fallback segmentation for pathological cases\n",
    "_FALLBACK_SPLIT = re.compile(r'(?<=[\\.!?])\\s+|[\\r\\n]+|(?<=;)\\s+|(?<=:)\\s+')\n",
    "\n",
    "def resegmentize_if_needed(text: str, nlp: Language, asl_hi: int = 100, min_tokens: int = 120, doc=None):\n",
    "    \"\"\"Return fallback sentence strings when avg sentence length is extreme.\"\"\"\n",
    "    doc = doc or nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    n_sent = len(sents)\n",
    "    n_tok = len(doc)\n",
    "    avg_len = (n_tok / max(n_sent, 1)) if n_tok else 0\n",
    "\n",
    "    if (n_tok >= min_tokens) and (n_sent <= 2 or avg_len >= asl_hi):\n",
    "        parts = [s.strip() for s in _FALLBACK_SPLIT.split(text) if s and not s.isspace()]\n",
    "        return parts\n",
    "    return None\n",
    "\n",
    "def prepare_doc(text: str, nlp: Language, skip_reseg: bool = False) -> Tuple[Doc, bool]:\n",
    "    \"\"\"\n",
    "    Parse text with spaCy and optionally resegment when heuristics trigger.\n",
    "    Set skip_reseg=True when you need accurate dependency trees.\n",
    "    \"\"\"\n",
    "    primary_doc = nlp(text)\n",
    "    \n",
    "    if skip_reseg:\n",
    "        return primary_doc, False\n",
    "    \n",
    "    fallback = resegmentize_if_needed(text, nlp, doc=primary_doc)\n",
    "    if fallback is None:\n",
    "        return primary_doc, False\n",
    "    \n",
    "    fixed_text = \". \".join(fallback)\n",
    "    return nlp(fixed_text), True\n",
    "\n",
    "def can_compute_readability(n_tokens: int, n_sents: int) -> bool:\n",
    "    return (n_tokens >= 100) and (n_sents >= 3)\n",
    "\n",
    "def safe_readability(tokens: int, sentences: int, syllable_counts: List[int], chars_per_word: float, complex_words: int, polysyllables: int) -> Dict[str, float]:\n",
    "    \"\"\"Safely compute readability metrics, falling back to NaN when unstable.\"\"\"\n",
    "    out = {\n",
    "        \"flesch_reading_ease\": np.nan,\n",
    "        \"gunning_fog\": np.nan,\n",
    "        \"smog_index\": np.nan,\n",
    "        \"automated_readability_index\": np.nan,\n",
    "    }\n",
    "    if not can_compute_readability(tokens, sentences):\n",
    "        return out\n",
    "\n",
    "    words = max(tokens, 1)\n",
    "    sents = max(sentences, 1)\n",
    "    syllables = max(int(np.sum(syllable_counts)), 1)\n",
    "    chars_per_word = float(chars_per_word) if chars_per_word else 0.0\n",
    "    complex_words = max(complex_words, 0)\n",
    "    polysyllables = max(polysyllables, 0)\n",
    "\n",
    "    out[\"flesch_reading_ease\"] = 206.835 - 1.015 * (words / sents) - 84.6 * (syllables / words)\n",
    "    out[\"gunning_fog\"] = 0.4 * ((words / sents) + 100.0 * (complex_words / words))\n",
    "    out[\"smog_index\"] = (1.043 * math.sqrt(30.0 * (polysyllables / sents)) + 3.1291) if polysyllables > 0 else np.nan\n",
    "    out[\"automated_readability_index\"] = 4.71 * chars_per_word + 0.5 * (words / sents) - 21.43\n",
    "    return out\n",
    "\n",
    "def _word_like(tok) -> bool:\n",
    "    \"\"\"Select lexical tokens (alphabetic, not space).\"\"\"\n",
    "    return tok.is_alpha and not tok.is_space\n",
    "\n",
    "def _alnum_char_count(token_text: str) -> int:\n",
    "    \"\"\"Count alphanumeric characters for ARI; excludes whitespace and punctuation.\"\"\"\n",
    "    return sum(ch.isalnum() for ch in token_text)\n",
    "\n",
    "def features_from_doc(doc: Doc, text: str, *, resegmented: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"Compute core lexical and readability-driven metrics with guards.\"\"\"\n",
    "    if doc.has_annotation(\"SENT_START\"):\n",
    "        sents = list(doc.sents)\n",
    "    else:\n",
    "        sents = list(doc.sents) if hasattr(doc, \"sents\") else [doc]\n",
    "    if not sents:\n",
    "        sents = [doc]\n",
    "    n_sents = len(sents)\n",
    "\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    punct_toks = [t for t in doc if t.is_punct]\n",
    "    nonspace_toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "    n_tokens = len(word_toks)\n",
    "    sent_word_counts = [sum(1 for t in sent if _word_like(t)) for sent in sents]\n",
    "    avg_sentence_length = float(np.mean(sent_word_counts)) if sent_word_counts else np.nan\n",
    "    sentence_length_std = float(np.std(sent_word_counts, ddof=0)) if len(sent_word_counts) > 1 else np.nan\n",
    "\n",
    "    word_lengths = [len(t.text) for t in word_toks] \n",
    "    avg_word_length = float(np.mean(word_lengths)) if word_lengths else np.nan\n",
    "\n",
    "    vocab = {t.text.lower() for t in word_toks}\n",
    "    type_token_ratio = (len(vocab) / n_tokens) if n_tokens > 0 else np.nan\n",
    "\n",
    "    stop_count = sum(1 for t in word_toks if t.is_stop)\n",
    "    stopword_ratio = (stop_count / n_tokens) if n_tokens > 0 else np.nan\n",
    "\n",
    "    chars_alnum = sum(_alnum_char_count(t.text) for t in nonspace_toks)\n",
    "    punct_chars = sum(len(t.text) for t in punct_toks)\n",
    "    nonspace_chars = sum(len(t.text) for t in nonspace_toks)\n",
    "    punctuation_ratio = (punct_chars / nonspace_chars) if nonspace_chars > 0 else np.nan\n",
    "\n",
    "    syllable_counts = [syllables_hybrid(t.text) for t in word_toks]\n",
    "    polysyllables = sum(1 for syl in syllable_counts if syl >= 3)\n",
    "    complex_words = polysyllables\n",
    "    chars_per_word = (chars_alnum / n_tokens) if n_tokens > 0 else 0.0\n",
    "\n",
    "    readability = safe_readability(\n",
    "        tokens=n_tokens,\n",
    "        sentences=n_sents,\n",
    "        syllable_counts=syllable_counts,\n",
    "        chars_per_word=chars_per_word,\n",
    "        complex_words=complex_words,\n",
    "        polysyllables=polysyllables,\n",
    "    )\n",
    "\n",
    "    features = {\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"punctuation_ratio\": punctuation_ratio,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"sentence_length_std\": sentence_length_std,\n",
    "        \"n_tokens_doc\": float(n_tokens),\n",
    "        \"n_sentences_doc\": float(n_sents),\n",
    "        \"resegmented\": bool(resegmented),\n",
    "    }\n",
    "    features.update(readability)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74acf093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram and burstiness utility functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# N-gram feature extraction utilities\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def ngram_diversity(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate n-gram diversity (unique n-grams / total n-grams).\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def ngram_entropy(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of n-gram distribution.\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_burstiness(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate burstiness coefficient based on word frequency distribution.\n",
    "    Burstiness = (sigma - mu) / (sigma + mu)\n",
    "    where mu is mean frequency and sigma is standard deviation.\n",
    "    Returns NaN if statistics are undefined.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "\n",
    "    word_counts = Counter(tokens)\n",
    "    frequencies = list(word_counts.values())\n",
    "\n",
    "    if len(frequencies) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(frequencies)\n",
    "    sigma = np.std(frequencies, ddof=0)\n",
    "\n",
    "    if mu + sigma == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return (sigma - mu) / (sigma + mu)\n",
    "\n",
    "def safe_ngram_stats(tokens: List[str], n: int = 2, min_ngrams: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Return diversity and entropy for n-grams when sample size is sufficient.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return {\"diversity\": np.nan, \"entropy\": np.nan}\n",
    "\n",
    "    grams = extract_ngrams(tokens, n)\n",
    "    if len(grams) < min_ngrams:\n",
    "        return {\"diversity\": np.nan, \"entropy\": np.nan}\n",
    "\n",
    "    counts = Counter(grams)\n",
    "    diversity = len(counts) / len(grams)\n",
    "    probs = np.array(list(counts.values()), dtype=float) / len(grams)\n",
    "    entropy = float(-np.sum(probs * np.log2(probs)))\n",
    "    return {\"diversity\": diversity, \"entropy\": entropy}\n",
    "\n",
    "print(\"N-gram and burstiness utility functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9638c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Character-level feature extraction\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def character_ngram_features(text: str, n: int = 3) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract character n-gram diversity and entropy.\n",
    "    Returns diversity ratio and entropy for character n-grams.\n",
    "    \"\"\"\n",
    "    if len(text) < n:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    char_ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "    if not char_ngrams:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    diversity = len(set(char_ngrams)) / len(char_ngrams)\n",
    "\n",
    "    counts = Counter(char_ngrams)\n",
    "    total = len(char_ngrams)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "\n",
    "    return diversity, entropy\n",
    "\n",
    "def compression_features(text: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute compression ratio and bits-per-character using zlib.\"\"\"\n",
    "    encoded = text.encode(\"utf-8\")\n",
    "    raw_len = len(encoded)\n",
    "    if raw_len == 0:\n",
    "        return {\"compression_ratio\": np.nan, \"bits_per_char\": np.nan}\n",
    "\n",
    "    compressed = zlib.compress(encoded, level=6)\n",
    "    ratio = len(compressed) / raw_len\n",
    "    bits_per_char = 8.0 * len(compressed) / raw_len\n",
    "    return {\"compression_ratio\": ratio, \"bits_per_char\": bits_per_char}\n",
    "\n",
    "def compression_ratio(text: str) -> float:\n",
    "    \"\"\"Backward-compatible helper returning only the compression ratio.\"\"\"\n",
    "    return compression_features(text)[\"compression_ratio\"]\n",
    "\n",
    "def character_statistics(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract surface-level character statistics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            \"uppercase_ratio\": np.nan,\n",
    "            \"digit_ratio\": np.nan,\n",
    "            \"whitespace_ratio\": np.nan,\n",
    "            \"unique_char_count\": np.nan,\n",
    "        }\n",
    "\n",
    "    total_chars = len(text)\n",
    "\n",
    "    return {\n",
    "        \"uppercase_ratio\": sum(1 for c in text if c.isupper()) / total_chars,\n",
    "        \"digit_ratio\": sum(1 for c in text if c.isdigit()) / total_chars,\n",
    "        \"whitespace_ratio\": sum(1 for c in text if c.isspace()) / total_chars,\n",
    "        \"unique_char_count\": float(len(set(text))),\n",
    "    }\n",
    "\n",
    "print(\"Character-level feature functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74b9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc, Token\n",
    "\n",
    "def _root_chain_depth(token: Token, max_steps: int) -> int:\n",
    "    \"\"\"\n",
    "    Length of the head chain from `token` up to the sentence/doc root.\n",
    "    Robust to cycles and malformed heads by bounding steps and tracking indices.\n",
    "    \"\"\"\n",
    "    depth = 0\n",
    "    cur = token\n",
    "    visited_idx = set()\n",
    "\n",
    "    # Use token indices (doc-relative) for identity; this is stable.\n",
    "    while cur.head.i != cur.i:\n",
    "        if cur.i in visited_idx:\n",
    "            # Cycle detected: bail out with 0 depth for this token\n",
    "            return 0\n",
    "        visited_idx.add(cur.i)\n",
    "\n",
    "        depth += 1\n",
    "        if depth > max_steps:\n",
    "            # Malformed graph (excessive chain): cap and exit\n",
    "            return max_steps\n",
    "\n",
    "        cur = cur.head\n",
    "    return depth\n",
    "\n",
    "def sent_max_depth(sent) -> int:\n",
    "    \"\"\"\n",
    "    Longest head-chain root->leaf depth within a sentence span.\n",
    "    Traversal is over the full doc-level heads; we only *measure* per sentence.\n",
    "    \"\"\"\n",
    "    if len(sent) == 0:\n",
    "        return 0\n",
    "\n",
    "    # A conservative upper bound: number of tokens in the *doc*\n",
    "    # (not just in the sentence), to safely handle cross-sentence heads.\n",
    "    max_steps = len(sent.doc) + 5\n",
    "\n",
    "    return max((_root_chain_depth(tok, max_steps) for tok in sent), default=0)\n",
    "\n",
    "def doc_depth_stats(doc: Doc) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Average and maximum tree depth across sentences in the original Doc.\n",
    "    Falls back gracefully for empty docs.\n",
    "    \"\"\"\n",
    "    sentences = list(doc.sents) if doc.has_annotation(\"SENT_START\") or hasattr(doc, \"sents\") else [doc]\n",
    "    if not sentences:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    depths = [sent_max_depth(sent) for sent in sentences if len(sent) > 0]\n",
    "    if not depths:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    return float(np.mean(depths)), float(np.max(depths))\n",
    "\n",
    "def dependency_tree_features(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract robust dependency-tree structural features from the original Doc.\n",
    "    Avoids Span.as_doc(), uses doc-level indices and bounds traversal.\n",
    "    \"\"\"\n",
    "    sentences = list(doc.sents) if doc.has_annotation(\"SENT_START\") or hasattr(doc, \"sents\") else [doc]\n",
    "    if not sentences:\n",
    "        sentences = [doc]\n",
    "\n",
    "    avg_depth, max_depth = doc_depth_stats(doc)\n",
    "\n",
    "    per_sentence_distances = []\n",
    "    left_deps = 0\n",
    "    right_deps = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        distances = []\n",
    "        for token in sent:\n",
    "            # Skip roots\n",
    "            if token.head.i == token.i:\n",
    "                continue\n",
    "            # Distance computed in doc coordinates is fine:\n",
    "            d = abs(token.i - token.head.i)\n",
    "            distances.append(d)\n",
    "            if token.i < token.head.i:\n",
    "                left_deps += 1\n",
    "            else:\n",
    "                right_deps += 1\n",
    "        if distances:\n",
    "            per_sentence_distances.append(float(np.mean(distances)))\n",
    "\n",
    "    avg_dep_distance = float(np.mean(per_sentence_distances)) if per_sentence_distances else 0.0\n",
    "    total_deps = left_deps + right_deps\n",
    "    left_ratio = (left_deps / total_deps) if total_deps else 0.0\n",
    "    right_ratio = (right_deps / total_deps) if total_deps else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_tree_depth\": avg_depth,\n",
    "        \"max_tree_depth\": max_depth,\n",
    "        \"avg_dependency_distance\": avg_dep_distance,\n",
    "        \"left_dependency_ratio\": left_ratio,\n",
    "        \"right_dependency_ratio\": right_ratio,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89de6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Optional\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# Token normalization helpers\n",
    "# ---------------------------\n",
    "\n",
    "def _normalize_tokens(\n",
    "    tokens: Iterable,\n",
    "    *,\n",
    "    lower: bool = True,\n",
    "    alpha_only: bool = True,\n",
    "    min_len: int = 2,\n",
    "    use_lemma: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize a sequence of tokens (strings or spaCy tokens).\n",
    "    - lower: lowercases\n",
    "    - alpha_only: keep tokens with .isalpha() (falls back to regex if string)\n",
    "    - min_len: drop tokens shorter than this after normalization\n",
    "    - use_lemma: if spaCy tokens are provided, use token.lemma_\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        s = t\n",
    "        # spaCy Token support\n",
    "        if hasattr(t, \"lemma_\") and use_lemma:\n",
    "            s = t.lemma_\n",
    "        elif hasattr(t, \"text\"):\n",
    "            s = t.text\n",
    "\n",
    "        if not isinstance(s, str):\n",
    "            s = str(s)\n",
    "\n",
    "        if lower:\n",
    "            s = s.lower()\n",
    "\n",
    "        if alpha_only:\n",
    "            # use spaCy attribute if available, else a regex fallback\n",
    "            if hasattr(t, \"is_alpha\"):\n",
    "                if not t.is_alpha:\n",
    "                    continue\n",
    "            else:\n",
    "                if not re.match(r\"^[a-zA-Z]+$\", s):\n",
    "                    continue\n",
    "\n",
    "        if len(s) < min_len:\n",
    "            continue\n",
    "\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Hapax ratio (stable variant)\n",
    "# ---------------------------\n",
    "\n",
    "def hapax_legomena_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Ratio of hapax tokens to TOTAL TOKENS (your original definition).\n",
    "    More common in lexicography is hapax / types; we keep your denominator,\n",
    "    but you may prefer hapax / unique_types for length-robustness.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(tokens)\n",
    "    hapax = sum(1 for c in cnt.values() if c == 1)\n",
    "    return hapax / float(len(tokens))\n",
    "\n",
    "\n",
    "def hapax_type_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Optional: hapax to TYPES ratio (often more stable than hapax/token).\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(tokens)\n",
    "    types = len(cnt)\n",
    "    if types == 0:\n",
    "        return float(\"nan\")\n",
    "    hapax = sum(1 for c in cnt.values() if c == 1)\n",
    "    return hapax / float(types)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Yule's K (guarded)\n",
    "# ---------------------------\n",
    "\n",
    "def yules_k(tokens: List[str], *, min_tokens: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Yule's K = 10^4 * (sum v^2 * V_v - N) / N^2\n",
    "    Guarded against short texts; returns NaN if N < min_tokens.\n",
    "    \"\"\"\n",
    "    N = len(tokens)\n",
    "    if N < min_tokens:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    cnt = Counter(tokens)\n",
    "    spectrum = Counter(cnt.values())  # V_v\n",
    "    # sum v^2 * V_v\n",
    "    s2 = sum((v * v) * Vv for v, Vv in spectrum.items())\n",
    "\n",
    "    # use float64\n",
    "    Nf = float(N)\n",
    "    K = 10000.0 * (s2 - Nf) / (Nf * Nf)\n",
    "    return float(K)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MTLD (standard, robust)\n",
    "# ---------------------------\n",
    "\n",
    "def _mtld_one_pass(tokens: List[str], threshold: float, min_segment: int) -> float:\n",
    "    \"\"\"\n",
    "    One-direction MTLD pass (standard algorithm):\n",
    "    Accumulate a segment until TTR falls below threshold; count a factor and reset.\n",
    "    The final partial segment contributes a fractional factor.\n",
    "    Returns the number of factors observed.\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    token_count = 0\n",
    "    factor_count = 0.0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / float(token_count)\n",
    "\n",
    "        # Only allow a factor to close if we have a minimally meaningful segment\n",
    "        if (ttr < threshold) and (token_count >= min_segment):\n",
    "            factor_count += 1.0\n",
    "            types.clear()\n",
    "            token_count = 0\n",
    "\n",
    "    # partial segment contribution\n",
    "    if token_count > 0:\n",
    "        # If ttr is already above threshold, this adds <1 factor,\n",
    "        # otherwise adds a smaller fraction.\n",
    "        ttr = len(types) / float(token_count)\n",
    "        if ttr != 1.0:  # avoid division by zero in degenerate case\n",
    "            factor_count += (1.0 - ttr) / (1.0 - threshold)\n",
    "        else:\n",
    "            # maximally diverse partial segment: count a tiny fraction\n",
    "            factor_count += 0.0\n",
    "\n",
    "    return factor_count\n",
    "\n",
    "\n",
    "def mtld(\n",
    "    tokens: List[str],\n",
    "    threshold: float = 0.72,\n",
    "    *,\n",
    "    min_tokens: int = 200,\n",
    "    min_segment: int = 50\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Measure of Textual Lexical Diversity (MTLD), forward/backward average.\n",
    "    Guard-rails:\n",
    "      - require at least `min_tokens` tokens, else NaN\n",
    "      - clamp threshold to [0.60, 0.80]\n",
    "      - enforce `min_segment` for factor completion\n",
    "    \"\"\"\n",
    "    n = len(tokens)\n",
    "    if n < min_tokens:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    threshold = max(0.60, min(0.80, float(threshold)))\n",
    "\n",
    "    f = _mtld_one_pass(tokens, threshold, min_segment)\n",
    "    b = _mtld_one_pass(list(reversed(tokens)), threshold, min_segment)\n",
    "\n",
    "    # If both are zero (pathological), return NaN rather than n/0\n",
    "    vals = [x for x in (f, b) if x > 0.0]\n",
    "    if not vals:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    mean_factors = float(np.mean(vals))\n",
    "    return n / mean_factors\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregator with normalization\n",
    "# ---------------------------\n",
    "\n",
    "def vocabulary_sophistication_features(\n",
    "    tokens: List,\n",
    "    *,\n",
    "    normalize: str = \"lower\",   # {\"none\",\"lower\",\"lemma\"}\n",
    "    alpha_only: bool = True,\n",
    "    min_len: int = 2,\n",
    "    use_lemma_if_spacy: Optional[bool] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - Accepts raw strings or spaCy Tokens.\n",
    "    - Normalizes then computes robust metrics with NaN for undersized texts.\n",
    "    - Adds hapax/type as a more length-stable complement (does not replace your original).\n",
    "    \"\"\"\n",
    "    if normalize not in {\"none\", \"lower\", \"lemma\"}:\n",
    "        raise ValueError(\"normalize must be one of {'none','lower','lemma'}\")\n",
    "\n",
    "    if use_lemma_if_spacy is None:\n",
    "        use_lemma_if_spacy = (normalize == \"lemma\")\n",
    "\n",
    "    toks = _normalize_tokens(\n",
    "        tokens,\n",
    "        lower=(normalize == \"lower\"),\n",
    "        alpha_only=alpha_only,\n",
    "        min_len=min_len,\n",
    "        use_lemma=use_lemma_if_spacy\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"hapax_legomena_ratio\": hapax_legomena_ratio(toks),\n",
    "        \"hapax_type_ratio\":     hapax_type_ratio(toks),\n",
    "        \"yules_k\":              yules_k(toks, min_tokens=100),\n",
    "        \"mtld\":                 mtld(toks, threshold=0.72, min_tokens=200, min_segment=50),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dfb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation pattern functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Punctuation pattern analysis\n",
    "\n",
    "def punctuation_patterns(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detailed punctuation pattern features beyond simple ratio.\n",
    "    \"\"\"\n",
    "    all_tokens = [t for t in doc if not t.is_space]\n",
    "    punct_tokens = [t for t in doc if t.is_punct]\n",
    "    \n",
    "    if not all_tokens:\n",
    "        return {\n",
    "            \"comma_ratio\": 0.0,\n",
    "            \"period_ratio\": 0.0,\n",
    "            \"question_ratio\": 0.0,\n",
    "            \"exclamation_ratio\": 0.0,\n",
    "            \"semicolon_ratio\": 0.0,\n",
    "            \"colon_ratio\": 0.0,\n",
    "            \"quote_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total = len(all_tokens)\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punct_text = ''.join([t.text for t in punct_tokens])\n",
    "    \n",
    "    return {\n",
    "        \"comma_ratio\": punct_text.count(',') / total,\n",
    "        \"period_ratio\": punct_text.count('.') / total,\n",
    "        \"question_ratio\": punct_text.count('?') / total,\n",
    "        \"exclamation_ratio\": punct_text.count('!') / total,\n",
    "        \"semicolon_ratio\": punct_text.count(';') / total,\n",
    "        \"colon_ratio\": punct_text.count(':') / total,\n",
    "        \"quote_ratio\": (punct_text.count('\"') + punct_text.count(\"'\")) / total,\n",
    "    }\n",
    "\n",
    "print(\"Punctuation pattern functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_features(text: str, doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract sentiment and emotional tone features.\n",
    "    Uses TextBlob for polarity and subjectivity.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"sentiment_subjectivity\": 0.0,\n",
    "            \"sentiment_polarity_variance\": 0.0,\n",
    "            \"positive_word_ratio\": 0.0,\n",
    "            \"negative_word_ratio\": 0.0,\n",
    "            \"neutral_sentence_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Overall document sentiment\n",
    "    blob = TextBlob(text)\n",
    "    features = {\n",
    "        \"sentiment_polarity\": blob.sentiment.polarity,  # -1 (negative) to 1 (positive)\n",
    "        \"sentiment_subjectivity\": blob.sentiment.subjectivity,  # 0 (objective) to 1 (subjective)\n",
    "    }\n",
    "    \n",
    "    # Sentence-level sentiment variance\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    sent_polarities = []\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_blob = TextBlob(sent.text)\n",
    "        polarity = sent_blob.sentiment.polarity\n",
    "        sent_polarities.append(polarity)\n",
    "        \n",
    "        # Count neutral sentences (polarity close to 0)\n",
    "        if abs(polarity) < 0.1:\n",
    "            neutral_count += 1\n",
    "    \n",
    "    features[\"sentiment_polarity_variance\"] = float(np.var(sent_polarities)) if len(sent_polarities) > 1 else 0.0\n",
    "    features[\"neutral_sentence_ratio\"] = neutral_count / len(sents) if sents else 0.0\n",
    "    \n",
    "    # Positive/negative word ratios using spaCy tokens\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    if word_toks:\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for token in word_toks:\n",
    "            word_blob = TextBlob(token.text.lower())\n",
    "            polarity = word_blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                positive_count += 1\n",
    "            elif polarity < -0.1:\n",
    "                negative_count += 1\n",
    "        \n",
    "        features[\"positive_word_ratio\"] = positive_count / len(word_toks)\n",
    "        features[\"negative_word_ratio\"] = negative_count / len(word_toks)\n",
    "    else:\n",
    "        features[\"positive_word_ratio\"] = 0.0\n",
    "        features[\"negative_word_ratio\"] = 0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1f44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature columns:\n",
      "['avg_word_length', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'digit_ratio', 'whitespace_ratio', 'unique_char_count', 'compression_ratio', 'bits_per_char', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'hapax_type_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio', 'had_urls', 'had_html', 'had_code', 'had_table']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <th>had_urls</th>\n",
       "      <th>had_html</th>\n",
       "      <th>had_code</th>\n",
       "      <th>had_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31dac42c-d620-48ef-9f4e-30a072594c4a</td>\n",
       "      <td>cd90a5fb-6aee-4a78-a32d-e34016718deb</td>\n",
       "      <td>0499bbab-4652-479a-8409-95f5ee077c0a</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>number</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Or From That Sea Of Time</td>\n",
       "      <td>The following is the full text of a poem title...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec269ad5-b494-4269-aff4-9d2b4023db56</td>\n",
       "      <td>5890f7ff-7153-49ba-b4f0-11cb0fc502fa</td>\n",
       "      <td>3a482121-a841-4cc5-b071-058785c10525</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>upper_lower</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Petit Fours</td>\n",
       "      <td>Write a recipe for \"Petit Fours\".</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187625</td>\n",
       "      <td>0.533749</td>\n",
       "      <td>0.021284</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>0.024138</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86644bab-0ba0-4b90-bba6-344df0e58ea7</td>\n",
       "      <td>e6d6a10d-6a5b-4801-b160-68ec66db6f9c</td>\n",
       "      <td>e16d6eac-e79b-439d-8f6c-673dcebaecad</td>\n",
       "      <td>cohere-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>no</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Formal specification</td>\n",
       "      <td>Write the body of a Wikipedia article titled \"...</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  31dac42c-d620-48ef-9f4e-30a072594c4a  cd90a5fb-6aee-4a78-a32d-e34016718deb   \n",
       "1  ec269ad5-b494-4269-aff4-9d2b4023db56  5890f7ff-7153-49ba-b4f0-11cb0fc502fa   \n",
       "2  86644bab-0ba0-4b90-bba6-344df0e58ea7  e6d6a10d-6a5b-4801-b160-68ec66db6f9c   \n",
       "\n",
       "                              source_id        model  decoding  \\\n",
       "0  0499bbab-4652-479a-8409-95f5ee077c0a         gpt2  sampling   \n",
       "1  3a482121-a841-4cc5-b071-058785c10525     mpt-chat  sampling   \n",
       "2  e16d6eac-e79b-439d-8f6c-673dcebaecad  cohere-chat  sampling   \n",
       "\n",
       "  repetition_penalty            attack   domain                     title  \\\n",
       "0                yes            number   poetry  Or From That Sea Of Time   \n",
       "1                yes       upper_lower  recipes               Petit Fours   \n",
       "2                 no  zero_width_space     wiki      Formal specification   \n",
       "\n",
       "                                              prompt  ... sentiment_polarity  \\\n",
       "0  The following is the full text of a poem title...  ...           0.450000   \n",
       "1                  Write a recipe for \"Petit Fours\".  ...           0.187625   \n",
       "2  Write the body of a Wikipedia article titled \"...  ...          -0.008333   \n",
       "\n",
       "   sentiment_subjectivity sentiment_polarity_variance neutral_sentence_ratio  \\\n",
       "0                0.650000                    0.050625               0.500000   \n",
       "1                0.533749                    0.021284               0.454545   \n",
       "2                0.287500                    0.013217               0.636364   \n",
       "\n",
       "   positive_word_ratio  negative_word_ratio  had_urls  had_html  had_code  \\\n",
       "0             0.040000             0.000000     False     False     False   \n",
       "1             0.072414             0.024138     False     False     False   \n",
       "2             0.021429             0.021429     False     False     False   \n",
       "\n",
       "   had_table  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "\n",
       "[3 rows x 85 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application doc processing with segmentation safeguards\n",
    "\n",
    "def window_tokens(doc: Doc, max_tokens: int = 500) -> List[str]:\n",
    "    \"\"\"Lowercase alphabetic tokens clipped to a comparison window.\"\"\"\n",
    "    return [t.text.lower() for t in doc if _word_like(t)][:max_tokens]\n",
    "\n",
    "feature_rows: List[Dict[str, float]] = []\n",
    "for idx, row in df.reset_index(drop=True).iterrows():\n",
    "    text = str(row.get(TEXT_COL, \"\"))\n",
    "    \n",
    "    # Use resegmented doc for readability and most features\n",
    "    doc, resegmented = prepare_doc(text, nlp)\n",
    "    doc_features = dict(features_from_doc(doc, text, resegmented=resegmented))\n",
    "\n",
    "    tok_win = window_tokens(doc, max_tokens=500)\n",
    "    unigram = safe_ngram_stats(tok_win, n=1, min_ngrams=100)\n",
    "    bigram = safe_ngram_stats(tok_win, n=2, min_ngrams=100)\n",
    "    trigram = safe_ngram_stats(tok_win, n=3, min_ngrams=100)\n",
    "    doc_features[\"unigram_diversity\"] = unigram[\"diversity\"]\n",
    "    doc_features[\"bigram_diversity\"] = bigram[\"diversity\"]\n",
    "    doc_features[\"trigram_diversity\"] = trigram[\"diversity\"]\n",
    "    doc_features[\"bigram_entropy\"] = bigram[\"entropy\"]\n",
    "    doc_features[\"trigram_entropy\"] = trigram[\"entropy\"]\n",
    "    doc_features[\"token_burstiness\"] = calculate_burstiness(tok_win) if len(tok_win) >= 2 else np.nan\n",
    "\n",
    "    char_diversity, char_entropy = character_ngram_features(text, n=3)\n",
    "    doc_features[\"char_trigram_diversity\"] = char_diversity\n",
    "    doc_features[\"char_trigram_entropy\"] = char_entropy\n",
    "    doc_features.update(character_statistics(text))\n",
    "    doc_features.update(compression_features(text))\n",
    "\n",
    "    # CRITICAL: Use clean parse for dependency features to avoid resegmentation artifacts\n",
    "    doc_clean, _ = prepare_doc(text, nlp, skip_reseg=True)\n",
    "    dep_feats = dependency_tree_features(doc_clean)\n",
    "    doc_features.update(dep_feats)\n",
    "    max_tree_depth = dep_feats.get(\"max_tree_depth\")\n",
    "    try:\n",
    "        depth_nan = math.isnan(max_tree_depth)\n",
    "    except (TypeError, ValueError):\n",
    "        depth_nan = False\n",
    "    depth_ok = depth_nan or max_tree_depth is None\n",
    "    if not depth_ok:\n",
    "        try:\n",
    "            depth_ok = max_tree_depth <= 50\n",
    "        except TypeError:\n",
    "            depth_ok = False\n",
    "    doc_features[\"depth_check_passed\"] = bool(depth_ok)\n",
    "\n",
    "    vocab_feats = vocabulary_sophistication_features(tok_win)\n",
    "    doc_features.update(vocab_feats)\n",
    "\n",
    "    doc_features.update(punctuation_patterns(doc))\n",
    "    doc_features.update(sentiment_features(text, doc))\n",
    "\n",
    "    for flag in [\"had_urls\", \"had_html\", \"had_code\", \"had_table\"]:\n",
    "        if flag in row.index:\n",
    "            doc_features[flag] = row[flag]\n",
    "\n",
    "    feature_rows.append(doc_features)\n",
    "\n",
    "feat_df = pd.DataFrame(feature_rows)\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Computed feature columns:\")\n",
    "print(list(feat_df.columns))\n",
    "df_with_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c5cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched dataset: raid_sample_medium_with_features_PREPOS.csv  (rows: 12000)\n",
      "Feature columns saved (48 total):\n",
      "['type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'whitespace_ratio', 'unique_char_count', 'compression_ratio', 'bits_per_char', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'hapax_type_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.145105</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.449778</td>\n",
       "      <td>0.581017</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopword_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.441644</td>\n",
       "      <td>0.167728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220585</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.609045</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.012669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016787</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>0.045329</td>\n",
       "      <td>0.229457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>22.773167</td>\n",
       "      <td>22.138380</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.562500</td>\n",
       "      <td>30.166667</td>\n",
       "      <td>427.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_length_std</th>\n",
       "      <td>11855.0</td>\n",
       "      <td>10.331872</td>\n",
       "      <td>11.410634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.230090</td>\n",
       "      <td>7.945998</td>\n",
       "      <td>16.084905</td>\n",
       "      <td>234.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_doc</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>253.744750</td>\n",
       "      <td>258.117404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>395.000000</td>\n",
       "      <td>12117.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_sentences_doc</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>13.583083</td>\n",
       "      <td>13.537794</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>801.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>10975.0</td>\n",
       "      <td>54.753926</td>\n",
       "      <td>24.490668</td>\n",
       "      <td>-151.824406</td>\n",
       "      <td>21.818801</td>\n",
       "      <td>58.642977</td>\n",
       "      <td>82.408221</td>\n",
       "      <td>130.124032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>10975.0</td>\n",
       "      <td>13.697227</td>\n",
       "      <td>5.771072</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>7.427190</td>\n",
       "      <td>13.018447</td>\n",
       "      <td>20.446632</td>\n",
       "      <td>73.726126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>10921.0</td>\n",
       "      <td>12.162749</td>\n",
       "      <td>3.998614</td>\n",
       "      <td>3.843193</td>\n",
       "      <td>7.492282</td>\n",
       "      <td>11.855464</td>\n",
       "      <td>17.122413</td>\n",
       "      <td>49.539670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>10975.0</td>\n",
       "      <td>11.878375</td>\n",
       "      <td>6.294377</td>\n",
       "      <td>-5.647825</td>\n",
       "      <td>5.765274</td>\n",
       "      <td>11.054197</td>\n",
       "      <td>18.041672</td>\n",
       "      <td>82.172177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_diversity</th>\n",
       "      <td>11127.0</td>\n",
       "      <td>0.577866</td>\n",
       "      <td>0.135299</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.454804</td>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_diversity</th>\n",
       "      <td>11106.0</td>\n",
       "      <td>0.894522</td>\n",
       "      <td>0.136156</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.823151</td>\n",
       "      <td>0.924314</td>\n",
       "      <td>0.982175</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_diversity</th>\n",
       "      <td>11085.0</td>\n",
       "      <td>0.952364</td>\n",
       "      <td>0.132062</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.925772</td>\n",
       "      <td>0.984962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_entropy</th>\n",
       "      <td>11106.0</td>\n",
       "      <td>7.525019</td>\n",
       "      <td>0.805859</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.767465</td>\n",
       "      <td>7.586656</td>\n",
       "      <td>8.438537</td>\n",
       "      <td>8.942856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_entropy</th>\n",
       "      <td>11085.0</td>\n",
       "      <td>7.661331</td>\n",
       "      <td>0.813671</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>6.865694</td>\n",
       "      <td>7.739547</td>\n",
       "      <td>8.573647</td>\n",
       "      <td>8.960002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_burstiness</th>\n",
       "      <td>11997.0</td>\n",
       "      <td>-0.010904</td>\n",
       "      <td>0.209583</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.241788</td>\n",
       "      <td>0.024053</td>\n",
       "      <td>0.202337</td>\n",
       "      <td>0.749782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_diversity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.125661</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.394140</td>\n",
       "      <td>0.526649</td>\n",
       "      <td>0.668577</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_entropy</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>8.871163</td>\n",
       "      <td>0.655246</td>\n",
       "      <td>3.007080</td>\n",
       "      <td>8.284809</td>\n",
       "      <td>8.957517</td>\n",
       "      <td>9.507826</td>\n",
       "      <td>10.686642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.026928</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.045428</td>\n",
       "      <td>0.520888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.170021</td>\n",
       "      <td>0.017826</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.145552</td>\n",
       "      <td>0.171186</td>\n",
       "      <td>0.191140</td>\n",
       "      <td>0.279001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_char_count</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>46.555917</td>\n",
       "      <td>10.280873</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.508337</td>\n",
       "      <td>0.087803</td>\n",
       "      <td>0.020956</td>\n",
       "      <td>0.440308</td>\n",
       "      <td>0.513477</td>\n",
       "      <td>0.586078</td>\n",
       "      <td>1.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bits_per_char</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>4.066694</td>\n",
       "      <td>0.702422</td>\n",
       "      <td>0.167652</td>\n",
       "      <td>3.522463</td>\n",
       "      <td>4.107813</td>\n",
       "      <td>4.688621</td>\n",
       "      <td>10.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_tree_depth</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>5.945475</td>\n",
       "      <td>5.013657</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.722222</td>\n",
       "      <td>5.555556</td>\n",
       "      <td>7.752778</td>\n",
       "      <td>190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tree_depth</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>9.835917</td>\n",
       "      <td>7.088795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>334.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>3.081287</td>\n",
       "      <td>1.148595</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.551199</td>\n",
       "      <td>2.990326</td>\n",
       "      <td>3.541391</td>\n",
       "      <td>47.527586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left_dependency_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.478993</td>\n",
       "      <td>0.067697</td>\n",
       "      <td>0.109302</td>\n",
       "      <td>0.402842</td>\n",
       "      <td>0.474026</td>\n",
       "      <td>0.561295</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_dependency_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.521007</td>\n",
       "      <td>0.067697</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438705</td>\n",
       "      <td>0.525974</td>\n",
       "      <td>0.597158</td>\n",
       "      <td>0.890698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_legomena_ratio</th>\n",
       "      <td>11711.0</td>\n",
       "      <td>0.465576</td>\n",
       "      <td>0.190785</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.284574</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.697959</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_type_ratio</th>\n",
       "      <td>11711.0</td>\n",
       "      <td>0.727506</td>\n",
       "      <td>0.143681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.738636</td>\n",
       "      <td>0.867647</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yules_k</th>\n",
       "      <td>10094.0</td>\n",
       "      <td>158.592979</td>\n",
       "      <td>295.094401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.008264</td>\n",
       "      <td>121.902827</td>\n",
       "      <td>215.638018</td>\n",
       "      <td>9918.699187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtld</th>\n",
       "      <td>6036.0</td>\n",
       "      <td>184.901667</td>\n",
       "      <td>753.583379</td>\n",
       "      <td>33.186479</td>\n",
       "      <td>55.958422</td>\n",
       "      <td>84.321343</td>\n",
       "      <td>154.842658</td>\n",
       "      <td>19792.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comma_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.046010</td>\n",
       "      <td>0.029333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>0.078343</td>\n",
       "      <td>0.389401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.042538</td>\n",
       "      <td>0.022302</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022983</td>\n",
       "      <td>0.040678</td>\n",
       "      <td>0.063402</td>\n",
       "      <td>1.050725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.228947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.004694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.006591</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.185771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.093871</td>\n",
       "      <td>0.130533</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.029865</td>\n",
       "      <td>0.085247</td>\n",
       "      <td>0.246476</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.421896</td>\n",
       "      <td>0.172734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.448516</td>\n",
       "      <td>0.597105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.040844</td>\n",
       "      <td>0.039201</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.031645</td>\n",
       "      <td>0.090930</td>\n",
       "      <td>0.547500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.550233</td>\n",
       "      <td>0.248680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.541667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.034842</td>\n",
       "      <td>0.028024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031746</td>\n",
       "      <td>0.063901</td>\n",
       "      <td>0.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.016123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.035045</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count        mean         std         min  \\\n",
       "type_token_ratio             12000.0    0.588571    0.145105    0.006803   \n",
       "stopword_ratio               12000.0    0.441644    0.167728    0.000000   \n",
       "punctuation_ratio            12000.0    0.029762    0.012669    0.000000   \n",
       "avg_sentence_length          12000.0   22.773167   22.138380    1.000000   \n",
       "sentence_length_std          11855.0   10.331872   11.410634    0.000000   \n",
       "n_tokens_doc                 12000.0  253.744750  258.117404    1.000000   \n",
       "n_sentences_doc              12000.0   13.583083   13.537794    1.000000   \n",
       "flesch_reading_ease          10975.0   54.753926   24.490668 -151.824406   \n",
       "gunning_fog                  10975.0   13.697227    5.771072    1.200000   \n",
       "smog_index                   10921.0   12.162749    3.998614    3.843193   \n",
       "automated_readability_index  10975.0   11.878375    6.294377   -5.647825   \n",
       "unigram_diversity            11127.0    0.577866    0.135299    0.006803   \n",
       "bigram_diversity             11106.0    0.894522    0.136156    0.008197   \n",
       "trigram_diversity            11085.0    0.952364    0.132062    0.008264   \n",
       "bigram_entropy               11106.0    7.525019    0.805859   -0.000000   \n",
       "trigram_entropy              11085.0    7.661331    0.813671   -0.000000   \n",
       "token_burstiness             11997.0   -0.010904    0.209583   -1.000000   \n",
       "char_trigram_diversity       12000.0    0.525953    0.125661    0.006455   \n",
       "char_trigram_entropy         12000.0    8.871163    0.655246    3.007080   \n",
       "uppercase_ratio              12000.0    0.026928    0.018735    0.000000   \n",
       "whitespace_ratio             12000.0    0.170021    0.017826    0.022472   \n",
       "unique_char_count            12000.0   46.555917   10.280873    9.000000   \n",
       "compression_ratio            12000.0    0.508337    0.087803    0.020956   \n",
       "bits_per_char                12000.0    4.066694    0.702422    0.167652   \n",
       "avg_tree_depth               12000.0    5.945475    5.013657    1.000000   \n",
       "max_tree_depth               12000.0    9.835917    7.088795    1.000000   \n",
       "avg_dependency_distance      12000.0    3.081287    1.148595    1.000000   \n",
       "left_dependency_ratio        12000.0    0.478993    0.067697    0.109302   \n",
       "right_dependency_ratio       12000.0    0.521007    0.067697    0.000000   \n",
       "hapax_legomena_ratio         11711.0    0.465576    0.190785    0.000000   \n",
       "hapax_type_ratio             11711.0    0.727506    0.143681    0.000000   \n",
       "yules_k                      10094.0  158.592979  295.094401    0.000000   \n",
       "mtld                          6036.0  184.901667  753.583379   33.186479   \n",
       "comma_ratio                  12000.0    0.046010    0.029333    0.000000   \n",
       "period_ratio                 12000.0    0.042538    0.022302    0.000000   \n",
       "question_ratio               12000.0    0.001547    0.005032    0.000000   \n",
       "exclamation_ratio            12000.0    0.001181    0.005099    0.000000   \n",
       "semicolon_ratio              12000.0    0.001393    0.005033    0.000000   \n",
       "colon_ratio                  12000.0    0.001836    0.004694    0.000000   \n",
       "quote_ratio                  12000.0    0.006591    0.013117    0.000000   \n",
       "sentiment_polarity           12000.0    0.093871    0.130533   -1.000000   \n",
       "sentiment_subjectivity       12000.0    0.421896    0.172734    0.000000   \n",
       "sentiment_polarity_variance  12000.0    0.040844    0.039201    0.000000   \n",
       "neutral_sentence_ratio       12000.0    0.550233    0.248680    0.000000   \n",
       "positive_word_ratio          12000.0    0.034842    0.028024    0.000000   \n",
       "negative_word_ratio          12000.0    0.016371    0.016123    0.000000   \n",
       "\n",
       "                                    10%         50%         90%           max  \n",
       "type_token_ratio               0.449778    0.581017    0.752700      1.000000  \n",
       "stopword_ratio                 0.220585    0.473684    0.609045      1.000000  \n",
       "punctuation_ratio              0.016787    0.028144    0.045329      0.229457  \n",
       "avg_sentence_length           12.000000   19.562500   30.166667    427.000000  \n",
       "sentence_length_std            4.230090    7.945998   16.084905    234.000000  \n",
       "n_tokens_doc                 110.000000  222.000000  395.000000  12117.000000  \n",
       "n_sentences_doc                5.000000   11.000000   24.000000    801.000000  \n",
       "flesch_reading_ease           21.818801   58.642977   82.408221    130.124032  \n",
       "gunning_fog                    7.427190   13.018447   20.446632     73.726126  \n",
       "smog_index                     7.492282   11.855464   17.122413     49.539670  \n",
       "automated_readability_index    5.765274   11.054197   18.041672     82.172177  \n",
       "unigram_diversity              0.454804    0.573529    0.715447      1.000000  \n",
       "bigram_diversity               0.823151    0.924314    0.982175      1.000000  \n",
       "trigram_diversity              0.925772    0.984962    1.000000      1.000000  \n",
       "bigram_entropy                 6.767465    7.586656    8.438537      8.942856  \n",
       "trigram_entropy                6.865694    7.739547    8.573647      8.960002  \n",
       "token_burstiness              -0.241788    0.024053    0.202337      0.749782  \n",
       "char_trigram_diversity         0.394140    0.526649    0.668577      1.000000  \n",
       "char_trigram_entropy           8.284809    8.957517    9.507826     10.686642  \n",
       "uppercase_ratio                0.010781    0.023726    0.045428      0.520888  \n",
       "whitespace_ratio               0.145552    0.171186    0.191140      0.279001  \n",
       "unique_char_count             34.000000   46.000000   60.000000    222.000000  \n",
       "compression_ratio              0.440308    0.513477    0.586078      1.266667  \n",
       "bits_per_char                  3.522463    4.107813    4.688621     10.133333  \n",
       "avg_tree_depth                 3.722222    5.555556    7.752778    190.000000  \n",
       "max_tree_depth                 6.000000    9.000000   13.000000    334.000000  \n",
       "avg_dependency_distance        2.551199    2.990326    3.541391     47.527586  \n",
       "left_dependency_ratio          0.402842    0.474026    0.561295      1.000000  \n",
       "right_dependency_ratio         0.438705    0.525974    0.597158      0.890698  \n",
       "hapax_legomena_ratio           0.284574    0.439024    0.697959      1.000000  \n",
       "hapax_type_ratio               0.596591    0.738636    0.867647      1.000000  \n",
       "yules_k                       69.008264  121.902827  215.638018   9918.699187  \n",
       "mtld                          55.958422   84.321343  154.842658  19792.640000  \n",
       "comma_ratio                    0.012987    0.042813    0.078343      0.389401  \n",
       "period_ratio                   0.022983    0.040678    0.063402      1.050725  \n",
       "question_ratio                 0.000000    0.000000    0.005321      0.083333  \n",
       "exclamation_ratio              0.000000    0.000000    0.003289      0.200000  \n",
       "semicolon_ratio                0.000000    0.000000    0.004348      0.228947  \n",
       "colon_ratio                    0.000000    0.000000    0.006623      0.166667  \n",
       "quote_ratio                    0.000000    0.000000    0.022727      0.185771  \n",
       "sentiment_polarity            -0.029865    0.085247    0.246476      1.000000  \n",
       "sentiment_subjectivity         0.200000    0.448516    0.597105      1.000000  \n",
       "sentiment_polarity_variance    0.000233    0.031645    0.090930      0.547500  \n",
       "neutral_sentence_ratio         0.250000    0.541667    1.000000      1.000000  \n",
       "positive_word_ratio            0.000000    0.031746    0.063901      0.811111  \n",
       "negative_word_ratio            0.000000    0.013761    0.035045      0.333333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Save enriched dataset and basic descriptive statistics\n",
    "\n",
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_PREPOS.csv\"\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS}  (rows: {len(df_with_features)})\")\n",
    "\n",
    "feature_cols = [col for col in df_with_features.columns if col not in df.columns]\n",
    "print(f\"Feature columns saved ({len(feature_cols)} total):\")\n",
    "print(feature_cols)\n",
    "\n",
    "display(df_with_features[feature_cols].describe(percentiles=[0.1, 0.5, 0.9]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125c533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b190a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length min: 1.0 max: 427.0\n",
      "flesch_reading_ease min: -151.8244062244062 max: 130.12403186866248\n",
      "gunning_fog min: 1.2000000000000002 max: 73.72612612612612\n",
      "automated_readability_index min: -5.6478253424657545 max: 82.17217701641684\n",
      "mtld min: 33.186479128856625 max: 19792.639999999872\n",
      "yules_k min: 0.0 max: 9918.69918699187\n",
      "max_tree_depth min: 1.0 max: 334.0\n"
     ]
    }
   ],
   "source": [
    "for col in [\"avg_sentence_length\",\"flesch_reading_ease\",\"gunning_fog\",\n",
    "            \"automated_readability_index\",\"mtld\",\"yules_k\",\"max_tree_depth\"]:\n",
    "    if col in df.columns:\n",
    "        print(col, \"min:\", df[col].min(), \"max:\", df[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b72a3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed thresholds:\n",
      "  asl_hi: 102.52500000000055\n",
      "  sls_hi: 62.493794768268565\n",
      "  fog_hi: 38.74899420726596\n",
      "  ari_hi: 43.06082617497611\n",
      "  fre_lo: -32.546580602409605\n",
      "  mtld_hi: 5196.842874999961\n",
      "  yk_hi: 1570.8391580740192\n",
      "  depth_max_hi: 29.0\n",
      "  depth_avg_hi: 18.0\n",
      "  depdist_hi: 5.915709908332465\n",
      "  comp_hi: 1.0\n",
      "  upper_hi: 0.09443931284611005\n",
      "  uniq_hi: 73.0\n",
      "  ws_lo: 0.12294160206644959\n",
      "  ws_hi: 0.21611750095420387\n",
      "\n",
      "Cause counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>markup_or_code_noise</th>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependency_depth_computation_bug</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_metric_instability_or_length_effects</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_segmentation_failure</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              count\n",
       "cause                                              \n",
       "markup_or_code_noise                            232\n",
       "dependency_depth_computation_bug                132\n",
       "lexical_metric_instability_or_length_effects     82\n",
       "sentence_segmentation_failure                    52"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flag counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <td>736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_noise</th>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_instability</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_outlier</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth_implausible</th>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_overhead</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count\n",
       "severity                    736\n",
       "markup_noise                228\n",
       "seg_len_extreme             208\n",
       "lexical_instability          82\n",
       "readability_outlier          74\n",
       "depth_implausible            74\n",
       "dep_distance_implausible     60\n",
       "compression_overhead          9\n",
       "ngram_edge_effects            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top offenders (with original feature values):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <th>readability_outlier</th>\n",
       "      <th>lexical_instability</th>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <th>depth_implausible</th>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <th>compression_overhead</th>\n",
       "      <th>markup_noise</th>\n",
       "      <th>suspected_causes</th>\n",
       "      <th>severity</th>\n",
       "      <th>...</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>unique_char_count</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>bigram_entropy</th>\n",
       "      <th>trigram_entropy</th>\n",
       "      <th>bigram_diversity</th>\n",
       "      <th>trigram_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022929</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.090237</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1604</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>6.403070</td>\n",
       "      <td>0.043023</td>\n",
       "      <td>0.183122</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.191947</td>\n",
       "      <td>2.321892</td>\n",
       "      <td>2.321892</td>\n",
       "      <td>0.014368</td>\n",
       "      <td>0.014409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>0.100967</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.219119</td>\n",
       "      <td>2.309552</td>\n",
       "      <td>2.622980</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>0.095588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.046154</td>\n",
       "      <td>0.067003</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.710958</td>\n",
       "      <td>0.715476</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3.260113</td>\n",
       "      <td>0.194100</td>\n",
       "      <td>0.043068</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.228909</td>\n",
       "      <td>5.366029</td>\n",
       "      <td>6.110405</td>\n",
       "      <td>0.280928</td>\n",
       "      <td>0.410853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.046154</td>\n",
       "      <td>0.067003</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.710958</td>\n",
       "      <td>0.715476</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>2.768896</td>\n",
       "      <td>0.207490</td>\n",
       "      <td>0.122256</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.149806</td>\n",
       "      <td>5.633553</td>\n",
       "      <td>6.112823</td>\n",
       "      <td>0.316092</td>\n",
       "      <td>0.368876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4765</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3.285005</td>\n",
       "      <td>0.473597</td>\n",
       "      <td>0.007013</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.106848</td>\n",
       "      <td>8.014676</td>\n",
       "      <td>8.016808</td>\n",
       "      <td>0.996154</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1.662116</td>\n",
       "      <td>0.020956</td>\n",
       "      <td>0.105320</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.157442</td>\n",
       "      <td>1.584946</td>\n",
       "      <td>1.584946</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.010274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7112</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>39.720000</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.190535</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.199754</td>\n",
       "      <td>1.999980</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.325773</td>\n",
       "      <td>0.026560</td>\n",
       "      <td>0.100062</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.199506</td>\n",
       "      <td>1.999979</td>\n",
       "      <td>1.999972</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>0.012422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>4.046154</td>\n",
       "      <td>0.067003</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>0.710958</td>\n",
       "      <td>0.715476</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10735</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>334.0</td>\n",
       "      <td>2.947532</td>\n",
       "      <td>0.142339</td>\n",
       "      <td>0.007249</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.153542</td>\n",
       "      <td>2.960582</td>\n",
       "      <td>2.965867</td>\n",
       "      <td>0.296137</td>\n",
       "      <td>0.296774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10670</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47.527586</td>\n",
       "      <td>0.031895</td>\n",
       "      <td>0.092871</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.272045</td>\n",
       "      <td>1.612653</td>\n",
       "      <td>1.640500</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.017301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>88.0</td>\n",
       "      <td>2.486275</td>\n",
       "      <td>0.026291</td>\n",
       "      <td>0.120188</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.358776</td>\n",
       "      <td>2.358839</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.016471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6901</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.903618</td>\n",
       "      <td>0.485387</td>\n",
       "      <td>0.002006</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.111175</td>\n",
       "      <td>8.599913</td>\n",
       "      <td>8.596190</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9687</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.627507</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.068085</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.238298</td>\n",
       "      <td>3.807157</td>\n",
       "      <td>4.693320</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.161290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9547</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>154.0</td>\n",
       "      <td>1.764890</td>\n",
       "      <td>0.083538</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>2.992157</td>\n",
       "      <td>3.503450</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.092050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11806</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.453664</td>\n",
       "      <td>0.500526</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.121010</td>\n",
       "      <td>8.426265</td>\n",
       "      <td>8.422065</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9057</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.568507</td>\n",
       "      <td>0.027940</td>\n",
       "      <td>0.045484</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.272255</td>\n",
       "      <td>2.584942</td>\n",
       "      <td>2.584929</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.014354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_len_extreme  readability_outlier  lexical_instability  \\\n",
       "9906              True                False                 True   \n",
       "1604              True                 True                 True   \n",
       "4418              True                False                 True   \n",
       "7828              True                False                 True   \n",
       "7524              True                 True                False   \n",
       "1691              True                False                 True   \n",
       "7337              True                 True                False   \n",
       "4765              True                 True                False   \n",
       "7242              True                False                 True   \n",
       "7112              True                False                 True   \n",
       "10755             True                False                 True   \n",
       "1954              True                False                 True   \n",
       "10735             True                 True                 True   \n",
       "10670             True                False                 True   \n",
       "9994              True                False                 True   \n",
       "6901              True                 True                False   \n",
       "9687              True                False                 True   \n",
       "9547              True                False                 True   \n",
       "11806             True                 True                 True   \n",
       "9057              True                False                 True   \n",
       "\n",
       "       ngram_edge_effects  depth_implausible  dep_distance_implausible  \\\n",
       "9906                 True               True                     False   \n",
       "1604                False              False                      True   \n",
       "4418                False               True                      True   \n",
       "7828                False               True                     False   \n",
       "7524                False               True                     False   \n",
       "1691                False               True                     False   \n",
       "7337                False               True                     False   \n",
       "4765                False               True                     False   \n",
       "7242                False               True                     False   \n",
       "7112                False              False                      True   \n",
       "10755               False               True                     False   \n",
       "1954                False               True                     False   \n",
       "10735               False               True                     False   \n",
       "10670               False              False                      True   \n",
       "9994                False               True                     False   \n",
       "6901                False               True                     False   \n",
       "9687                False               True                     False   \n",
       "9547                False               True                     False   \n",
       "11806               False              False                     False   \n",
       "9057                False               True                     False   \n",
       "\n",
       "       compression_overhead  markup_noise  \\\n",
       "9906                  False          True   \n",
       "1604                  False          True   \n",
       "4418                  False          True   \n",
       "7828                  False          True   \n",
       "7524                  False          True   \n",
       "1691                  False          True   \n",
       "7337                  False          True   \n",
       "4765                  False          True   \n",
       "7242                  False          True   \n",
       "7112                  False          True   \n",
       "10755                 False          True   \n",
       "1954                  False          True   \n",
       "10735                 False         False   \n",
       "10670                 False          True   \n",
       "9994                  False          True   \n",
       "6901                  False          True   \n",
       "9687                  False          True   \n",
       "9547                  False          True   \n",
       "11806                 False          True   \n",
       "9057                  False          True   \n",
       "\n",
       "                                        suspected_causes  severity  ...  \\\n",
       "9906   [dependency_depth_computation_bug, lexical_met...         5  ...   \n",
       "1604   [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "4418   [dependency_depth_computation_bug, lexical_met...         5  ...   \n",
       "7828   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "7524   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "1691   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "7337   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "4765   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "7242   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "7112   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "10755  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "1954   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "10735  [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "10670  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "9994   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "6901   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "9687   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "9547   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "11806  [sentence_segmentation_failure, lexical_metric...         4  ...   \n",
       "9057   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "\n",
       "       max_tree_depth  avg_dependency_distance  compression_ratio  \\\n",
       "9906            122.0                 1.000000           0.022929   \n",
       "1604              8.0                 6.403070           0.043023   \n",
       "4418             24.0                 7.400000           0.100967   \n",
       "7828             22.0                 4.046154           0.067003   \n",
       "7524             43.0                 3.260113           0.194100   \n",
       "1691             22.0                 4.046154           0.067003   \n",
       "7337             64.0                 2.768896           0.207490   \n",
       "4765            162.0                 3.285005           0.473597   \n",
       "7242             99.0                 1.662116           0.020956   \n",
       "7112              5.0                39.720000           0.050400   \n",
       "10755            82.0                 2.325773           0.026560   \n",
       "1954             22.0                 4.046154           0.067003   \n",
       "10735           334.0                 2.947532           0.142339   \n",
       "10670             3.0                47.527586           0.031895   \n",
       "9994             88.0                 2.486275           0.026291   \n",
       "6901             83.0                 2.903618           0.485387   \n",
       "9687             30.0                 3.627507           0.148936   \n",
       "9547            154.0                 1.764890           0.083538   \n",
       "11806            26.0                 4.453664           0.500526   \n",
       "9057             71.0                 2.568507           0.027940   \n",
       "\n",
       "       uppercase_ratio  unique_char_count  whitespace_ratio  bigram_entropy  \\\n",
       "9906          0.090976               10.0          0.090237       -0.000000   \n",
       "1604          0.183122               16.0          0.191947        2.321892   \n",
       "4418          0.006445               29.0          0.219119        2.309552   \n",
       "7828          0.018732               27.0          0.093660        0.710958   \n",
       "7524          0.043068               28.0          0.228909        5.366029   \n",
       "1691          0.018732               27.0          0.093660        0.710958   \n",
       "7337          0.122256               46.0          0.149806        5.633553   \n",
       "4765          0.007013               40.0          0.106848        8.014676   \n",
       "7242          0.105320                9.0          0.157442        1.584946   \n",
       "7112          0.190535               14.0          0.199754        1.999980   \n",
       "10755         0.100062               15.0          0.199506        1.999979   \n",
       "1954          0.018732               27.0          0.093660        0.710958   \n",
       "10735         0.007249               37.0          0.153542        2.960582   \n",
       "10670         0.092871                9.0          0.272045        1.612653   \n",
       "9994          0.120188               19.0          0.200000        2.358776   \n",
       "6901          0.002006               37.0          0.111175        8.599913   \n",
       "9687          0.068085               34.0          0.238298        3.807157   \n",
       "9547          0.003276               24.0          0.260442        2.992157   \n",
       "11806         0.001052               34.0          0.121010        8.426265   \n",
       "9057          0.045484               13.0          0.272255        2.584942   \n",
       "\n",
       "       trigram_entropy  bigram_diversity  trigram_diversity  \n",
       "9906         -0.000000          0.008197           0.008264  \n",
       "1604          2.321892          0.014368           0.014409  \n",
       "4418          2.622980          0.087591           0.095588  \n",
       "7828          0.715476          0.092308           0.093023  \n",
       "7524          6.110405          0.280928           0.410853  \n",
       "1691          0.715476          0.092308           0.093023  \n",
       "7337          6.112823          0.316092           0.368876  \n",
       "4765          8.016808          0.996154           1.000000  \n",
       "7242          1.584946          0.010239           0.010274  \n",
       "7112          2.000000          0.012308           0.012346  \n",
       "10755         1.999972          0.012384           0.012422  \n",
       "1954          0.715476          0.092308           0.093023  \n",
       "10735         2.965867          0.296137           0.296774  \n",
       "10670         1.640500          0.013793           0.017301  \n",
       "9994          2.358839          0.016432           0.016471  \n",
       "6901          8.596190          1.000000           1.000000  \n",
       "9687          4.693320          0.114286           0.161290  \n",
       "9547          3.503450          0.079167           0.092050  \n",
       "11806         8.422065          1.000000           1.000000  \n",
       "9057          2.584929          0.014320           0.014354  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"bigram_entropy\",\"trigram_entropy\",\"bigram_diversity\",\"trigram_diversity\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\"whitespace_ratio\"\n",
    "]\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols=NUMERIC_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Coerce known feature columns to numeric (if present).\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _q(df, col, q):\n",
    "    return float(np.nanquantile(df[col].values, q)) if col in df else np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# Thresholds and Diagnostics\n",
    "# -------------------------------\n",
    "def compute_diagnostic_thresholds(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Data-driven thresholds using quantiles only.\"\"\"\n",
    "    thr = {}\n",
    "    thr[\"asl_hi\"]       = _q(df, \"avg_sentence_length\", 0.99)\n",
    "    thr[\"sls_hi\"]       = _q(df, \"sentence_length_std\", 0.99)\n",
    "    thr[\"fog_hi\"]       = _q(df, \"gunning_fog\", 0.995)\n",
    "    thr[\"ari_hi\"]       = _q(df, \"automated_readability_index\", 0.995)\n",
    "    thr[\"fre_lo\"]       = _q(df, \"flesch_reading_ease\", 0.005)\n",
    "    thr[\"mtld_hi\"]      = _q(df, \"mtld\", 0.995)\n",
    "    thr[\"yk_hi\"]        = _q(df, \"yules_k\", 0.995)\n",
    "    thr[\"depth_max_hi\"] = _q(df, \"max_tree_depth\", 0.995)\n",
    "    thr[\"depth_avg_hi\"] = _q(df, \"avg_tree_depth\", 0.995)\n",
    "    thr[\"depdist_hi\"]   = _q(df, \"avg_dependency_distance\", 0.995)\n",
    "    thr[\"comp_hi\"]      = 1.0  # Compression > 1.0 = expansion\n",
    "    thr[\"upper_hi\"]     = _q(df, \"uppercase_ratio\", 0.995)\n",
    "    thr[\"uniq_hi\"]      = _q(df, \"unique_char_count\", 0.995)\n",
    "    thr[\"ws_lo\"]        = _q(df, \"whitespace_ratio\", 0.005)\n",
    "    thr[\"ws_hi\"]        = _q(df, \"whitespace_ratio\", 0.995)\n",
    "    return thr\n",
    "\n",
    "def diagnose_feature_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return diagnostics dataframe with boolean flags and suspected causes list.\"\"\"\n",
    "    # Ensure numeric so comparisons fire correctly\n",
    "    df = ensure_numeric(df)\n",
    "\n",
    "    thr = compute_diagnostic_thresholds(df)\n",
    "    D = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Flag definitions\n",
    "    D[\"seg_len_extreme\"] = (\n",
    "        (df.get(\"avg_sentence_length\", np.nan) > thr[\"asl_hi\"]) |\n",
    "        (df.get(\"sentence_length_std\", np.nan) > thr[\"sls_hi\"])\n",
    "    )\n",
    "    D[\"readability_outlier\"] = (\n",
    "        (df.get(\"flesch_reading_ease\", np.nan) < thr[\"fre_lo\"]) |\n",
    "        (df.get(\"gunning_fog\", np.nan) > thr[\"fog_hi\"]) |\n",
    "        (df.get(\"automated_readability_index\", np.nan) > thr[\"ari_hi\"])\n",
    "    )\n",
    "    D[\"lexical_instability\"] = (\n",
    "        (df.get(\"mtld\", np.nan) > thr[\"mtld_hi\"]) |\n",
    "        (df.get(\"yules_k\", np.nan) > thr[\"yk_hi\"])\n",
    "    )\n",
    "    D[\"ngram_edge_effects\"] = (\n",
    "    (df.get(\"bigram_entropy\", np.nan) == 0) |\n",
    "    (df.get(\"trigram_entropy\", np.nan) == 0) |\n",
    "    ((df.get(\"n_tokens_ws\", 1000) < 50) & (df.get(\"trigram_diversity\", 0) == 1.0)) |\n",
    "    ((df.get(\"n_tokens_ws\", 1000) < 50) & (df.get(\"bigram_diversity\", 0) == 1.0))\n",
    "    )\n",
    "    D[\"depth_implausible\"] = (\n",
    "        (df.get(\"max_tree_depth\", np.nan) > thr[\"depth_max_hi\"]) |\n",
    "        (df.get(\"avg_tree_depth\", np.nan) > thr[\"depth_avg_hi\"])\n",
    "    )\n",
    "    D[\"dep_distance_implausible\"] = (df.get(\"avg_dependency_distance\", np.nan) > thr[\"depdist_hi\"])\n",
    "    D[\"compression_overhead\"] = (df.get(\"compression_ratio\", np.nan) > thr[\"comp_hi\"])\n",
    "    D[\"markup_noise\"] = (\n",
    "    (df.get(\"uppercase_ratio\", np.nan) > thr[\"upper_hi\"]) |\n",
    "    (df.get(\"unique_char_count\", np.nan) > thr[\"uniq_hi\"]) |\n",
    "    (df.get(\"whitespace_ratio\", np.nan) < thr[\"ws_lo\"]) |\n",
    "    (df.get(\"whitespace_ratio\", np.nan) > thr[\"ws_hi\"])\n",
    "    )\n",
    "\n",
    "    # Suspected causes column (pre-allocate + .at)\n",
    "    D[\"suspected_causes\"] = pd.Series([[] for _ in range(len(D))], index=D.index, dtype=object)\n",
    "    for i in D.index:\n",
    "        c = []\n",
    "        if bool(D.at[i, \"depth_implausible\"]) or bool(D.at[i, \"dep_distance_implausible\"]):\n",
    "            c.append(\"dependency_depth_computation_bug\")\n",
    "        if bool(D.at[i, \"seg_len_extreme\"]) and bool(D.at[i, \"readability_outlier\"]):\n",
    "            c.append(\"sentence_segmentation_failure\")\n",
    "        if bool(D.at[i, \"lexical_instability\"]) or bool(D.at[i, \"ngram_edge_effects\"]):\n",
    "            c.append(\"lexical_metric_instability_or_length_effects\")\n",
    "        if bool(D.at[i, \"compression_overhead\"]) or bool(D.at[i, \"markup_noise\"]):\n",
    "            c.append(\"markup_or_code_noise\")\n",
    "        D.at[i, \"suspected_causes\"] = c\n",
    "\n",
    "    # Severity score\n",
    "    flag_cols = [c for c in D.columns if c != \"suspected_causes\"]\n",
    "    for c in flag_cols:\n",
    "        D[c] = D[c].fillna(False).astype(bool)\n",
    "    D[\"severity\"] = D[flag_cols].sum(axis=1).astype(int)\n",
    "\n",
    "    # Attach thresholds for later inspection\n",
    "    D.attrs[\"thresholds\"] = thr\n",
    "    return D\n",
    "\n",
    "def build_diagnostic_report(diag: pd.DataFrame, top_k: int = 15):\n",
    "    \"\"\"Summarize counts by cause and return top offending rows by severity.\"\"\"\n",
    "    cause_counts = (\n",
    "        diag[\"suspected_causes\"].explode().value_counts(dropna=True)\n",
    "        .rename_axis(\"cause\").to_frame(\"count\")\n",
    "    )\n",
    "    flag_counts = (\n",
    "        diag.drop(columns=[\"suspected_causes\"])\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .to_frame(\"count\")\n",
    "    )\n",
    "    top_offenders = diag.sort_values(\"severity\", ascending=False).head(top_k)\n",
    "    return {\"cause_counts\": cause_counts, \"flag_counts\": flag_counts, \"top_offenders\": top_offenders}\n",
    "\n",
    "# -------------------------------\n",
    "# Runner / Example usage\n",
    "# -------------------------------\n",
    "\n",
    "# Assume you already have `df` with your features\n",
    "# If your features were just computed and may be strings, the coercion inside\n",
    "# diagnose_feature_outliers will handle them; coercing here is optional:\n",
    "# df = ensure_numeric(df)\n",
    "\n",
    "diag = diagnose_feature_outliers(df)\n",
    "rep  = build_diagnostic_report(diag, top_k=20)\n",
    "\n",
    "print(\"Computed thresholds:\")\n",
    "for k, v in diag.attrs.get(\"thresholds\", {}).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nCause counts:\")\n",
    "display(rep[\"cause_counts\"])\n",
    "\n",
    "print(\"\\nFlag counts:\")\n",
    "display(rep[\"flag_counts\"])\n",
    "\n",
    "print(\"\\nTop offenders (with original feature values):\")\n",
    "cols_to_show = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\n",
    "    \"whitespace_ratio\",\"bigram_entropy\",\"trigram_entropy\",\n",
    "    \"bigram_diversity\",\"trigram_diversity\"\n",
    "]\n",
    "existing_cols = [c for c in cols_to_show if c in df.columns]\n",
    "display(rep[\"top_offenders\"].join(df[existing_cols], how=\"left\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81c0ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FEATURES_TO_CAP = [\n",
    "    'avg_sentence_length','sentence_length_std','flesch_reading_ease',\n",
    "    'gunning_fog','automated_readability_index','mtld','yules_k',\n",
    "    'avg_dependency_distance','max_tree_depth','compression_ratio',\n",
    "    'bigram_entropy','trigram_entropy','bigram_diversity','trigram_diversity'\n",
    "]\n",
    "\n",
    "# Natural/practical bounds to prevent absurd caps\n",
    "BOUNDED = {\n",
    "    'bigram_diversity': (0.0, 1.0),\n",
    "    'trigram_diversity': (0.0, 1.0),\n",
    "    'compression_ratio': (0.0, np.inf),\n",
    "    'flesch_reading_ease': (-100.0, 150.0),   # practical working range\n",
    "}\n",
    "\n",
    "def _finite(s: pd.Series) -> pd.Series:\n",
    "    return s[np.isfinite(s.values)]\n",
    "\n",
    "def calculate_percentile_caps(df: pd.DataFrame, lower_pct=1, upper_pct=99) -> dict:\n",
    "    caps = {}\n",
    "    for feat in FEATURES_TO_CAP:\n",
    "        if feat not in df.columns: \n",
    "            continue\n",
    "        s = _finite(df[feat].dropna())\n",
    "        if s.empty:\n",
    "            continue\n",
    "        lo = float(np.percentile(s, lower_pct))\n",
    "        hi = float(np.percentile(s, upper_pct))\n",
    "        if feat in BOUNDED:\n",
    "            blo, bhi = BOUNDED[feat]\n",
    "            lo = max(lo, blo)\n",
    "            hi = min(hi, bhi)\n",
    "        if lo > hi:  # degenerate case\n",
    "            lo, hi = hi, lo\n",
    "        caps[feat] = (lo, hi)\n",
    "        print(f\"{feat}: [{lo:.2f}, {hi:.2f}]\")\n",
    "    return caps\n",
    "\n",
    "def cap_extreme_features(df: pd.DataFrame, caps: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['features_capped'] = 0  # keep your counter if you like\n",
    "\n",
    "    for feat, (lo, hi) in caps.items():\n",
    "        if feat not in df.columns:\n",
    "            continue\n",
    "        col = df[feat]\n",
    "        before = col.copy()\n",
    "        # clip in-place while preserving NaNs\n",
    "        df[feat] = col.clip(lower=lo, upper=hi)\n",
    "        changed = (before != df[feat]) & df[feat].notna() & before.notna()\n",
    "        if changed.any():\n",
    "            print(f\"Capped {feat}: {int(changed.sum())} values\")\n",
    "            df['features_capped'] += changed.astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee2f54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length: [7.58, 102.53]\n",
      "sentence_length_std: [1.43, 62.49]\n",
      "flesch_reading_ease: [-16.40, 97.13]\n",
      "gunning_fog: [4.66, 33.45]\n",
      "automated_readability_index: [2.00, 36.17]\n",
      "mtld: [42.08, 2615.82]\n",
      "yules_k: [2.68, 1028.99]\n",
      "avg_dependency_distance: [2.12, 4.79]\n",
      "max_tree_depth: [4.00, 21.00]\n",
      "compression_ratio: [0.10, 0.71]\n",
      "bigram_entropy: [4.19, 8.83]\n",
      "trigram_entropy: [4.37, 8.95]\n",
      "bigram_diversity: [0.12, 1.00]\n",
      "trigram_diversity: [0.14, 1.00]\n",
      "Capped avg_sentence_length: 240 values\n",
      "Capped sentence_length_std: 238 values\n",
      "Capped flesch_reading_ease: 220 values\n",
      "Capped gunning_fog: 220 values\n",
      "Capped automated_readability_index: 220 values\n",
      "Capped mtld: 122 values\n",
      "Capped yules_k: 202 values\n",
      "Capped avg_dependency_distance: 240 values\n",
      "Capped max_tree_depth: 136 values\n",
      "Capped compression_ratio: 240 values\n",
      "Capped bigram_entropy: 223 values\n",
      "Capped trigram_entropy: 174 values\n",
      "Capped bigram_diversity: 112 values\n",
      "Capped trigram_diversity: 111 values\n"
     ]
    }
   ],
   "source": [
    "actual_caps = calculate_percentile_caps(df_with_features, lower_pct=1, upper_pct=99)\n",
    "df_with_features = cap_extreme_features(df_with_features, actual_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c5e1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3.0 standard deviations as threshold\n",
      "Parse issues: 180 texts\n",
      "Readability anomalies: 244 texts\n",
      "Lexical anomalies: 472 texts\n",
      "Quality score distribution:\n",
      "quality_score\n",
      "0       27\n",
      "1      160\n",
      "2      495\n",
      "3    11318\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "READABILITY_FEATS = ['flesch_reading_ease','gunning_fog','automated_readability_index']\n",
    "LEXICAL_FEATS     = ['mtld','yules_k','type_token_ratio']\n",
    "PARSE_FEATS       = ['avg_dependency_distance']  # plus tree-depth presence checks\n",
    "\n",
    "# Features where z-score should be computed on log1p to tame skew (values stay untouched)\n",
    "LOG_FOR_Z = {'mtld','yules_k','avg_dependency_distance','sentence_length_std','max_tree_depth'}\n",
    "\n",
    "def _z_on(series: pd.Series, log_if_needed: bool) -> pd.Series:\n",
    "    s = series.astype(float).replace([np.inf,-np.inf], np.nan)\n",
    "    if log_if_needed:\n",
    "        nonan = s.dropna()\n",
    "        if (nonan >= 0).all():\n",
    "            s = np.log1p(s)\n",
    "    # classical population z-score (ddof=0); fill NaNs with mean to avoid bias in stats.zscore\n",
    "    m = s.mean()\n",
    "    sd = s.std(ddof=0)\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - m) / sd\n",
    "\n",
    "def add_quality_flags_statistical(df: pd.DataFrame, n_std: float = 3.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Compute z-scores (abs) transiently; do not keep columns\n",
    "    z_map = {}\n",
    "\n",
    "    for feat in set(READABILITY_FEATS + LEXICAL_FEATS + PARSE_FEATS):\n",
    "        if feat in df.columns:\n",
    "            z = _z_on(df[feat], log_if_needed=(feat in LOG_FOR_Z)).abs()\n",
    "            z_map[feat] = z\n",
    "\n",
    "    df['parse_quality_issue'] = (\n",
    "        df['max_tree_depth'].isna() |\n",
    "        df['avg_tree_depth'].isna() |\n",
    "        (z_map.get('avg_dependency_distance', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['readability_anomaly'] = (\n",
    "        (z_map.get('flesch_reading_ease', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('gunning_fog', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('automated_readability_index', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['lexical_anomaly'] = (\n",
    "        (z_map.get('mtld', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('yules_k', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('type_token_ratio', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['quality_score'] = (3 - (\n",
    "        df['parse_quality_issue'].astype(int) +\n",
    "        df['readability_anomaly'].astype(int) +\n",
    "        df['lexical_anomaly'].astype(int)\n",
    "    )).clip(lower=0, upper=3)\n",
    "\n",
    "    print(f\"Using {n_std} standard deviations as threshold\")\n",
    "    print(f\"Parse issues: {int(df['parse_quality_issue'].sum())} texts\")\n",
    "    print(f\"Readability anomalies: {int(df['readability_anomaly'].sum())} texts\")\n",
    "    print(f\"Lexical anomalies: {int(df['lexical_anomaly'].sum())} texts\")\n",
    "    print(f\"Quality score distribution:\\n{df['quality_score'].value_counts().sort_index()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_features = add_quality_flags_statistical(df_with_features, n_std=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a3c1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Impute] Dropping duplicate columns: ['avg_word_length', 'digit_ratio', 'had_urls', 'had_html', 'had_code', 'had_table']\n",
      "\n",
      "[Impute] Dropping 2 features with >40% missing:\n",
      "  - not_text_reason: 100.0% missing\n",
      "  - mtld: 49.7% missing\n",
      "\n",
      "[Impute] Filled 11277 missing values across 14 features\n",
      "Top 10 imputed features:\n",
      "yules_k                        1906\n",
      "smog_index                     1079\n",
      "automated_readability_index    1025\n",
      "gunning_fog                    1025\n",
      "flesch_reading_ease            1025\n",
      "trigram_diversity               915\n",
      "trigram_entropy                 915\n",
      "bigram_diversity                894\n",
      "bigram_entropy                  894\n",
      "unigram_diversity               873\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def impute_after_capping(df: pd.DataFrame, max_missing_pct: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing values with group medians, dropping features with excessive missingness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    max_missing_pct : float\n",
    "        Maximum proportion of missing values allowed (default 0.4 = 40%)\n",
    "        Features exceeding this threshold are dropped before imputation\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop duplicate columns (keep first)\n",
    "    if df.columns.duplicated().any():\n",
    "        dup_mask = df.columns.duplicated()\n",
    "        print(f\"[Impute] Dropping duplicate columns: {df.columns[dup_mask].tolist()}\")\n",
    "        df = df.loc[:, ~dup_mask]\n",
    "\n",
    "    # Numeric features except target\n",
    "    num_feats = [c for c in df.select_dtypes(include=[np.number]).columns if c != \"is_ai\"]\n",
    "    if not num_feats:\n",
    "        return df\n",
    "\n",
    "    # Check missingness and drop high-missing features FIRST\n",
    "    missing_pct = df[num_feats].isna().mean()\n",
    "    high_missing = missing_pct[missing_pct > max_missing_pct].sort_values(ascending=False)\n",
    "    \n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\n[Impute] Dropping {len(high_missing)} features with >{max_missing_pct*100:.0f}% missing:\")\n",
    "        for feat, pct in high_missing.items():\n",
    "            print(f\"  - {feat}: {pct*100:.1f}% missing\")\n",
    "        \n",
    "        # Remove high-missing features\n",
    "        num_feats = [f for f in num_feats if f not in high_missing.index]\n",
    "        df.drop(columns=high_missing.index, inplace=True)\n",
    "    \n",
    "    if not num_feats:\n",
    "        print(\"[Impute] No features remaining after missingness filter\")\n",
    "        return df\n",
    "\n",
    "    # Group keys for stratified imputation\n",
    "    group_keys = []\n",
    "    if \"source_type\" in df.columns:\n",
    "        group_keys.append(\"source_type\")\n",
    "    if \"n_tokens_doc\" in df.columns:\n",
    "        bins = [0, 100, 250, 500, 10_000]\n",
    "        labels = [\"S\", \"M\", \"L\", \"XL\"]\n",
    "        df[\"__len_bin__\"] = pd.cut(df[\"n_tokens_doc\"], bins=bins, right=False, labels=labels)\n",
    "        group_keys.append(\"__len_bin__\")\n",
    "\n",
    "    before_missing = df[num_feats].isna().sum()\n",
    "    values = df[num_feats].to_numpy(dtype=float)\n",
    "    mask = np.isnan(values)\n",
    "\n",
    "    # Group-wise median imputation\n",
    "    if group_keys:\n",
    "        med_df = df.groupby(group_keys, dropna=False, observed=False)[num_feats].transform(\"median\")\n",
    "        med_vals = med_df.to_numpy(dtype=float)\n",
    "        values = np.where(mask, med_vals, values)\n",
    "\n",
    "    # Global per-column median fallback\n",
    "    still_nan = np.isnan(values)\n",
    "    if still_nan.any():\n",
    "        # Check for columns that are entirely NaN after group fill\n",
    "        all_nan_cols = np.where(np.isnan(values).all(axis=0))[0]\n",
    "        if len(all_nan_cols):\n",
    "            drop_cols = [num_feats[i] for i in all_nan_cols]\n",
    "            print(f\"[Impute] Dropping {len(drop_cols)} all-NaN features: {drop_cols}\")\n",
    "            \n",
    "            keep_idx = [i for i in range(values.shape[1]) if i not in all_nan_cols]\n",
    "            values = values[:, keep_idx]\n",
    "            num_feats = [num_feats[i] for i in keep_idx]\n",
    "            df.drop(columns=drop_cols, inplace=True)\n",
    "            still_nan = np.isnan(values)\n",
    "\n",
    "        if still_nan.any():\n",
    "            col_medians = np.nanmedian(values, axis=0)\n",
    "            row_idx, col_idx = np.where(still_nan)\n",
    "            values[row_idx, col_idx] = col_medians[col_idx]\n",
    "\n",
    "    # Write back\n",
    "    df.loc[:, num_feats] = values\n",
    "    df.drop(columns=[\"__len_bin__\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Report imputation summary\n",
    "    after_missing = df[num_feats].isna().sum()\n",
    "    imputed_counts = (before_missing.reindex(num_feats).fillna(0).astype(int)\n",
    "                      - after_missing.reindex(num_feats).fillna(0).astype(int))\n",
    "    total_imputed = int(imputed_counts.sum())\n",
    "    \n",
    "    if total_imputed:\n",
    "        print(f\"\\n[Impute] Filled {total_imputed} missing values across {(imputed_counts > 0).sum()} features\")\n",
    "        top_imputed = imputed_counts[imputed_counts > 0].sort_values(ascending=False).head(10)\n",
    "        print(\"Top 10 imputed features:\")\n",
    "        print(top_imputed.to_string())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_with_features = impute_after_capping(df_with_features, max_missing_pct=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9480101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Correlation] Found 23 highly correlated pairs (>0.85):\n",
      "                  feature_1              feature_2  correlation\n",
      "     right_dependency_ratio  left_dependency_ratio     1.000000\n",
      "               n_tokens_doc                  n_tok     0.999707\n",
      "                n_tokens_ws                  n_tok     0.999104\n",
      "               n_tokens_doc            n_tokens_ws     0.999078\n",
      "                n_tokens_ws                n_chars     0.993868\n",
      "                      n_tok                n_chars     0.993749\n",
      "               n_tokens_doc                n_chars     0.993528\n",
      "            trigram_entropy         bigram_entropy     0.990946\n",
      "              bits_per_char      compression_ratio     0.985084\n",
      "          trigram_diversity       bigram_diversity     0.968594\n",
      "                 smog_index    flesch_reading_ease     0.939904\n",
      "                gunning_fog    flesch_reading_ease     0.934012\n",
      "                 smog_index            gunning_fog     0.929187\n",
      "           whitespace_ratio        avg_word_length     0.928331\n",
      "automated_readability_index            gunning_fog     0.913777\n",
      "          compression_ratio char_trigram_diversity     0.903511\n",
      "          unigram_diversity       type_token_ratio     0.903132\n",
      "              bits_per_char char_trigram_diversity     0.899456\n",
      "           hapax_type_ratio   hapax_legomena_ratio     0.874116\n",
      "            n_sentences_doc            n_tokens_ws     0.865951\n",
      "            n_sentences_doc           n_tokens_doc     0.861596\n",
      "            n_sentences_doc                  n_tok     0.860135\n",
      "     char_trigram_diversity       type_token_ratio     0.859092\n",
      "  Drop 'left_dependency_ratio', keep 'right_dependency_ratio' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_tok', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_tokens_ws', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_chars', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'bigram_entropy', keep 'trigram_entropy' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'compression_ratio', keep 'bits_per_char' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'bigram_diversity', keep 'trigram_diversity' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'flesch_reading_ease', keep 'smog_index' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'gunning_fog', keep 'smog_index' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'avg_word_length', keep 'whitespace_ratio' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'type_token_ratio', keep 'unigram_diversity' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'char_trigram_diversity', keep 'bits_per_char' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'hapax_legomena_ratio', keep 'hapax_type_ratio' (missing: 0.0% vs 0.0%)\n",
      "  Drop 'n_tokens_doc', keep 'n_sentences_doc' (missing: 0.0% vs 0.0%)\n",
      "\n",
      "[Correlation] Dropped 14 redundant features\n",
      "  Remaining features: 43\n"
     ]
    }
   ],
   "source": [
    "def drop_correlated_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str] = None,\n",
    "    threshold: float = 0.85,\n",
    "    method: str = 'pearson',\n",
    "    keep_strategy: str = 'lower_missing'\n",
    ") -> Tuple[pd.DataFrame, List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Identify and drop highly correlated features.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature_cols : List[str], optional\n",
    "        List of feature columns to check. If None, uses all numeric columns except 'is_ai'\n",
    "    threshold : float\n",
    "        Correlation threshold (default 0.85). Pairs above this are considered redundant\n",
    "    method : str\n",
    "        Correlation method: 'pearson', 'spearman', or 'kendall'\n",
    "    keep_strategy : str\n",
    "        Which feature to keep from correlated pairs:\n",
    "        - 'lower_missing': Keep feature with less missing data\n",
    "        - 'higher_variance': Keep feature with higher variance\n",
    "        - 'first': Keep the first feature alphabetically\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df_filtered : pd.DataFrame\n",
    "        Dataframe with redundant features removed\n",
    "    dropped_features : List[str]\n",
    "        List of dropped feature names\n",
    "    corr_pairs : pd.DataFrame\n",
    "        DataFrame showing correlated pairs and correlation values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify feature columns\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                       if c != 'is_ai']\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[feature_cols].corr(method=method).abs()\n",
    "    \n",
    "    # Get upper triangle (avoid double-counting pairs)\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find correlated pairs\n",
    "    correlated_pairs = []\n",
    "    for col in upper_tri.columns:\n",
    "        high_corr = upper_tri[col][upper_tri[col] > threshold]\n",
    "        for idx, corr_val in high_corr.items():\n",
    "            correlated_pairs.append({\n",
    "                'feature_1': col,\n",
    "                'feature_2': idx,\n",
    "                'correlation': corr_val\n",
    "            })\n",
    "    \n",
    "    if not correlated_pairs:\n",
    "        print(f\"[Correlation] No feature pairs with correlation > {threshold}\")\n",
    "        return df, [], pd.DataFrame()\n",
    "    \n",
    "    # Create dataframe of correlated pairs\n",
    "    corr_df = pd.DataFrame(correlated_pairs).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\n[Correlation] Found {len(corr_df)} highly correlated pairs (>{threshold}):\")\n",
    "    print(corr_df.to_string(index=False))\n",
    "    \n",
    "    # Determine which features to drop\n",
    "    to_drop: Set[str] = set()\n",
    "    \n",
    "    for _, row in corr_df.iterrows():\n",
    "        feat1, feat2 = row['feature_1'], row['feature_2']\n",
    "        \n",
    "        # Skip if either already marked for dropping\n",
    "        if feat1 in to_drop or feat2 in to_drop:\n",
    "            continue\n",
    "        \n",
    "        # Decide which to keep based on strategy\n",
    "        if keep_strategy == 'lower_missing':\n",
    "            miss1 = df[feat1].isna().mean()\n",
    "            miss2 = df[feat2].isna().mean()\n",
    "            drop_feat = feat1 if miss1 > miss2 else feat2\n",
    "            keep_feat = feat2 if miss1 > miss2 else feat1\n",
    "            reason = f\"missing: {miss1:.1%} vs {miss2:.1%}\"\n",
    "            \n",
    "        elif keep_strategy == 'higher_variance':\n",
    "            var1 = df[feat1].var()\n",
    "            var2 = df[feat2].var()\n",
    "            drop_feat = feat1 if var1 < var2 else feat2\n",
    "            keep_feat = feat2 if var1 < var2 else feat1\n",
    "            reason = f\"variance: {var1:.2f} vs {var2:.2f}\"\n",
    "            \n",
    "        else:  # 'first'\n",
    "            drop_feat = max(feat1, feat2)  # Drop lexicographically later\n",
    "            keep_feat = min(feat1, feat2)\n",
    "            reason = \"alphabetical\"\n",
    "        \n",
    "        to_drop.add(drop_feat)\n",
    "        print(f\"  Drop '{drop_feat}', keep '{keep_feat}' ({reason})\")\n",
    "    \n",
    "    # Drop features\n",
    "    dropped_list = sorted(to_drop)\n",
    "    df_filtered = df.drop(columns=dropped_list)\n",
    "    \n",
    "    print(f\"\\n[Correlation] Dropped {len(dropped_list)} redundant features\")\n",
    "    print(f\"  Remaining features: {len([c for c in df_filtered.columns if c in feature_cols])}\")\n",
    "    \n",
    "    return df_filtered, dropped_list, corr_df\n",
    "\n",
    "\n",
    "# Usage example\n",
    "df_with_features, dropped_features, correlation_pairs = drop_correlated_features(\n",
    "    df_with_features,\n",
    "    threshold=0.85,\n",
    "    keep_strategy='lower_missing'\n",
    ")\n",
    "\n",
    "# Optional: Visualize correlation matrix before/after\n",
    "def plot_correlation_heatmap(df, feature_cols=None, title=\"Feature Correlations\"):\n",
    "    \"\"\"Plot correlation heatmap for visual inspection\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                       if c != 'is_ai']\n",
    "    \n",
    "    corr = df[feature_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "    sns.heatmap(corr, mask=mask, annot=False, cmap='coolwarm', \n",
    "                center=0, vmin=-1, vmax=1, square=True,\n",
    "                linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize before/after (optional)\n",
    "# plot_correlation_heatmap(df_original, title=\"Before Dropping Correlated Features\")\n",
    "# plot_correlation_heatmap(df_with_features, title=\"After Dropping Correlated Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5695e6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Strategic Drop] Removing 7 redundant features:\n",
      "\n",
      "  ✗ right_dependency_ratio\n",
      "    → Perfect inverse of left_dependency_ratio\n",
      "\n",
      "  ✗ trigram_entropy\n",
      "    → Redundant with trigram_diversity (0.969 correlation)\n",
      "\n",
      "  ✗ bits_per_char\n",
      "    → Redundant with compression_ratio (0.990 correlation)\n",
      "\n",
      "  ✗ smog_index\n",
      "    → Redundant with flesch_reading_ease (0.937 correlation)\n",
      "\n",
      "  ✗ automated_readability_index\n",
      "    → Redundant with gunning_fog (0.903 correlation)\n",
      "\n",
      "  ✗ unigram_diversity\n",
      "    → Redundant with type_token_ratio (0.908 correlation)\n",
      "\n",
      "  ✗ hapax_type_ratio\n",
      "    → Redundant with hapax_legomena_ratio (0.881 correlation)\n",
      "\n",
      "[Success] No correlations > 0.85 remain\n",
      "\n",
      "[Final] 36 features retained\n"
     ]
    }
   ],
   "source": [
    "# >>>> Tailored to my code\n",
    "\n",
    "def drop_correlated_features_strategic(df: pd.DataFrame, threshold: float = 0.85) -> Tuple[pd.DataFrame, dict]:\n",
    "    \"\"\"\n",
    "    Drop correlated features with domain-informed decisions about which to keep.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Manual decisions based on your correlation output\n",
    "    drops = {\n",
    "        # Perfect/near-perfect duplicates - keep the most interpretable\n",
    "        'right_dependency_ratio': 'Perfect inverse of left_dependency_ratio',\n",
    "        'n_tok': 'Duplicate of n_tokens_ws (0.998 correlation)',\n",
    "        'n_chars': 'Redundant with n_tokens_ws (0.987 correlation)',\n",
    "        \n",
    "        # N-gram entropy/diversity - keep diversity (more interpretable)\n",
    "        'bigram_entropy': 'Redundant with bigram_diversity (0.991 correlation)',\n",
    "        'trigram_entropy': 'Redundant with trigram_diversity (0.969 correlation)',\n",
    "        \n",
    "        # Compression metrics - keep ratio (more standard)\n",
    "        'bits_per_char': 'Redundant with compression_ratio (0.990 correlation)',\n",
    "        \n",
    "        # Readability indices - keep Flesch (most widely used)\n",
    "        'smog_index': 'Redundant with flesch_reading_ease (0.937 correlation)',\n",
    "        'gunning_fog': 'Redundant with flesch_reading_ease (0.934 correlation)',\n",
    "        'automated_readability_index': 'Redundant with gunning_fog (0.903 correlation)',\n",
    "        \n",
    "        # Lexical diversity - keep type_token_ratio (simpler)\n",
    "        'unigram_diversity': 'Redundant with type_token_ratio (0.908 correlation)',\n",
    "        'hapax_type_ratio': 'Redundant with hapax_legomena_ratio (0.881 correlation)',\n",
    "        \n",
    "        # Character/compression overlap - keep char_trigram_diversity\n",
    "        # (compression_ratio already kept, so this reduces triple redundancy)\n",
    "    }\n",
    "    \n",
    "    # Check which features actually exist\n",
    "    existing_drops = {k: v for k, v in drops.items() if k in df.columns}\n",
    "    \n",
    "    print(f\"[Strategic Drop] Removing {len(existing_drops)} redundant features:\\n\")\n",
    "    for feat, reason in existing_drops.items():\n",
    "        print(f\"  ✗ {feat}\")\n",
    "        print(f\"    → {reason}\\n\")\n",
    "    \n",
    "    df_filtered = df.drop(columns=list(existing_drops.keys()))\n",
    "    \n",
    "    # Verify no high correlations remain\n",
    "    feature_cols = [c for c in df_filtered.select_dtypes(include=[np.number]).columns \n",
    "                   if c != 'is_ai']\n",
    "    corr_matrix = df_filtered[feature_cols].corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    remaining_high = []\n",
    "    for col in upper_tri.columns:\n",
    "        high_corr = upper_tri[col][upper_tri[col] > threshold]\n",
    "        if not high_corr.empty:\n",
    "            for idx, val in high_corr.items():\n",
    "                remaining_high.append((col, idx, val))\n",
    "    \n",
    "    if remaining_high:\n",
    "        print(f\"[Warning] {len(remaining_high)} pairs still exceed threshold:\")\n",
    "        for f1, f2, corr in sorted(remaining_high, key=lambda x: x[2], reverse=True)[:5]:\n",
    "            print(f\"  {f1} <-> {f2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"[Success] No correlations > {threshold} remain\")\n",
    "    \n",
    "    print(f\"\\n[Final] {len(feature_cols)} features retained\")\n",
    "    \n",
    "    return df_filtered, existing_drops\n",
    "\n",
    "# Apply strategic dropping\n",
    "df_with_features, drop_log = drop_correlated_features_strategic(df_with_features, threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5581ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quality distribution by source_type:\n",
      "quality_score   0   1    2     3\n",
      "model                           \n",
      "chatgpt         0   0    5   542\n",
      "cohere          0   0   10   537\n",
      "cohere-chat     0   0    4   543\n",
      "gpt2            1  12   89   445\n",
      "gpt3            0   0    8   537\n",
      "gpt4            0   0    3   542\n",
      "human           0  11   93  5896\n",
      "llama-chat      0   0   14   531\n",
      "mistral         0  10   57   478\n",
      "mistral-chat    0   1    2   541\n",
      "mpt            11  81  162   290\n",
      "mpt-chat       15  45   48   436\n",
      "Saved enriched dataset: raid_sample_medium_with_features_CLEANED.csv (rows: 12000)\n"
     ]
    }
   ],
   "source": [
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_CLEANED.csv\"\n",
    "\n",
    "# Ensure balanced quality across train/test when you split later\n",
    "print(f\"\\nQuality distribution by source_type:\")\n",
    "print(df_with_features.groupby(['model', 'quality_score']).size().unstack(fill_value=0))\n",
    "\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS} (rows: {len(df_with_features)})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
