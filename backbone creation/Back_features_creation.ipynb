{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6aff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded small dataset with 3000 rows.\n",
      "df_columns: Index(['id', 'adv_source_id', 'source_id', 'model', 'decoding',\n",
      "       'repetition_penalty', 'attack', 'domain', 'title', 'prompt',\n",
      "       'generation', 'is_ai', 'source_type', 'generation_raw', 'had_urls',\n",
      "       'had_html', 'had_code', 'had_table', 'n_chars', 'n_tok', 'alpha_ratio',\n",
      "       'digit_ratio', 'punct_ratio', 'avg_word_length', 'std_word_length',\n",
      "       'entropy_bits', 'entropy_norm', 'is_text_like', 'not_text_reason',\n",
      "       'n_tokens_ws', 'length_bin'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>std_word_length</th>\n",
       "      <th>entropy_bits</th>\n",
       "      <th>entropy_norm</th>\n",
       "      <th>is_text_like</th>\n",
       "      <th>not_text_reason</th>\n",
       "      <th>n_tokens_ws</th>\n",
       "      <th>length_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d0c221a-3f9e-4611-862b-9f9bca3ab465</td>\n",
       "      <td>539e1569-b698-4516-8f27-fda4eaf7e820</td>\n",
       "      <td>4513a0a2-4f95-4496-9717-03e564c26e45</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>sampling</td>\n",
       "      <td>no</td>\n",
       "      <td>paraphrase</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Always For The First Time</td>\n",
       "      <td>Write the body of a poem titled \"Always For Th...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056528</td>\n",
       "      <td>4.883333</td>\n",
       "      <td>2.608267</td>\n",
       "      <td>4.306218</td>\n",
       "      <td>0.846437</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6181f746-70a9-46ff-8d99-4706f6e1ead1</td>\n",
       "      <td>97fa8383-21e7-4129-97b6-3a214b35bc3d</td>\n",
       "      <td>39b13253-a740-4787-88a1-1347f0e0c9ec</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>greedy</td>\n",
       "      <td>no</td>\n",
       "      <td>synonym</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Bogdan Karaičić</td>\n",
       "      <td>The following is the full text of an article t...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043926</td>\n",
       "      <td>0.034165</td>\n",
       "      <td>4.846690</td>\n",
       "      <td>2.492675</td>\n",
       "      <td>4.807520</td>\n",
       "      <td>0.762641</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>312</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328008d2-3cfe-45c1-9add-800a027e8b2e</td>\n",
       "      <td>ecd67188-83c5-4bb4-8b02-7fe276112a35</td>\n",
       "      <td>ecd67188-83c5-4bb4-8b02-7fe276112a35</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>books</td>\n",
       "      <td>Envy: A Luxe Novel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027123</td>\n",
       "      <td>4.798824</td>\n",
       "      <td>2.409930</td>\n",
       "      <td>4.331635</td>\n",
       "      <td>0.771480</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>845</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  5d0c221a-3f9e-4611-862b-9f9bca3ab465  539e1569-b698-4516-8f27-fda4eaf7e820   \n",
       "1  6181f746-70a9-46ff-8d99-4706f6e1ead1  97fa8383-21e7-4129-97b6-3a214b35bc3d   \n",
       "2  328008d2-3cfe-45c1-9add-800a027e8b2e  ecd67188-83c5-4bb4-8b02-7fe276112a35   \n",
       "\n",
       "                              source_id    model  decoding repetition_penalty  \\\n",
       "0  4513a0a2-4f95-4496-9717-03e564c26e45  chatgpt  sampling                 no   \n",
       "1  39b13253-a740-4787-88a1-1347f0e0c9ec     gpt3    greedy                 no   \n",
       "2  ecd67188-83c5-4bb4-8b02-7fe276112a35    human       NaN                NaN   \n",
       "\n",
       "             attack  domain                      title  \\\n",
       "0        paraphrase  poetry  Always For The First Time   \n",
       "1           synonym    wiki            Bogdan Karaičić   \n",
       "2  zero_width_space   books         Envy: A Luxe Novel   \n",
       "\n",
       "                                              prompt  ... digit_ratio  \\\n",
       "0  Write the body of a poem titled \"Always For Th...  ...    0.000000   \n",
       "1  The following is the full text of an article t...  ...    0.043926   \n",
       "2                                                NaN  ...    0.000000   \n",
       "\n",
       "   punct_ratio avg_word_length std_word_length  entropy_bits  entropy_norm  \\\n",
       "0     0.056528        4.883333        2.608267      4.306218      0.846437   \n",
       "1     0.034165        4.846690        2.492675      4.807520      0.762641   \n",
       "2     0.027123        4.798824        2.409930      4.331635      0.771480   \n",
       "\n",
       "   is_text_like  not_text_reason  n_tokens_ws  length_bin  \n",
       "0          True              NaN          116       short  \n",
       "1          True              NaN          312        long  \n",
       "2          True              NaN          845        long  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuration and dataset selection\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths to the three samples\n",
    "SAMPLE_PATHS = {\n",
    "    \"small\":  \"raid_sample_small.csv\",\n",
    "    \"medium\": \"raid_sample_medium.csv\",\n",
    "    \"large\":  \"raid_sample_large.csv\",\n",
    "}\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Set this to one of: \"small\", \"medium\", \"large\"\n",
    "SELECTED_DATASET = \"small\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "TEXT_COL = \"generation\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATHS[SELECTED_DATASET])\n",
    "print(f\"Loaded {SELECTED_DATASET} dataset with {len(df)} rows.\")\n",
    "print(f\"df_columns: {df.columns}\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b6afd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy, CMUdict (NLTK), and g2p_en fallback\n",
    "\n",
    "# Ensure CMUdict is available\n",
    "nltk.download('cmudict', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "CMU = cmudict.dict()  # key: lowercase word, value: list of pronunciations (list of ARPAbet tokens)\n",
    "\n",
    "# Load spaCy English model (use 'en_core_web_sm' unless you already have md/lg installed)\n",
    "try:\n",
    "    nlp: Language = spacy.load(\"en_core_web_lg\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure sentence boundaries are available (parser usually handles this; add sentencizer if needed)\n",
    "if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# g2p_en for OOV coverage\n",
    "G2P = G2p()\n",
    "\n",
    "# ARPAbet vowel bases (stress digits removed when checking)\n",
    "ARPA_VOWELS = {\n",
    "    \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
    "    \"EH\", \"ER\", \"EY\",\n",
    "    \"IH\", \"IY\",\n",
    "    \"OW\", \"OY\",\n",
    "    \"UH\", \"UW\"\n",
    "}\n",
    "\n",
    "# Cache syllable counts for speed\n",
    "_SYLL_CACHE: Dict[str, int] = {}\n",
    "\n",
    "def cmu_syllables(word: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Returns syllable count using CMUdict if available; else None.\n",
    "    Policy: use the first pronunciation variant.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w not in CMU:\n",
    "        return None\n",
    "    phones = CMU[w][0]\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    return max(count, 1)  # at least one for non-empty alphabetic words\n",
    "\n",
    "def g2p_syllables(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns syllable count using neural g2p_en; counts vowel phonemes.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w in _SYLL_CACHE:\n",
    "        return _SYLL_CACHE[w]\n",
    "    phones = G2P(w)\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    # Guard: ensure >=1 for alphabetic tokens\n",
    "    if count == 0 and re.search(r\"[A-Za-z]\", w):\n",
    "        count = 1\n",
    "    _SYLL_CACHE[w] = count\n",
    "    return count\n",
    "\n",
    "def syllables_hybrid(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Hybrid policy: try CMUdict first; if OOV, fall back to g2p_en.\n",
    "    \"\"\"\n",
    "    c = cmu_syllables(word)\n",
    "    if c is not None:\n",
    "        return c\n",
    "    return g2p_syllables(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3020ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation utilities using spaCy + CMUdict with g2p_en fallback\n",
    "\n",
    "import zlib\n",
    "\n",
    "# permissive fallback segmentation for pathological cases\n",
    "_FALLBACK_SPLIT = re.compile(r'(?<=[\\.!?])\\s+|[\\r\\n]+|(?<=;)\\s+|(?<=:)\\s+')\n",
    "\n",
    "def resegmentize_if_needed(text: str, nlp: Language, asl_hi: int = 100, min_tokens: int = 120, doc=None):\n",
    "    \"\"\"Return fallback sentence strings when avg sentence length is extreme.\"\"\"\n",
    "    doc = doc or nlp(text)\n",
    "    sents = list(doc.sents)\n",
    "    n_sent = len(sents)\n",
    "    n_tok = len(doc)\n",
    "    avg_len = (n_tok / max(n_sent, 1)) if n_tok else 0\n",
    "\n",
    "    if (n_tok >= min_tokens) and (n_sent <= 2 or avg_len >= asl_hi):\n",
    "        parts = [s.strip() for s in _FALLBACK_SPLIT.split(text) if s and not s.isspace()]\n",
    "        return parts\n",
    "    return None\n",
    "\n",
    "def prepare_doc(text: str, nlp: Language, skip_reseg: bool = False) -> Tuple[Doc, bool]:\n",
    "    \"\"\"\n",
    "    Parse text with spaCy and optionally resegment when heuristics trigger.\n",
    "    Set skip_reseg=True when you need accurate dependency trees.\n",
    "    \"\"\"\n",
    "    primary_doc = nlp(text)\n",
    "    \n",
    "    if skip_reseg:\n",
    "        return primary_doc, False\n",
    "    \n",
    "    fallback = resegmentize_if_needed(text, nlp, doc=primary_doc)\n",
    "    if fallback is None:\n",
    "        return primary_doc, False\n",
    "    \n",
    "    fixed_text = \". \".join(fallback)\n",
    "    return nlp(fixed_text), True\n",
    "\n",
    "def can_compute_readability(n_tokens: int, n_sents: int) -> bool:\n",
    "    return (n_tokens >= 100) and (n_sents >= 3)\n",
    "\n",
    "def safe_readability(tokens: int, sentences: int, syllable_counts: List[int], chars_per_word: float, complex_words: int, polysyllables: int) -> Dict[str, float]:\n",
    "    \"\"\"Safely compute readability metrics, falling back to NaN when unstable.\"\"\"\n",
    "    out = {\n",
    "        \"flesch_reading_ease\": np.nan,\n",
    "        \"gunning_fog\": np.nan,\n",
    "        \"smog_index\": np.nan,\n",
    "        \"automated_readability_index\": np.nan,\n",
    "    }\n",
    "    if not can_compute_readability(tokens, sentences):\n",
    "        return out\n",
    "\n",
    "    words = max(tokens, 1)\n",
    "    sents = max(sentences, 1)\n",
    "    syllables = max(int(np.sum(syllable_counts)), 1)\n",
    "    chars_per_word = float(chars_per_word) if chars_per_word else 0.0\n",
    "    complex_words = max(complex_words, 0)\n",
    "    polysyllables = max(polysyllables, 0)\n",
    "\n",
    "    out[\"flesch_reading_ease\"] = 206.835 - 1.015 * (words / sents) - 84.6 * (syllables / words)\n",
    "    out[\"gunning_fog\"] = 0.4 * ((words / sents) + 100.0 * (complex_words / words))\n",
    "    out[\"smog_index\"] = (1.043 * math.sqrt(30.0 * (polysyllables / sents)) + 3.1291) if polysyllables > 0 else np.nan\n",
    "    out[\"automated_readability_index\"] = 4.71 * chars_per_word + 0.5 * (words / sents) - 21.43\n",
    "    return out\n",
    "\n",
    "def _word_like(tok) -> bool:\n",
    "    \"\"\"Select lexical tokens (alphabetic, not space).\"\"\"\n",
    "    return tok.is_alpha and not tok.is_space\n",
    "\n",
    "def _alnum_char_count(token_text: str) -> int:\n",
    "    \"\"\"Count alphanumeric characters for ARI; excludes whitespace and punctuation.\"\"\"\n",
    "    return sum(ch.isalnum() for ch in token_text)\n",
    "\n",
    "def features_from_doc(doc: Doc, text: str, *, resegmented: bool = False) -> Dict[str, float]:\n",
    "    \"\"\"Compute core lexical and readability-driven metrics with guards.\"\"\"\n",
    "    if doc.has_annotation(\"SENT_START\"):\n",
    "        sents = list(doc.sents)\n",
    "    else:\n",
    "        sents = list(doc.sents) if hasattr(doc, \"sents\") else [doc]\n",
    "    if not sents:\n",
    "        sents = [doc]\n",
    "    n_sents = len(sents)\n",
    "\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    punct_toks = [t for t in doc if t.is_punct]\n",
    "    nonspace_toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "    n_tokens = len(word_toks)\n",
    "    sent_word_counts = [sum(1 for t in sent if _word_like(t)) for sent in sents]\n",
    "    avg_sentence_length = float(np.mean(sent_word_counts)) if sent_word_counts else np.nan\n",
    "    sentence_length_std = float(np.std(sent_word_counts, ddof=0)) if len(sent_word_counts) > 1 else np.nan\n",
    "\n",
    "    word_lengths = [len(t.text) for t in word_toks] \n",
    "    avg_word_length = float(np.mean(word_lengths)) if word_lengths else np.nan\n",
    "\n",
    "    vocab = {t.text.lower() for t in word_toks}\n",
    "    type_token_ratio = (len(vocab) / n_tokens) if n_tokens > 0 else np.nan\n",
    "\n",
    "    stop_count = sum(1 for t in word_toks if t.is_stop)\n",
    "    stopword_ratio = (stop_count / n_tokens) if n_tokens > 0 else np.nan\n",
    "\n",
    "    chars_alnum = sum(_alnum_char_count(t.text) for t in nonspace_toks)\n",
    "    punct_chars = sum(len(t.text) for t in punct_toks)\n",
    "    nonspace_chars = sum(len(t.text) for t in nonspace_toks)\n",
    "    punctuation_ratio = (punct_chars / nonspace_chars) if nonspace_chars > 0 else np.nan\n",
    "\n",
    "    syllable_counts = [syllables_hybrid(t.text) for t in word_toks]\n",
    "    polysyllables = sum(1 for syl in syllable_counts if syl >= 3)\n",
    "    complex_words = polysyllables\n",
    "    chars_per_word = (chars_alnum / n_tokens) if n_tokens > 0 else 0.0\n",
    "\n",
    "    readability = safe_readability(\n",
    "        tokens=n_tokens,\n",
    "        sentences=n_sents,\n",
    "        syllable_counts=syllable_counts,\n",
    "        chars_per_word=chars_per_word,\n",
    "        complex_words=complex_words,\n",
    "        polysyllables=polysyllables,\n",
    "    )\n",
    "\n",
    "    features = {\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"punctuation_ratio\": punctuation_ratio,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"sentence_length_std\": sentence_length_std,\n",
    "        \"n_tokens_doc\": float(n_tokens),\n",
    "        \"n_sentences_doc\": float(n_sents),\n",
    "        \"resegmented\": bool(resegmented),\n",
    "    }\n",
    "    features.update(readability)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74acf093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram and burstiness utility functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# N-gram feature extraction utilities\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def ngram_diversity(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate n-gram diversity (unique n-grams / total n-grams).\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def ngram_entropy(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of n-gram distribution.\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "\n",
    "    counts = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_burstiness(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate burstiness coefficient based on word frequency distribution.\n",
    "    Burstiness = (sigma - mu) / (sigma + mu)\n",
    "    where mu is mean frequency and sigma is standard deviation.\n",
    "    Returns NaN if statistics are undefined.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return np.nan\n",
    "\n",
    "    word_counts = Counter(tokens)\n",
    "    frequencies = list(word_counts.values())\n",
    "\n",
    "    if len(frequencies) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    mu = np.mean(frequencies)\n",
    "    sigma = np.std(frequencies, ddof=0)\n",
    "\n",
    "    if mu + sigma == 0:\n",
    "        return np.nan\n",
    "\n",
    "    return (sigma - mu) / (sigma + mu)\n",
    "\n",
    "def safe_ngram_stats(tokens: List[str], n: int = 2, min_ngrams: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Return diversity and entropy for n-grams when sample size is sufficient.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return {\"diversity\": np.nan, \"entropy\": np.nan}\n",
    "\n",
    "    grams = extract_ngrams(tokens, n)\n",
    "    if len(grams) < min_ngrams:\n",
    "        return {\"diversity\": np.nan, \"entropy\": np.nan}\n",
    "\n",
    "    counts = Counter(grams)\n",
    "    diversity = len(counts) / len(grams)\n",
    "    probs = np.array(list(counts.values()), dtype=float) / len(grams)\n",
    "    entropy = float(-np.sum(probs * np.log2(probs)))\n",
    "    return {\"diversity\": diversity, \"entropy\": entropy}\n",
    "\n",
    "print(\"N-gram and burstiness utility functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9638c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Character-level feature extraction\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def character_ngram_features(text: str, n: int = 3) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Extract character n-gram diversity and entropy.\n",
    "    Returns diversity ratio and entropy for character n-grams.\n",
    "    \"\"\"\n",
    "    if len(text) < n:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    char_ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "\n",
    "    if not char_ngrams:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    diversity = len(set(char_ngrams)) / len(char_ngrams)\n",
    "\n",
    "    counts = Counter(char_ngrams)\n",
    "    total = len(char_ngrams)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "\n",
    "    return diversity, entropy\n",
    "\n",
    "def compression_features(text: str) -> Dict[str, float]:\n",
    "    \"\"\"Compute compression ratio and bits-per-character using zlib.\"\"\"\n",
    "    encoded = text.encode(\"utf-8\")\n",
    "    raw_len = len(encoded)\n",
    "    if raw_len == 0:\n",
    "        return {\"compression_ratio\": np.nan, \"bits_per_char\": np.nan}\n",
    "\n",
    "    compressed = zlib.compress(encoded, level=6)\n",
    "    ratio = len(compressed) / raw_len\n",
    "    bits_per_char = 8.0 * len(compressed) / raw_len\n",
    "    return {\"compression_ratio\": ratio, \"bits_per_char\": bits_per_char}\n",
    "\n",
    "def compression_ratio(text: str) -> float:\n",
    "    \"\"\"Backward-compatible helper returning only the compression ratio.\"\"\"\n",
    "    return compression_features(text)[\"compression_ratio\"]\n",
    "\n",
    "def character_statistics(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract surface-level character statistics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            \"uppercase_ratio\": np.nan,\n",
    "            \"digit_ratio\": np.nan,\n",
    "            \"whitespace_ratio\": np.nan,\n",
    "            \"unique_char_count\": np.nan,\n",
    "        }\n",
    "\n",
    "    total_chars = len(text)\n",
    "\n",
    "    return {\n",
    "        \"uppercase_ratio\": sum(1 for c in text if c.isupper()) / total_chars,\n",
    "        \"digit_ratio\": sum(1 for c in text if c.isdigit()) / total_chars,\n",
    "        \"whitespace_ratio\": sum(1 for c in text if c.isspace()) / total_chars,\n",
    "        \"unique_char_count\": float(len(set(text))),\n",
    "    }\n",
    "\n",
    "print(\"Character-level feature functions loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74b9e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "from spacy.tokens import Doc, Token\n",
    "\n",
    "def _root_chain_depth(token: Token, max_steps: int) -> int:\n",
    "    \"\"\"\n",
    "    Length of the head chain from `token` up to the sentence/doc root.\n",
    "    Robust to cycles and malformed heads by bounding steps and tracking indices.\n",
    "    \"\"\"\n",
    "    depth = 0\n",
    "    cur = token\n",
    "    visited_idx = set()\n",
    "\n",
    "    # Use token indices (doc-relative) for identity; this is stable.\n",
    "    while cur.head.i != cur.i:\n",
    "        if cur.i in visited_idx:\n",
    "            # Cycle detected: bail out with 0 depth for this token\n",
    "            return 0\n",
    "        visited_idx.add(cur.i)\n",
    "\n",
    "        depth += 1\n",
    "        if depth > max_steps:\n",
    "            # Malformed graph (excessive chain): cap and exit\n",
    "            return max_steps\n",
    "\n",
    "        cur = cur.head\n",
    "    return depth\n",
    "\n",
    "def sent_max_depth(sent) -> int:\n",
    "    \"\"\"\n",
    "    Longest head-chain root->leaf depth within a sentence span.\n",
    "    Traversal is over the full doc-level heads; we only *measure* per sentence.\n",
    "    \"\"\"\n",
    "    if len(sent) == 0:\n",
    "        return 0\n",
    "\n",
    "    # A conservative upper bound: number of tokens in the *doc*\n",
    "    # (not just in the sentence), to safely handle cross-sentence heads.\n",
    "    max_steps = len(sent.doc) + 5\n",
    "\n",
    "    return max((_root_chain_depth(tok, max_steps) for tok in sent), default=0)\n",
    "\n",
    "def doc_depth_stats(doc: Doc) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Average and maximum tree depth across sentences in the original Doc.\n",
    "    Falls back gracefully for empty docs.\n",
    "    \"\"\"\n",
    "    sentences = list(doc.sents) if doc.has_annotation(\"SENT_START\") or hasattr(doc, \"sents\") else [doc]\n",
    "    if not sentences:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    depths = [sent_max_depth(sent) for sent in sentences if len(sent) > 0]\n",
    "    if not depths:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    return float(np.mean(depths)), float(np.max(depths))\n",
    "\n",
    "def dependency_tree_features(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract robust dependency-tree structural features from the original Doc.\n",
    "    Avoids Span.as_doc(), uses doc-level indices and bounds traversal.\n",
    "    \"\"\"\n",
    "    sentences = list(doc.sents) if doc.has_annotation(\"SENT_START\") or hasattr(doc, \"sents\") else [doc]\n",
    "    if not sentences:\n",
    "        sentences = [doc]\n",
    "\n",
    "    avg_depth, max_depth = doc_depth_stats(doc)\n",
    "\n",
    "    per_sentence_distances = []\n",
    "    left_deps = 0\n",
    "    right_deps = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        distances = []\n",
    "        for token in sent:\n",
    "            # Skip roots\n",
    "            if token.head.i == token.i:\n",
    "                continue\n",
    "            # Distance computed in doc coordinates is fine:\n",
    "            d = abs(token.i - token.head.i)\n",
    "            distances.append(d)\n",
    "            if token.i < token.head.i:\n",
    "                left_deps += 1\n",
    "            else:\n",
    "                right_deps += 1\n",
    "        if distances:\n",
    "            per_sentence_distances.append(float(np.mean(distances)))\n",
    "\n",
    "    avg_dep_distance = float(np.mean(per_sentence_distances)) if per_sentence_distances else 0.0\n",
    "    total_deps = left_deps + right_deps\n",
    "    left_ratio = (left_deps / total_deps) if total_deps else 0.0\n",
    "    right_ratio = (right_deps / total_deps) if total_deps else 0.0\n",
    "\n",
    "    return {\n",
    "        \"avg_tree_depth\": avg_depth,\n",
    "        \"max_tree_depth\": max_depth,\n",
    "        \"avg_dependency_distance\": avg_dep_distance,\n",
    "        \"left_dependency_ratio\": left_ratio,\n",
    "        \"right_dependency_ratio\": right_ratio,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89de6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable, Tuple, Optional\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "\n",
    "# ---------------------------\n",
    "# Token normalization helpers\n",
    "# ---------------------------\n",
    "\n",
    "def _normalize_tokens(\n",
    "    tokens: Iterable,\n",
    "    *,\n",
    "    lower: bool = True,\n",
    "    alpha_only: bool = True,\n",
    "    min_len: int = 2,\n",
    "    use_lemma: bool = False\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Normalize a sequence of tokens (strings or spaCy tokens).\n",
    "    - lower: lowercases\n",
    "    - alpha_only: keep tokens with .isalpha() (falls back to regex if string)\n",
    "    - min_len: drop tokens shorter than this after normalization\n",
    "    - use_lemma: if spaCy tokens are provided, use token.lemma_\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for t in tokens:\n",
    "        s = t\n",
    "        # spaCy Token support\n",
    "        if hasattr(t, \"lemma_\") and use_lemma:\n",
    "            s = t.lemma_\n",
    "        elif hasattr(t, \"text\"):\n",
    "            s = t.text\n",
    "\n",
    "        if not isinstance(s, str):\n",
    "            s = str(s)\n",
    "\n",
    "        if lower:\n",
    "            s = s.lower()\n",
    "\n",
    "        if alpha_only:\n",
    "            # use spaCy attribute if available, else a regex fallback\n",
    "            if hasattr(t, \"is_alpha\"):\n",
    "                if not t.is_alpha:\n",
    "                    continue\n",
    "            else:\n",
    "                if not re.match(r\"^[a-zA-Z]+$\", s):\n",
    "                    continue\n",
    "\n",
    "        if len(s) < min_len:\n",
    "            continue\n",
    "\n",
    "        out.append(s)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Hapax ratio (stable variant)\n",
    "# ---------------------------\n",
    "\n",
    "def hapax_legomena_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Ratio of hapax tokens to TOTAL TOKENS (your original definition).\n",
    "    More common in lexicography is hapax / types; we keep your denominator,\n",
    "    but you may prefer hapax / unique_types for length-robustness.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(tokens)\n",
    "    hapax = sum(1 for c in cnt.values() if c == 1)\n",
    "    return hapax / float(len(tokens))\n",
    "\n",
    "\n",
    "def hapax_type_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Optional: hapax to TYPES ratio (often more stable than hapax/token).\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(tokens)\n",
    "    types = len(cnt)\n",
    "    if types == 0:\n",
    "        return float(\"nan\")\n",
    "    hapax = sum(1 for c in cnt.values() if c == 1)\n",
    "    return hapax / float(types)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Yule's K (guarded)\n",
    "# ---------------------------\n",
    "\n",
    "def yules_k(tokens: List[str], *, min_tokens: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Yule's K = 10^4 * (sum v^2 * V_v - N) / N^2\n",
    "    Guarded against short texts; returns NaN if N < min_tokens.\n",
    "    \"\"\"\n",
    "    N = len(tokens)\n",
    "    if N < min_tokens:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    cnt = Counter(tokens)\n",
    "    spectrum = Counter(cnt.values())  # V_v\n",
    "    # sum v^2 * V_v\n",
    "    s2 = sum((v * v) * Vv for v, Vv in spectrum.items())\n",
    "\n",
    "    # use float64\n",
    "    Nf = float(N)\n",
    "    K = 10000.0 * (s2 - Nf) / (Nf * Nf)\n",
    "    return float(K)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# MTLD (standard, robust)\n",
    "# ---------------------------\n",
    "\n",
    "def _mtld_one_pass(tokens: List[str], threshold: float, min_segment: int) -> float:\n",
    "    \"\"\"\n",
    "    One-direction MTLD pass (standard algorithm):\n",
    "    Accumulate a segment until TTR falls below threshold; count a factor and reset.\n",
    "    The final partial segment contributes a fractional factor.\n",
    "    Returns the number of factors observed.\n",
    "    \"\"\"\n",
    "    types = set()\n",
    "    token_count = 0\n",
    "    factor_count = 0.0\n",
    "\n",
    "    for tok in tokens:\n",
    "        token_count += 1\n",
    "        types.add(tok)\n",
    "        ttr = len(types) / float(token_count)\n",
    "\n",
    "        # Only allow a factor to close if we have a minimally meaningful segment\n",
    "        if (ttr < threshold) and (token_count >= min_segment):\n",
    "            factor_count += 1.0\n",
    "            types.clear()\n",
    "            token_count = 0\n",
    "\n",
    "    # partial segment contribution\n",
    "    if token_count > 0:\n",
    "        # If ttr is already above threshold, this adds <1 factor,\n",
    "        # otherwise adds a smaller fraction.\n",
    "        ttr = len(types) / float(token_count)\n",
    "        if ttr != 1.0:  # avoid division by zero in degenerate case\n",
    "            factor_count += (1.0 - ttr) / (1.0 - threshold)\n",
    "        else:\n",
    "            # maximally diverse partial segment: count a tiny fraction\n",
    "            factor_count += 0.0\n",
    "\n",
    "    return factor_count\n",
    "\n",
    "\n",
    "def mtld(\n",
    "    tokens: List[str],\n",
    "    threshold: float = 0.72,\n",
    "    *,\n",
    "    min_tokens: int = 200,\n",
    "    min_segment: int = 50\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Measure of Textual Lexical Diversity (MTLD), forward/backward average.\n",
    "    Guard-rails:\n",
    "      - require at least `min_tokens` tokens, else NaN\n",
    "      - clamp threshold to [0.60, 0.80]\n",
    "      - enforce `min_segment` for factor completion\n",
    "    \"\"\"\n",
    "    n = len(tokens)\n",
    "    if n < min_tokens:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    threshold = max(0.60, min(0.80, float(threshold)))\n",
    "\n",
    "    f = _mtld_one_pass(tokens, threshold, min_segment)\n",
    "    b = _mtld_one_pass(list(reversed(tokens)), threshold, min_segment)\n",
    "\n",
    "    # If both are zero (pathological), return NaN rather than n/0\n",
    "    vals = [x for x in (f, b) if x > 0.0]\n",
    "    if not vals:\n",
    "        return float(\"nan\")\n",
    "\n",
    "    mean_factors = float(np.mean(vals))\n",
    "    return n / mean_factors\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Aggregator with normalization\n",
    "# ---------------------------\n",
    "\n",
    "def vocabulary_sophistication_features(\n",
    "    tokens: List,\n",
    "    *,\n",
    "    normalize: str = \"lower\",   # {\"none\",\"lower\",\"lemma\"}\n",
    "    alpha_only: bool = True,\n",
    "    min_len: int = 2,\n",
    "    use_lemma_if_spacy: Optional[bool] = None\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    - Accepts raw strings or spaCy Tokens.\n",
    "    - Normalizes then computes robust metrics with NaN for undersized texts.\n",
    "    - Adds hapax/type as a more length-stable complement (does not replace your original).\n",
    "    \"\"\"\n",
    "    if normalize not in {\"none\", \"lower\", \"lemma\"}:\n",
    "        raise ValueError(\"normalize must be one of {'none','lower','lemma'}\")\n",
    "\n",
    "    if use_lemma_if_spacy is None:\n",
    "        use_lemma_if_spacy = (normalize == \"lemma\")\n",
    "\n",
    "    toks = _normalize_tokens(\n",
    "        tokens,\n",
    "        lower=(normalize == \"lower\"),\n",
    "        alpha_only=alpha_only,\n",
    "        min_len=min_len,\n",
    "        use_lemma=use_lemma_if_spacy\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"hapax_legomena_ratio\": hapax_legomena_ratio(toks),\n",
    "        \"hapax_type_ratio\":     hapax_type_ratio(toks),\n",
    "        \"yules_k\":              yules_k(toks, min_tokens=100),\n",
    "        \"mtld\":                 mtld(toks, threshold=0.72, min_tokens=200, min_segment=50),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40dfb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation pattern functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Punctuation pattern analysis\n",
    "\n",
    "def punctuation_patterns(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detailed punctuation pattern features beyond simple ratio.\n",
    "    \"\"\"\n",
    "    all_tokens = [t for t in doc if not t.is_space]\n",
    "    punct_tokens = [t for t in doc if t.is_punct]\n",
    "    \n",
    "    if not all_tokens:\n",
    "        return {\n",
    "            \"comma_ratio\": 0.0,\n",
    "            \"period_ratio\": 0.0,\n",
    "            \"question_ratio\": 0.0,\n",
    "            \"exclamation_ratio\": 0.0,\n",
    "            \"semicolon_ratio\": 0.0,\n",
    "            \"colon_ratio\": 0.0,\n",
    "            \"quote_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total = len(all_tokens)\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punct_text = ''.join([t.text for t in punct_tokens])\n",
    "    \n",
    "    return {\n",
    "        \"comma_ratio\": punct_text.count(',') / total,\n",
    "        \"period_ratio\": punct_text.count('.') / total,\n",
    "        \"question_ratio\": punct_text.count('?') / total,\n",
    "        \"exclamation_ratio\": punct_text.count('!') / total,\n",
    "        \"semicolon_ratio\": punct_text.count(';') / total,\n",
    "        \"colon_ratio\": punct_text.count(':') / total,\n",
    "        \"quote_ratio\": (punct_text.count('\"') + punct_text.count(\"'\")) / total,\n",
    "    }\n",
    "\n",
    "print(\"Punctuation pattern functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_features(text: str, doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract sentiment and emotional tone features.\n",
    "    Uses TextBlob for polarity and subjectivity.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"sentiment_subjectivity\": 0.0,\n",
    "            \"sentiment_polarity_variance\": 0.0,\n",
    "            \"positive_word_ratio\": 0.0,\n",
    "            \"negative_word_ratio\": 0.0,\n",
    "            \"neutral_sentence_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Overall document sentiment\n",
    "    blob = TextBlob(text)\n",
    "    features = {\n",
    "        \"sentiment_polarity\": blob.sentiment.polarity,  # -1 (negative) to 1 (positive)\n",
    "        \"sentiment_subjectivity\": blob.sentiment.subjectivity,  # 0 (objective) to 1 (subjective)\n",
    "    }\n",
    "    \n",
    "    # Sentence-level sentiment variance\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    sent_polarities = []\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_blob = TextBlob(sent.text)\n",
    "        polarity = sent_blob.sentiment.polarity\n",
    "        sent_polarities.append(polarity)\n",
    "        \n",
    "        # Count neutral sentences (polarity close to 0)\n",
    "        if abs(polarity) < 0.1:\n",
    "            neutral_count += 1\n",
    "    \n",
    "    features[\"sentiment_polarity_variance\"] = float(np.var(sent_polarities)) if len(sent_polarities) > 1 else 0.0\n",
    "    features[\"neutral_sentence_ratio\"] = neutral_count / len(sents) if sents else 0.0\n",
    "    \n",
    "    # Positive/negative word ratios using spaCy tokens\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    if word_toks:\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for token in word_toks:\n",
    "            word_blob = TextBlob(token.text.lower())\n",
    "            polarity = word_blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                positive_count += 1\n",
    "            elif polarity < -0.1:\n",
    "                negative_count += 1\n",
    "        \n",
    "        features[\"positive_word_ratio\"] = positive_count / len(word_toks)\n",
    "        features[\"negative_word_ratio\"] = negative_count / len(word_toks)\n",
    "    else:\n",
    "        features[\"positive_word_ratio\"] = 0.0\n",
    "        features[\"negative_word_ratio\"] = 0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1f44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature columns:\n",
      "['avg_word_length', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'digit_ratio', 'whitespace_ratio', 'unique_char_count', 'compression_ratio', 'bits_per_char', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'hapax_type_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio', 'had_urls', 'had_html', 'had_code', 'had_table']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <th>had_urls</th>\n",
       "      <th>had_html</th>\n",
       "      <th>had_code</th>\n",
       "      <th>had_table</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5d0c221a-3f9e-4611-862b-9f9bca3ab465</td>\n",
       "      <td>539e1569-b698-4516-8f27-fda4eaf7e820</td>\n",
       "      <td>4513a0a2-4f95-4496-9717-03e564c26e45</td>\n",
       "      <td>chatgpt</td>\n",
       "      <td>sampling</td>\n",
       "      <td>no</td>\n",
       "      <td>paraphrase</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Always For The First Time</td>\n",
       "      <td>Write the body of a poem titled \"Always For Th...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245746</td>\n",
       "      <td>0.559324</td>\n",
       "      <td>0.025961</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.008333</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6181f746-70a9-46ff-8d99-4706f6e1ead1</td>\n",
       "      <td>97fa8383-21e7-4129-97b6-3a214b35bc3d</td>\n",
       "      <td>39b13253-a740-4787-88a1-1347f0e0c9ec</td>\n",
       "      <td>gpt3</td>\n",
       "      <td>greedy</td>\n",
       "      <td>no</td>\n",
       "      <td>synonym</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Bogdan Karaičić</td>\n",
       "      <td>The following is the full text of an article t...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.302778</td>\n",
       "      <td>0.269444</td>\n",
       "      <td>0.129025</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.027972</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>328008d2-3cfe-45c1-9add-800a027e8b2e</td>\n",
       "      <td>ecd67188-83c5-4bb4-8b02-7fe276112a35</td>\n",
       "      <td>ecd67188-83c5-4bb4-8b02-7fe276112a35</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>books</td>\n",
       "      <td>Envy: A Luxe Novel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153908</td>\n",
       "      <td>0.472417</td>\n",
       "      <td>0.070823</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.044864</td>\n",
       "      <td>0.020071</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  5d0c221a-3f9e-4611-862b-9f9bca3ab465  539e1569-b698-4516-8f27-fda4eaf7e820   \n",
       "1  6181f746-70a9-46ff-8d99-4706f6e1ead1  97fa8383-21e7-4129-97b6-3a214b35bc3d   \n",
       "2  328008d2-3cfe-45c1-9add-800a027e8b2e  ecd67188-83c5-4bb4-8b02-7fe276112a35   \n",
       "\n",
       "                              source_id    model  decoding repetition_penalty  \\\n",
       "0  4513a0a2-4f95-4496-9717-03e564c26e45  chatgpt  sampling                 no   \n",
       "1  39b13253-a740-4787-88a1-1347f0e0c9ec     gpt3    greedy                 no   \n",
       "2  ecd67188-83c5-4bb4-8b02-7fe276112a35    human       NaN                NaN   \n",
       "\n",
       "             attack  domain                      title  \\\n",
       "0        paraphrase  poetry  Always For The First Time   \n",
       "1           synonym    wiki            Bogdan Karaičić   \n",
       "2  zero_width_space   books         Envy: A Luxe Novel   \n",
       "\n",
       "                                              prompt  ... sentiment_polarity  \\\n",
       "0  Write the body of a poem titled \"Always For Th...  ...           0.245746   \n",
       "1  The following is the full text of an article t...  ...           0.302778   \n",
       "2                                                NaN  ...           0.153908   \n",
       "\n",
       "   sentiment_subjectivity sentiment_polarity_variance neutral_sentence_ratio  \\\n",
       "0                0.559324                    0.025961               0.333333   \n",
       "1                0.269444                    0.129025               0.550000   \n",
       "2                0.472417                    0.070823               0.500000   \n",
       "\n",
       "   positive_word_ratio  negative_word_ratio  had_urls  had_html  had_code  \\\n",
       "0             0.075000             0.008333     False     False     False   \n",
       "1             0.027972             0.006993     False     False     False   \n",
       "2             0.044864             0.020071     False     False     False   \n",
       "\n",
       "   had_table  \n",
       "0      False  \n",
       "1      False  \n",
       "2      False  \n",
       "\n",
       "[3 rows x 85 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application doc processing with segmentation safeguards\n",
    "\n",
    "def window_tokens(doc: Doc, max_tokens: int = 500) -> List[str]:\n",
    "    \"\"\"Lowercase alphabetic tokens clipped to a comparison window.\"\"\"\n",
    "    return [t.text.lower() for t in doc if _word_like(t)][:max_tokens]\n",
    "\n",
    "feature_rows: List[Dict[str, float]] = []\n",
    "for idx, row in df.reset_index(drop=True).iterrows():\n",
    "    text = str(row.get(TEXT_COL, \"\"))\n",
    "    \n",
    "    # Use resegmented doc for readability and most features\n",
    "    doc, resegmented = prepare_doc(text, nlp)\n",
    "    doc_features = dict(features_from_doc(doc, text, resegmented=resegmented))\n",
    "\n",
    "    tok_win = window_tokens(doc, max_tokens=500)\n",
    "    unigram = safe_ngram_stats(tok_win, n=1, min_ngrams=100)\n",
    "    bigram = safe_ngram_stats(tok_win, n=2, min_ngrams=100)\n",
    "    trigram = safe_ngram_stats(tok_win, n=3, min_ngrams=100)\n",
    "    doc_features[\"unigram_diversity\"] = unigram[\"diversity\"]\n",
    "    doc_features[\"bigram_diversity\"] = bigram[\"diversity\"]\n",
    "    doc_features[\"trigram_diversity\"] = trigram[\"diversity\"]\n",
    "    doc_features[\"bigram_entropy\"] = bigram[\"entropy\"]\n",
    "    doc_features[\"trigram_entropy\"] = trigram[\"entropy\"]\n",
    "    doc_features[\"token_burstiness\"] = calculate_burstiness(tok_win) if len(tok_win) >= 2 else np.nan\n",
    "\n",
    "    char_diversity, char_entropy = character_ngram_features(text, n=3)\n",
    "    doc_features[\"char_trigram_diversity\"] = char_diversity\n",
    "    doc_features[\"char_trigram_entropy\"] = char_entropy\n",
    "    doc_features.update(character_statistics(text))\n",
    "    doc_features.update(compression_features(text))\n",
    "\n",
    "    # CRITICAL: Use clean parse for dependency features to avoid resegmentation artifacts\n",
    "    doc_clean, _ = prepare_doc(text, nlp, skip_reseg=True)\n",
    "    dep_feats = dependency_tree_features(doc_clean)\n",
    "    doc_features.update(dep_feats)\n",
    "    max_tree_depth = dep_feats.get(\"max_tree_depth\")\n",
    "    try:\n",
    "        depth_nan = math.isnan(max_tree_depth)\n",
    "    except (TypeError, ValueError):\n",
    "        depth_nan = False\n",
    "    depth_ok = depth_nan or max_tree_depth is None\n",
    "    if not depth_ok:\n",
    "        try:\n",
    "            depth_ok = max_tree_depth <= 50\n",
    "        except TypeError:\n",
    "            depth_ok = False\n",
    "    doc_features[\"depth_check_passed\"] = bool(depth_ok)\n",
    "\n",
    "    vocab_feats = vocabulary_sophistication_features(tok_win)\n",
    "    doc_features.update(vocab_feats)\n",
    "\n",
    "    doc_features.update(punctuation_patterns(doc))\n",
    "    doc_features.update(sentiment_features(text, doc))\n",
    "\n",
    "    for flag in [\"had_urls\", \"had_html\", \"had_code\", \"had_table\"]:\n",
    "        if flag in row.index:\n",
    "            doc_features[flag] = row[flag]\n",
    "\n",
    "    feature_rows.append(doc_features)\n",
    "\n",
    "feat_df = pd.DataFrame(feature_rows)\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Computed feature columns:\")\n",
    "print(list(feat_df.columns))\n",
    "df_with_features.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c5cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched dataset: raid_sample_small_with_features_PREPOS.csv  (rows: 3000)\n",
      "Feature columns saved (48 total):\n",
      "['type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'whitespace_ratio', 'unique_char_count', 'compression_ratio', 'bits_per_char', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'hapax_type_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.586055</td>\n",
       "      <td>0.146975</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.445179</td>\n",
       "      <td>0.577254</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopword_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.444772</td>\n",
       "      <td>0.162443</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.259209</td>\n",
       "      <td>0.475461</td>\n",
       "      <td>0.607976</td>\n",
       "      <td>0.859223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.030478</td>\n",
       "      <td>0.013802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>0.028535</td>\n",
       "      <td>0.045698</td>\n",
       "      <td>0.240035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>22.399651</td>\n",
       "      <td>20.384846</td>\n",
       "      <td>2.734694</td>\n",
       "      <td>11.909091</td>\n",
       "      <td>19.387500</td>\n",
       "      <td>30.150794</td>\n",
       "      <td>372.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_length_std</th>\n",
       "      <td>2963.0</td>\n",
       "      <td>10.138241</td>\n",
       "      <td>10.968209</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>4.224925</td>\n",
       "      <td>7.943588</td>\n",
       "      <td>16.077229</td>\n",
       "      <td>232.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_tokens_doc</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>249.418667</td>\n",
       "      <td>171.584008</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>390.100000</td>\n",
       "      <td>2313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_sentences_doc</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>13.466000</td>\n",
       "      <td>9.565180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>2766.0</td>\n",
       "      <td>55.251627</td>\n",
       "      <td>24.091923</td>\n",
       "      <td>-107.644167</td>\n",
       "      <td>22.134746</td>\n",
       "      <td>58.848714</td>\n",
       "      <td>81.844976</td>\n",
       "      <td>191.616616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>2766.0</td>\n",
       "      <td>13.548053</td>\n",
       "      <td>5.425465</td>\n",
       "      <td>2.215209</td>\n",
       "      <td>7.574388</td>\n",
       "      <td>13.079720</td>\n",
       "      <td>20.073260</td>\n",
       "      <td>59.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>2750.0</td>\n",
       "      <td>12.092650</td>\n",
       "      <td>3.871336</td>\n",
       "      <td>3.879220</td>\n",
       "      <td>7.554174</td>\n",
       "      <td>11.855464</td>\n",
       "      <td>16.975883</td>\n",
       "      <td>41.593039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>2766.0</td>\n",
       "      <td>11.711924</td>\n",
       "      <td>5.873829</td>\n",
       "      <td>-5.663764</td>\n",
       "      <td>5.755777</td>\n",
       "      <td>10.966026</td>\n",
       "      <td>17.820554</td>\n",
       "      <td>65.888927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_diversity</th>\n",
       "      <td>2807.0</td>\n",
       "      <td>0.575693</td>\n",
       "      <td>0.137383</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.449707</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.719558</td>\n",
       "      <td>0.996564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_diversity</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>0.891706</td>\n",
       "      <td>0.141051</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.814574</td>\n",
       "      <td>0.923166</td>\n",
       "      <td>0.981982</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_diversity</th>\n",
       "      <td>2796.0</td>\n",
       "      <td>0.949905</td>\n",
       "      <td>0.137224</td>\n",
       "      <td>0.012397</td>\n",
       "      <td>0.923224</td>\n",
       "      <td>0.984399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_entropy</th>\n",
       "      <td>2800.0</td>\n",
       "      <td>7.509389</td>\n",
       "      <td>0.814372</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>6.730009</td>\n",
       "      <td>7.584336</td>\n",
       "      <td>8.416338</td>\n",
       "      <td>8.895742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_entropy</th>\n",
       "      <td>2796.0</td>\n",
       "      <td>7.646812</td>\n",
       "      <td>0.821226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.841016</td>\n",
       "      <td>7.734932</td>\n",
       "      <td>8.552790</td>\n",
       "      <td>8.960002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_burstiness</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>-0.010279</td>\n",
       "      <td>0.209750</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.236061</td>\n",
       "      <td>0.025374</td>\n",
       "      <td>0.200373</td>\n",
       "      <td>0.630398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_diversity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.524394</td>\n",
       "      <td>0.126233</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>0.391776</td>\n",
       "      <td>0.523580</td>\n",
       "      <td>0.665903</td>\n",
       "      <td>0.987013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_entropy</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>8.873556</td>\n",
       "      <td>0.642104</td>\n",
       "      <td>3.807348</td>\n",
       "      <td>8.298459</td>\n",
       "      <td>8.953668</td>\n",
       "      <td>9.506143</td>\n",
       "      <td>10.393655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>0.018471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010917</td>\n",
       "      <td>0.024326</td>\n",
       "      <td>0.046569</td>\n",
       "      <td>0.409091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.169834</td>\n",
       "      <td>0.017801</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>0.145160</td>\n",
       "      <td>0.170694</td>\n",
       "      <td>0.190544</td>\n",
       "      <td>0.303688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_char_count</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>46.745667</td>\n",
       "      <td>10.562678</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>221.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.507016</td>\n",
       "      <td>0.089222</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.436628</td>\n",
       "      <td>0.512629</td>\n",
       "      <td>0.585877</td>\n",
       "      <td>1.050633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bits_per_char</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>4.056130</td>\n",
       "      <td>0.713775</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>3.493021</td>\n",
       "      <td>4.101032</td>\n",
       "      <td>4.687013</td>\n",
       "      <td>8.405063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_tree_depth</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>5.849982</td>\n",
       "      <td>4.145966</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>3.730769</td>\n",
       "      <td>5.511905</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tree_depth</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>9.737333</td>\n",
       "      <td>5.233113</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3.088215</td>\n",
       "      <td>1.284013</td>\n",
       "      <td>1.603251</td>\n",
       "      <td>2.555134</td>\n",
       "      <td>2.998605</td>\n",
       "      <td>3.542737</td>\n",
       "      <td>45.902077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left_dependency_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.477628</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.064865</td>\n",
       "      <td>0.403670</td>\n",
       "      <td>0.472777</td>\n",
       "      <td>0.556085</td>\n",
       "      <td>0.957447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_dependency_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.522372</td>\n",
       "      <td>0.064286</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.443915</td>\n",
       "      <td>0.527223</td>\n",
       "      <td>0.596330</td>\n",
       "      <td>0.935135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_legomena_ratio</th>\n",
       "      <td>2931.0</td>\n",
       "      <td>0.463964</td>\n",
       "      <td>0.193388</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.278027</td>\n",
       "      <td>0.436937</td>\n",
       "      <td>0.698113</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_type_ratio</th>\n",
       "      <td>2931.0</td>\n",
       "      <td>0.727022</td>\n",
       "      <td>0.143646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596273</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yules_k</th>\n",
       "      <td>2561.0</td>\n",
       "      <td>160.506394</td>\n",
       "      <td>266.218712</td>\n",
       "      <td>0.239461</td>\n",
       "      <td>66.249159</td>\n",
       "      <td>120.931743</td>\n",
       "      <td>216.844506</td>\n",
       "      <td>4938.271605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtld</th>\n",
       "      <td>1509.0</td>\n",
       "      <td>173.682477</td>\n",
       "      <td>746.222151</td>\n",
       "      <td>34.148230</td>\n",
       "      <td>54.755718</td>\n",
       "      <td>83.714702</td>\n",
       "      <td>155.360275</td>\n",
       "      <td>23385.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comma_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.046687</td>\n",
       "      <td>0.029148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013733</td>\n",
       "      <td>0.043659</td>\n",
       "      <td>0.078073</td>\n",
       "      <td>0.249231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.042821</td>\n",
       "      <td>0.020632</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022654</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.063708</td>\n",
       "      <td>0.411765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.079137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.006839</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003268</td>\n",
       "      <td>0.279070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.091787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006624</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.007317</td>\n",
       "      <td>0.015667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024540</td>\n",
       "      <td>0.314183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.091950</td>\n",
       "      <td>0.127477</td>\n",
       "      <td>-0.750000</td>\n",
       "      <td>-0.032644</td>\n",
       "      <td>0.084737</td>\n",
       "      <td>0.245847</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.426827</td>\n",
       "      <td>0.172328</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.455188</td>\n",
       "      <td>0.604411</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.041301</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.031508</td>\n",
       "      <td>0.092313</td>\n",
       "      <td>0.290421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.551330</td>\n",
       "      <td>0.244453</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.034911</td>\n",
       "      <td>0.025074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.031974</td>\n",
       "      <td>0.065226</td>\n",
       "      <td>0.304348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.016675</td>\n",
       "      <td>0.015986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.035465</td>\n",
       "      <td>0.277778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              count        mean         std         min  \\\n",
       "type_token_ratio             3000.0    0.586055    0.146975    0.012295   \n",
       "stopword_ratio               3000.0    0.444772    0.162443    0.000000   \n",
       "punctuation_ratio            3000.0    0.030478    0.013802    0.000000   \n",
       "avg_sentence_length          3000.0   22.399651   20.384846    2.734694   \n",
       "sentence_length_std          2963.0   10.138241   10.968209    0.433013   \n",
       "n_tokens_doc                 3000.0  249.418667  171.584008    9.000000   \n",
       "n_sentences_doc              3000.0   13.466000    9.565180    1.000000   \n",
       "flesch_reading_ease          2766.0   55.251627   24.091923 -107.644167   \n",
       "gunning_fog                  2766.0   13.548053    5.425465    2.215209   \n",
       "smog_index                   2750.0   12.092650    3.871336    3.879220   \n",
       "automated_readability_index  2766.0   11.711924    5.873829   -5.663764   \n",
       "unigram_diversity            2807.0    0.575693    0.137383    0.012295   \n",
       "bigram_diversity             2800.0    0.891706    0.141051    0.012346   \n",
       "trigram_diversity            2796.0    0.949905    0.137224    0.012397   \n",
       "bigram_entropy               2800.0    7.509389    0.814372    0.999935   \n",
       "trigram_entropy              2796.0    7.646812    0.821226    1.000000   \n",
       "token_burstiness             3000.0   -0.010279    0.209750   -1.000000   \n",
       "char_trigram_diversity       3000.0    0.524394    0.126233    0.012357   \n",
       "char_trigram_entropy         3000.0    8.873556    0.642104    3.807348   \n",
       "uppercase_ratio              3000.0    0.027401    0.018471    0.000000   \n",
       "whitespace_ratio             3000.0    0.169834    0.017801    0.062937   \n",
       "unique_char_count            3000.0   46.745667   10.562678   10.000000   \n",
       "compression_ratio            3000.0    0.507016    0.089222    0.027397   \n",
       "bits_per_char                3000.0    4.056130    0.713775    0.219178   \n",
       "avg_tree_depth               3000.0    5.849982    4.145966    1.600000   \n",
       "max_tree_depth               3000.0    9.737333    5.233113    2.000000   \n",
       "avg_dependency_distance      3000.0    3.088215    1.284013    1.603251   \n",
       "left_dependency_ratio        3000.0    0.477628    0.064286    0.064865   \n",
       "right_dependency_ratio       3000.0    0.522372    0.064286    0.042553   \n",
       "hapax_legomena_ratio         2931.0    0.463964    0.193388    0.000000   \n",
       "hapax_type_ratio             2931.0    0.727022    0.143646    0.000000   \n",
       "yules_k                      2561.0  160.506394  266.218712    0.239461   \n",
       "mtld                         1509.0  173.682477  746.222151   34.148230   \n",
       "comma_ratio                  3000.0    0.046687    0.029148    0.000000   \n",
       "period_ratio                 3000.0    0.042821    0.020632    0.000000   \n",
       "question_ratio               3000.0    0.001383    0.004406    0.000000   \n",
       "exclamation_ratio            3000.0    0.001249    0.006839    0.000000   \n",
       "semicolon_ratio              3000.0    0.001395    0.004736    0.000000   \n",
       "colon_ratio                  3000.0    0.001885    0.004433    0.000000   \n",
       "quote_ratio                  3000.0    0.007317    0.015667    0.000000   \n",
       "sentiment_polarity           3000.0    0.091950    0.127477   -0.750000   \n",
       "sentiment_subjectivity       3000.0    0.426827    0.172328    0.000000   \n",
       "sentiment_polarity_variance  3000.0    0.041301    0.039681    0.000000   \n",
       "neutral_sentence_ratio       3000.0    0.551330    0.244453    0.000000   \n",
       "positive_word_ratio          3000.0    0.034911    0.025074    0.000000   \n",
       "negative_word_ratio          3000.0    0.016675    0.015986    0.000000   \n",
       "\n",
       "                                    10%         50%         90%           max  \n",
       "type_token_ratio               0.445179    0.577254    0.750000      1.000000  \n",
       "stopword_ratio                 0.259209    0.475461    0.607976      0.859223  \n",
       "punctuation_ratio              0.016940    0.028535    0.045698      0.240035  \n",
       "avg_sentence_length           11.909091   19.387500   30.150794    372.000000  \n",
       "sentence_length_std            4.224925    7.943588   16.077229    232.000000  \n",
       "n_tokens_doc                 112.000000  223.000000  390.100000   2313.000000  \n",
       "n_sentences_doc                5.000000   12.000000   24.000000    126.000000  \n",
       "flesch_reading_ease           22.134746   58.848714   81.844976    191.616616  \n",
       "gunning_fog                    7.574388   13.079720   20.073260     59.666667  \n",
       "smog_index                     7.554174   11.855464   16.975883     41.593039  \n",
       "automated_readability_index    5.755777   10.966026   17.820554     65.888927  \n",
       "unigram_diversity              0.449707    0.571429    0.719558      0.996564  \n",
       "bigram_diversity               0.814574    0.923166    0.981982      1.000000  \n",
       "trigram_diversity              0.923224    0.984399    1.000000      1.000000  \n",
       "bigram_entropy                 6.730009    7.584336    8.416338      8.895742  \n",
       "trigram_entropy                6.841016    7.734932    8.552790      8.960002  \n",
       "token_burstiness              -0.236061    0.025374    0.200373      0.630398  \n",
       "char_trigram_diversity         0.391776    0.523580    0.665903      0.987013  \n",
       "char_trigram_entropy           8.298459    8.953668    9.506143     10.393655  \n",
       "uppercase_ratio                0.010917    0.024326    0.046569      0.409091  \n",
       "whitespace_ratio               0.145160    0.170694    0.190544      0.303688  \n",
       "unique_char_count             34.000000   46.000000   60.000000    221.000000  \n",
       "compression_ratio              0.436628    0.512629    0.585877      1.050633  \n",
       "bits_per_char                  3.493021    4.101032    4.687013      8.405063  \n",
       "avg_tree_depth                 3.730769    5.511905    7.666667    140.000000  \n",
       "max_tree_depth                 6.000000    9.000000   13.000000    143.000000  \n",
       "avg_dependency_distance        2.555134    2.998605    3.542737     45.902077  \n",
       "left_dependency_ratio          0.403670    0.472777    0.556085      0.957447  \n",
       "right_dependency_ratio         0.443915    0.527223    0.596330      0.935135  \n",
       "hapax_legomena_ratio           0.278027    0.436937    0.698113      1.000000  \n",
       "hapax_type_ratio               0.596273    0.736842    0.871795      1.000000  \n",
       "yules_k                       66.249159  120.931743  216.844506   4938.271605  \n",
       "mtld                          54.755718   83.714702  155.360275  23385.880000  \n",
       "comma_ratio                    0.013733    0.043659    0.078073      0.249231  \n",
       "period_ratio                   0.022654    0.041667    0.063708      0.411765  \n",
       "question_ratio                 0.000000    0.000000    0.004836      0.079137  \n",
       "exclamation_ratio              0.000000    0.000000    0.003268      0.279070  \n",
       "semicolon_ratio                0.000000    0.000000    0.004310      0.091787  \n",
       "colon_ratio                    0.000000    0.000000    0.006624      0.062500  \n",
       "quote_ratio                    0.000000    0.000000    0.024540      0.314183  \n",
       "sentiment_polarity            -0.032644    0.084737    0.245847      1.000000  \n",
       "sentiment_subjectivity         0.200000    0.455188    0.604411      1.000000  \n",
       "sentiment_polarity_variance    0.000621    0.031508    0.092313      0.290421  \n",
       "neutral_sentence_ratio         0.250000    0.538462    1.000000      1.000000  \n",
       "positive_word_ratio            0.002779    0.031974    0.065226      0.304348  \n",
       "negative_word_ratio            0.000000    0.014388    0.035465      0.277778  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Save enriched dataset and basic descriptive statistics\n",
    "\n",
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_PREPOS.csv\"\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS}  (rows: {len(df_with_features)})\")\n",
    "\n",
    "feature_cols = [col for col in df_with_features.columns if col not in df.columns]\n",
    "print(f\"Feature columns saved ({len(feature_cols)} total):\")\n",
    "print(feature_cols)\n",
    "\n",
    "display(df_with_features[feature_cols].describe(percentiles=[0.1, 0.5, 0.9]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125c533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b190a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length min: 2.7346938775510203 max: 372.0\n",
      "flesch_reading_ease min: -107.64416666666664 max: 191.61661585365854\n",
      "gunning_fog min: 2.2152089518154834 max: 59.66666666666668\n",
      "automated_readability_index min: -5.663763961280715 max: 65.88892708333333\n",
      "mtld min: 34.14823008849558 max: 23385.879999999644\n",
      "yules_k min: 0.2394607344260725 max: 4938.271604938272\n",
      "max_tree_depth min: 2.0 max: 143.0\n"
     ]
    }
   ],
   "source": [
    "for col in [\"avg_sentence_length\",\"flesch_reading_ease\",\"gunning_fog\",\n",
    "            \"automated_readability_index\",\"mtld\",\"yules_k\",\"max_tree_depth\"]:\n",
    "    if col in df.columns:\n",
    "        print(col, \"min:\", df[col].min(), \"max:\", df[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b72a3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed thresholds:\n",
      "  asl_hi: 98.00999999999976\n",
      "  sls_hi: 55.89946594343152\n",
      "  fog_hi: 35.01621768175455\n",
      "  ari_hi: 40.022487090314115\n",
      "  fre_lo: -24.323863976377957\n",
      "  mtld_hi: 3757.0124307692317\n",
      "  yk_hi: 1997.6630420500649\n",
      "  depth_max_hi: 26.00500000000011\n",
      "  depth_avg_hi: 16.666666666666668\n",
      "  depdist_hi: 5.655639369479048\n",
      "  comp_hi: 1.0\n",
      "  upper_hi: 0.1044532589072649\n",
      "  uniq_hi: 75.0\n",
      "  ws_lo: 0.12376187270761338\n",
      "  ws_hi: 0.21583238511231842\n",
      "\n",
      "Cause counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>markup_or_code_noise</th>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependency_depth_computation_bug</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_metric_instability_or_length_effects</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_segmentation_failure</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              count\n",
       "cause                                              \n",
       "markup_or_code_noise                             55\n",
       "dependency_depth_computation_bug                 32\n",
       "lexical_metric_instability_or_length_effects     21\n",
       "sentence_segmentation_failure                    11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flag counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_noise</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_instability</th>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_outlier</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth_implausible</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_overhead</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count\n",
       "severity                    181\n",
       "markup_noise                 54\n",
       "seg_len_extreme              52\n",
       "lexical_instability          21\n",
       "readability_outlier          20\n",
       "depth_implausible            18\n",
       "dep_distance_implausible     15\n",
       "compression_overhead          1\n",
       "ngram_edge_effects            0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top offenders (with original feature values):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <th>readability_outlier</th>\n",
       "      <th>lexical_instability</th>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <th>depth_implausible</th>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <th>compression_overhead</th>\n",
       "      <th>markup_noise</th>\n",
       "      <th>suspected_causes</th>\n",
       "      <th>severity</th>\n",
       "      <th>...</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>unique_char_count</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>bigram_entropy</th>\n",
       "      <th>trigram_entropy</th>\n",
       "      <th>bigram_diversity</th>\n",
       "      <th>trigram_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>44.0</td>\n",
       "      <td>6.084670</td>\n",
       "      <td>0.523683</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.120500</td>\n",
       "      <td>8.317413</td>\n",
       "      <td>8.312883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.697481</td>\n",
       "      <td>0.185150</td>\n",
       "      <td>0.119831</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.172932</td>\n",
       "      <td>4.417189</td>\n",
       "      <td>5.048976</td>\n",
       "      <td>0.332226</td>\n",
       "      <td>0.396667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>42.885475</td>\n",
       "      <td>0.082751</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.062937</td>\n",
       "      <td>2.011303</td>\n",
       "      <td>2.015439</td>\n",
       "      <td>0.137056</td>\n",
       "      <td>0.137755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.189112</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.234454</td>\n",
       "      <td>2.054177</td>\n",
       "      <td>2.054363</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.021583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.194286</td>\n",
       "      <td>0.045553</td>\n",
       "      <td>0.077007</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.303688</td>\n",
       "      <td>2.142810</td>\n",
       "      <td>2.207105</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.025090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.283136</td>\n",
       "      <td>0.179336</td>\n",
       "      <td>0.023019</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.123662</td>\n",
       "      <td>4.256583</td>\n",
       "      <td>4.314395</td>\n",
       "      <td>0.467532</td>\n",
       "      <td>0.491304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>81.0</td>\n",
       "      <td>2.981481</td>\n",
       "      <td>0.028194</td>\n",
       "      <td>0.143612</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.214097</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>1.584938</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.012397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>75.0</td>\n",
       "      <td>3.995318</td>\n",
       "      <td>0.194007</td>\n",
       "      <td>0.143071</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0.155056</td>\n",
       "      <td>3.046500</td>\n",
       "      <td>3.167528</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.284314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[sentence_segmentation_failure, markup_or_code...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.570365</td>\n",
       "      <td>0.530976</td>\n",
       "      <td>0.026593</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.122735</td>\n",
       "      <td>8.675957</td>\n",
       "      <td>8.672425</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.046296</td>\n",
       "      <td>0.160458</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.164279</td>\n",
       "      <td>4.292624</td>\n",
       "      <td>5.470337</td>\n",
       "      <td>0.294574</td>\n",
       "      <td>0.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.305784</td>\n",
       "      <td>0.530415</td>\n",
       "      <td>0.009273</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.130193</td>\n",
       "      <td>8.449629</td>\n",
       "      <td>8.451211</td>\n",
       "      <td>0.997151</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.672926</td>\n",
       "      <td>0.547706</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.149939</td>\n",
       "      <td>8.531381</td>\n",
       "      <td>8.527477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3.457002</td>\n",
       "      <td>0.540937</td>\n",
       "      <td>0.006078</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.136932</td>\n",
       "      <td>8.588715</td>\n",
       "      <td>8.584963</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1741</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, markup_or_c...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>14.397787</td>\n",
       "      <td>0.196033</td>\n",
       "      <td>0.127188</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.169195</td>\n",
       "      <td>3.468883</td>\n",
       "      <td>3.515598</td>\n",
       "      <td>0.138408</td>\n",
       "      <td>0.152778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1894</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, markup_or_c...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>53.0</td>\n",
       "      <td>3.023077</td>\n",
       "      <td>0.057787</td>\n",
       "      <td>0.131078</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.206483</td>\n",
       "      <td>2.321879</td>\n",
       "      <td>2.321855</td>\n",
       "      <td>0.020492</td>\n",
       "      <td>0.020576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.364924</td>\n",
       "      <td>0.544224</td>\n",
       "      <td>0.007591</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.141914</td>\n",
       "      <td>8.714246</td>\n",
       "      <td>8.710806</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2201</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>3.099451</td>\n",
       "      <td>0.238066</td>\n",
       "      <td>0.082175</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.132326</td>\n",
       "      <td>4.978844</td>\n",
       "      <td>5.762975</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.511416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.766424</td>\n",
       "      <td>0.523250</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.136214</td>\n",
       "      <td>8.179909</td>\n",
       "      <td>8.174926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1.797143</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.038904</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>2.343533</td>\n",
       "      <td>2.343571</td>\n",
       "      <td>0.017143</td>\n",
       "      <td>0.017192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure]</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>4.880177</td>\n",
       "      <td>0.532478</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.138294</td>\n",
       "      <td>8.475733</td>\n",
       "      <td>8.471675</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      seg_len_extreme  readability_outlier  lexical_instability  \\\n",
       "1662             True                 True                False   \n",
       "337              True                False                 True   \n",
       "251              True                False                 True   \n",
       "1109             True                False                 True   \n",
       "220              True                False                 True   \n",
       "1689             True                 True                 True   \n",
       "2880             True                False                 True   \n",
       "2685             True                False                 True   \n",
       "109              True                 True                False   \n",
       "929              True                False                 True   \n",
       "1199             True                 True                 True   \n",
       "292              True                 True                False   \n",
       "1730             True                 True                 True   \n",
       "1741             True                False                False   \n",
       "1894             True                False                False   \n",
       "2018             True                 True                 True   \n",
       "2201             True                 True                False   \n",
       "2347             True                 True                 True   \n",
       "2786             True                False                 True   \n",
       "1248             True                 True                False   \n",
       "\n",
       "      ngram_edge_effects  depth_implausible  dep_distance_implausible  \\\n",
       "1662               False               True                      True   \n",
       "337                False               True                     False   \n",
       "251                False              False                      True   \n",
       "1109               False               True                     False   \n",
       "220                False               True                     False   \n",
       "1689               False              False                     False   \n",
       "2880               False               True                     False   \n",
       "2685               False               True                     False   \n",
       "109                False              False                     False   \n",
       "929                False              False                      True   \n",
       "1199               False              False                     False   \n",
       "292                False              False                      True   \n",
       "1730               False              False                     False   \n",
       "1741               False              False                      True   \n",
       "1894               False               True                     False   \n",
       "2018               False              False                     False   \n",
       "2201               False               True                     False   \n",
       "2347               False              False                     False   \n",
       "2786               False               True                     False   \n",
       "1248               False              False                     False   \n",
       "\n",
       "      compression_overhead  markup_noise  \\\n",
       "1662                 False          True   \n",
       "337                  False          True   \n",
       "251                  False          True   \n",
       "1109                 False          True   \n",
       "220                  False          True   \n",
       "1689                 False          True   \n",
       "2880                 False          True   \n",
       "2685                 False          True   \n",
       "109                  False          True   \n",
       "929                  False         False   \n",
       "1199                 False         False   \n",
       "292                  False         False   \n",
       "1730                 False         False   \n",
       "1741                 False          True   \n",
       "1894                 False          True   \n",
       "2018                 False         False   \n",
       "2201                 False         False   \n",
       "2347                 False         False   \n",
       "2786                 False         False   \n",
       "1248                 False         False   \n",
       "\n",
       "                                       suspected_causes  severity  ...  \\\n",
       "1662  [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "337   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "251   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "1109  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "220   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "1689  [sentence_segmentation_failure, lexical_metric...         4  ...   \n",
       "2880  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "2685  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "109   [sentence_segmentation_failure, markup_or_code...         3  ...   \n",
       "929   [dependency_depth_computation_bug, lexical_met...         3  ...   \n",
       "1199  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "292   [dependency_depth_computation_bug, sentence_se...         3  ...   \n",
       "1730  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "1741  [dependency_depth_computation_bug, markup_or_c...         3  ...   \n",
       "1894  [dependency_depth_computation_bug, markup_or_c...         3  ...   \n",
       "2018  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "2201  [dependency_depth_computation_bug, sentence_se...         3  ...   \n",
       "2347  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "2786  [dependency_depth_computation_bug, lexical_met...         3  ...   \n",
       "1248                    [sentence_segmentation_failure]         2  ...   \n",
       "\n",
       "      max_tree_depth  avg_dependency_distance  compression_ratio  \\\n",
       "1662            44.0                 6.084670           0.523683   \n",
       "337             30.0                 2.697481           0.185150   \n",
       "251              8.0                42.885475           0.082751   \n",
       "1109            71.0                 2.189112           0.035294   \n",
       "220             71.0                 2.194286           0.045553   \n",
       "1689            22.0                 3.283136           0.179336   \n",
       "2880            81.0                 2.981481           0.028194   \n",
       "2685            75.0                 3.995318           0.194007   \n",
       "109             13.0                 2.570365           0.530976   \n",
       "929              7.0                17.046296           0.160458   \n",
       "1199            18.0                 4.305784           0.530415   \n",
       "292             12.0                 7.672926           0.547706   \n",
       "1730            12.0                 3.457002           0.540937   \n",
       "1741            11.0                14.397787           0.196033   \n",
       "1894            53.0                 3.023077           0.057787   \n",
       "2018            12.0                 4.364924           0.544224   \n",
       "2201            42.0                 3.099451           0.238066   \n",
       "2347            22.0                 3.766424           0.523250   \n",
       "2786           140.0                 1.797143           0.027397   \n",
       "1248            17.0                 4.880177           0.532478   \n",
       "\n",
       "      uppercase_ratio  unique_char_count  whitespace_ratio  bigram_entropy  \\\n",
       "1662         0.004926               44.0          0.120500        8.317413   \n",
       "337          0.119831               47.0          0.172932        4.417189   \n",
       "251          0.001748               29.0          0.062937        2.011303   \n",
       "1109         0.058824               12.0          0.234454        2.054177   \n",
       "220          0.077007               14.0          0.303688        2.142810   \n",
       "1689         0.023019               35.0          0.123662        4.256583   \n",
       "2880         0.143612               10.0          0.214097        1.584963   \n",
       "2685         0.143071               49.0          0.155056        3.046500   \n",
       "109          0.026593               67.0          0.122735        8.675957   \n",
       "929          0.000955               31.0          0.164279        4.292624   \n",
       "1199         0.009273               43.0          0.130193        8.449629   \n",
       "292          0.007314               44.0          0.149939        8.531381   \n",
       "1730         0.006078               47.0          0.136932        8.588715   \n",
       "1741         0.127188               44.0          0.169195        3.468883   \n",
       "1894         0.131078               20.0          0.206483        2.321879   \n",
       "2018         0.007591               62.0          0.141914        8.714246   \n",
       "2201         0.082175               40.0          0.132326        4.978844   \n",
       "2347         0.000939               34.0          0.136214        8.179909   \n",
       "2786         0.038904               14.0          0.191781        2.343533   \n",
       "1248         0.006623               44.0          0.138294        8.475733   \n",
       "\n",
       "      trigram_entropy  bigram_diversity  trigram_diversity  \n",
       "1662         8.312883          1.000000           1.000000  \n",
       "337          5.048976          0.332226           0.396667  \n",
       "251          2.015439          0.137056           0.137755  \n",
       "1109         2.054363          0.021505           0.021583  \n",
       "220          2.207105          0.021429           0.025090  \n",
       "1689         4.314395          0.467532           0.491304  \n",
       "2880         1.584938          0.012346           0.012397  \n",
       "2685         3.167528          0.268293           0.284314  \n",
       "109          8.672425          1.000000           1.000000  \n",
       "929          5.470337          0.294574           0.421875  \n",
       "1199         8.451211          0.997151           1.000000  \n",
       "292          8.527477          1.000000           1.000000  \n",
       "1730         8.584963          1.000000           1.000000  \n",
       "1741         3.515598          0.138408           0.152778  \n",
       "1894         2.321855          0.020492           0.020576  \n",
       "2018         8.710806          1.000000           1.000000  \n",
       "2201         5.762975          0.400000           0.511416  \n",
       "2347         8.174926          1.000000           1.000000  \n",
       "2786         2.343571          0.017143           0.017192  \n",
       "1248         8.471675          1.000000           1.000000  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"bigram_entropy\",\"trigram_entropy\",\"bigram_diversity\",\"trigram_diversity\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\"whitespace_ratio\"\n",
    "]\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols=NUMERIC_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Coerce known feature columns to numeric (if present).\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _q(df, col, q):\n",
    "    return float(np.nanquantile(df[col].values, q)) if col in df else np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# Thresholds and Diagnostics\n",
    "# -------------------------------\n",
    "def compute_diagnostic_thresholds(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Data-driven thresholds using quantiles only.\"\"\"\n",
    "    thr = {}\n",
    "    thr[\"asl_hi\"]       = _q(df, \"avg_sentence_length\", 0.99)\n",
    "    thr[\"sls_hi\"]       = _q(df, \"sentence_length_std\", 0.99)\n",
    "    thr[\"fog_hi\"]       = _q(df, \"gunning_fog\", 0.995)\n",
    "    thr[\"ari_hi\"]       = _q(df, \"automated_readability_index\", 0.995)\n",
    "    thr[\"fre_lo\"]       = _q(df, \"flesch_reading_ease\", 0.005)\n",
    "    thr[\"mtld_hi\"]      = _q(df, \"mtld\", 0.995)\n",
    "    thr[\"yk_hi\"]        = _q(df, \"yules_k\", 0.995)\n",
    "    thr[\"depth_max_hi\"] = _q(df, \"max_tree_depth\", 0.995)\n",
    "    thr[\"depth_avg_hi\"] = _q(df, \"avg_tree_depth\", 0.995)\n",
    "    thr[\"depdist_hi\"]   = _q(df, \"avg_dependency_distance\", 0.995)\n",
    "    thr[\"comp_hi\"]      = 1.0  # Compression > 1.0 = expansion\n",
    "    thr[\"upper_hi\"]     = _q(df, \"uppercase_ratio\", 0.995)\n",
    "    thr[\"uniq_hi\"]      = _q(df, \"unique_char_count\", 0.995)\n",
    "    thr[\"ws_lo\"]        = _q(df, \"whitespace_ratio\", 0.005)\n",
    "    thr[\"ws_hi\"]        = _q(df, \"whitespace_ratio\", 0.995)\n",
    "    return thr\n",
    "\n",
    "def diagnose_feature_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return diagnostics dataframe with boolean flags and suspected causes list.\"\"\"\n",
    "    # Ensure numeric so comparisons fire correctly\n",
    "    df = ensure_numeric(df)\n",
    "\n",
    "    thr = compute_diagnostic_thresholds(df)\n",
    "    D = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Flag definitions\n",
    "    D[\"seg_len_extreme\"] = (\n",
    "        (df.get(\"avg_sentence_length\", np.nan) > thr[\"asl_hi\"]) |\n",
    "        (df.get(\"sentence_length_std\", np.nan) > thr[\"sls_hi\"])\n",
    "    )\n",
    "    D[\"readability_outlier\"] = (\n",
    "        (df.get(\"flesch_reading_ease\", np.nan) < thr[\"fre_lo\"]) |\n",
    "        (df.get(\"gunning_fog\", np.nan) > thr[\"fog_hi\"]) |\n",
    "        (df.get(\"automated_readability_index\", np.nan) > thr[\"ari_hi\"])\n",
    "    )\n",
    "    D[\"lexical_instability\"] = (\n",
    "        (df.get(\"mtld\", np.nan) > thr[\"mtld_hi\"]) |\n",
    "        (df.get(\"yules_k\", np.nan) > thr[\"yk_hi\"])\n",
    "    )\n",
    "    D[\"ngram_edge_effects\"] = (\n",
    "    (df.get(\"bigram_entropy\", np.nan) == 0) |\n",
    "    (df.get(\"trigram_entropy\", np.nan) == 0) |\n",
    "    ((df.get(\"n_tokens_ws\", 1000) < 50) & (df.get(\"trigram_diversity\", 0) == 1.0)) |\n",
    "    ((df.get(\"n_tokens_ws\", 1000) < 50) & (df.get(\"bigram_diversity\", 0) == 1.0))\n",
    "    )\n",
    "    D[\"depth_implausible\"] = (\n",
    "        (df.get(\"max_tree_depth\", np.nan) > thr[\"depth_max_hi\"]) |\n",
    "        (df.get(\"avg_tree_depth\", np.nan) > thr[\"depth_avg_hi\"])\n",
    "    )\n",
    "    D[\"dep_distance_implausible\"] = (df.get(\"avg_dependency_distance\", np.nan) > thr[\"depdist_hi\"])\n",
    "    D[\"compression_overhead\"] = (df.get(\"compression_ratio\", np.nan) > thr[\"comp_hi\"])\n",
    "    D[\"markup_noise\"] = (\n",
    "    (df.get(\"uppercase_ratio\", np.nan) > thr[\"upper_hi\"]) |\n",
    "    (df.get(\"unique_char_count\", np.nan) > thr[\"uniq_hi\"]) |\n",
    "    (df.get(\"whitespace_ratio\", np.nan) < thr[\"ws_lo\"]) |\n",
    "    (df.get(\"whitespace_ratio\", np.nan) > thr[\"ws_hi\"])\n",
    "    )\n",
    "\n",
    "    # Suspected causes column (pre-allocate + .at)\n",
    "    D[\"suspected_causes\"] = pd.Series([[] for _ in range(len(D))], index=D.index, dtype=object)\n",
    "    for i in D.index:\n",
    "        c = []\n",
    "        if bool(D.at[i, \"depth_implausible\"]) or bool(D.at[i, \"dep_distance_implausible\"]):\n",
    "            c.append(\"dependency_depth_computation_bug\")\n",
    "        if bool(D.at[i, \"seg_len_extreme\"]) and bool(D.at[i, \"readability_outlier\"]):\n",
    "            c.append(\"sentence_segmentation_failure\")\n",
    "        if bool(D.at[i, \"lexical_instability\"]) or bool(D.at[i, \"ngram_edge_effects\"]):\n",
    "            c.append(\"lexical_metric_instability_or_length_effects\")\n",
    "        if bool(D.at[i, \"compression_overhead\"]) or bool(D.at[i, \"markup_noise\"]):\n",
    "            c.append(\"markup_or_code_noise\")\n",
    "        D.at[i, \"suspected_causes\"] = c\n",
    "\n",
    "    # Severity score\n",
    "    flag_cols = [c for c in D.columns if c != \"suspected_causes\"]\n",
    "    for c in flag_cols:\n",
    "        D[c] = D[c].fillna(False).astype(bool)\n",
    "    D[\"severity\"] = D[flag_cols].sum(axis=1).astype(int)\n",
    "\n",
    "    # Attach thresholds for later inspection\n",
    "    D.attrs[\"thresholds\"] = thr\n",
    "    return D\n",
    "\n",
    "def build_diagnostic_report(diag: pd.DataFrame, top_k: int = 15):\n",
    "    \"\"\"Summarize counts by cause and return top offending rows by severity.\"\"\"\n",
    "    cause_counts = (\n",
    "        diag[\"suspected_causes\"].explode().value_counts(dropna=True)\n",
    "        .rename_axis(\"cause\").to_frame(\"count\")\n",
    "    )\n",
    "    flag_counts = (\n",
    "        diag.drop(columns=[\"suspected_causes\"])\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .to_frame(\"count\")\n",
    "    )\n",
    "    top_offenders = diag.sort_values(\"severity\", ascending=False).head(top_k)\n",
    "    return {\"cause_counts\": cause_counts, \"flag_counts\": flag_counts, \"top_offenders\": top_offenders}\n",
    "\n",
    "# -------------------------------\n",
    "# Runner / Example usage\n",
    "# -------------------------------\n",
    "\n",
    "# Assume you already have `df` with your features\n",
    "# If your features were just computed and may be strings, the coercion inside\n",
    "# diagnose_feature_outliers will handle them; coercing here is optional:\n",
    "# df = ensure_numeric(df)\n",
    "\n",
    "diag = diagnose_feature_outliers(df)\n",
    "rep  = build_diagnostic_report(diag, top_k=20)\n",
    "\n",
    "print(\"Computed thresholds:\")\n",
    "for k, v in diag.attrs.get(\"thresholds\", {}).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nCause counts:\")\n",
    "display(rep[\"cause_counts\"])\n",
    "\n",
    "print(\"\\nFlag counts:\")\n",
    "display(rep[\"flag_counts\"])\n",
    "\n",
    "print(\"\\nTop offenders (with original feature values):\")\n",
    "cols_to_show = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\n",
    "    \"whitespace_ratio\",\"bigram_entropy\",\"trigram_entropy\",\n",
    "    \"bigram_diversity\",\"trigram_diversity\"\n",
    "]\n",
    "existing_cols = [c for c in cols_to_show if c in df.columns]\n",
    "display(rep[\"top_offenders\"].join(df[existing_cols], how=\"left\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81c0ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "FEATURES_TO_CAP = [\n",
    "    'avg_sentence_length','sentence_length_std','flesch_reading_ease',\n",
    "    'gunning_fog','automated_readability_index','mtld','yules_k',\n",
    "    'avg_dependency_distance','max_tree_depth','compression_ratio',\n",
    "    'bigram_entropy','trigram_entropy','bigram_diversity','trigram_diversity'\n",
    "]\n",
    "\n",
    "# Natural/practical bounds to prevent absurd caps\n",
    "BOUNDED = {\n",
    "    'bigram_diversity': (0.0, 1.0),\n",
    "    'trigram_diversity': (0.0, 1.0),\n",
    "    'compression_ratio': (0.0, np.inf),\n",
    "    'flesch_reading_ease': (-100.0, 150.0),   # practical working range\n",
    "}\n",
    "\n",
    "def _finite(s: pd.Series) -> pd.Series:\n",
    "    return s[np.isfinite(s.values)]\n",
    "\n",
    "def calculate_percentile_caps(df: pd.DataFrame, lower_pct=1, upper_pct=99) -> dict:\n",
    "    caps = {}\n",
    "    for feat in FEATURES_TO_CAP:\n",
    "        if feat not in df.columns: \n",
    "            continue\n",
    "        s = _finite(df[feat].dropna())\n",
    "        if s.empty:\n",
    "            continue\n",
    "        lo = float(np.percentile(s, lower_pct))\n",
    "        hi = float(np.percentile(s, upper_pct))\n",
    "        if feat in BOUNDED:\n",
    "            blo, bhi = BOUNDED[feat]\n",
    "            lo = max(lo, blo)\n",
    "            hi = min(hi, bhi)\n",
    "        if lo > hi:  # degenerate case\n",
    "            lo, hi = hi, lo\n",
    "        caps[feat] = (lo, hi)\n",
    "        print(f\"{feat}: [{lo:.2f}, {hi:.2f}]\")\n",
    "    return caps\n",
    "\n",
    "def cap_extreme_features(df: pd.DataFrame, caps: dict) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['features_capped'] = 0  # keep your counter if you like\n",
    "\n",
    "    for feat, (lo, hi) in caps.items():\n",
    "        if feat not in df.columns:\n",
    "            continue\n",
    "        col = df[feat]\n",
    "        before = col.copy()\n",
    "        # clip in-place while preserving NaNs\n",
    "        df[feat] = col.clip(lower=lo, upper=hi)\n",
    "        changed = (before != df[feat]) & df[feat].notna() & before.notna()\n",
    "        if changed.any():\n",
    "            print(f\"Capped {feat}: {int(changed.sum())} values\")\n",
    "            df['features_capped'] += changed.astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee2f54f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length: [7.87, 98.01]\n",
      "sentence_length_std: [1.32, 55.90]\n",
      "flesch_reading_ease: [-8.27, 98.34]\n",
      "gunning_fog: [4.68, 30.13]\n",
      "automated_readability_index: [1.97, 33.72]\n",
      "mtld: [41.68, 2508.16]\n",
      "yules_k: [2.79, 1288.42]\n",
      "avg_dependency_distance: [2.13, 4.79]\n",
      "max_tree_depth: [5.00, 20.00]\n",
      "compression_ratio: [0.10, 0.71]\n",
      "bigram_entropy: [4.19, 8.83]\n",
      "trigram_entropy: [4.50, 8.95]\n",
      "bigram_diversity: [0.13, 1.00]\n",
      "trigram_diversity: [0.14, 1.00]\n",
      "Capped avg_sentence_length: 60 values\n",
      "Capped sentence_length_std: 60 values\n",
      "Capped flesch_reading_ease: 56 values\n",
      "Capped gunning_fog: 56 values\n",
      "Capped automated_readability_index: 56 values\n",
      "Capped mtld: 32 values\n",
      "Capped yules_k: 52 values\n",
      "Capped avg_dependency_distance: 60 values\n",
      "Capped max_tree_depth: 49 values\n",
      "Capped compression_ratio: 60 values\n",
      "Capped bigram_entropy: 56 values\n",
      "Capped trigram_entropy: 51 values\n",
      "Capped bigram_diversity: 28 values\n",
      "Capped trigram_diversity: 28 values\n"
     ]
    }
   ],
   "source": [
    "actual_caps = calculate_percentile_caps(df_with_features, lower_pct=1, upper_pct=99)\n",
    "df_with_features = cap_extreme_features(df_with_features, actual_caps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c5e1826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3.0 standard deviations as threshold\n",
      "Parse issues: 47 texts\n",
      "Readability anomalies: 54 texts\n",
      "Lexical anomalies: 127 texts\n",
      "Quality score distribution:\n",
      "quality_score\n",
      "0       6\n",
      "1      43\n",
      "2     124\n",
      "3    2827\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "READABILITY_FEATS = ['flesch_reading_ease','gunning_fog','automated_readability_index']\n",
    "LEXICAL_FEATS     = ['mtld','yules_k','type_token_ratio']\n",
    "PARSE_FEATS       = ['avg_dependency_distance']  # plus tree-depth presence checks\n",
    "\n",
    "# Features where z-score should be computed on log1p to tame skew (values stay untouched)\n",
    "LOG_FOR_Z = {'mtld','yules_k','avg_dependency_distance','sentence_length_std','max_tree_depth'}\n",
    "\n",
    "def _z_on(series: pd.Series, log_if_needed: bool) -> pd.Series:\n",
    "    s = series.astype(float).replace([np.inf,-np.inf], np.nan)\n",
    "    if log_if_needed:\n",
    "        nonan = s.dropna()\n",
    "        if (nonan >= 0).all():\n",
    "            s = np.log1p(s)\n",
    "    # classical population z-score (ddof=0); fill NaNs with mean to avoid bias in stats.zscore\n",
    "    m = s.mean()\n",
    "    sd = s.std(ddof=0)\n",
    "    if sd == 0 or np.isnan(sd):\n",
    "        return pd.Series(0.0, index=s.index)\n",
    "    return (s - m) / sd\n",
    "\n",
    "def add_quality_flags_statistical(df: pd.DataFrame, n_std: float = 3.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Compute z-scores (abs) transiently; do not keep columns\n",
    "    z_map = {}\n",
    "\n",
    "    for feat in set(READABILITY_FEATS + LEXICAL_FEATS + PARSE_FEATS):\n",
    "        if feat in df.columns:\n",
    "            z = _z_on(df[feat], log_if_needed=(feat in LOG_FOR_Z)).abs()\n",
    "            z_map[feat] = z\n",
    "\n",
    "    df['parse_quality_issue'] = (\n",
    "        df['max_tree_depth'].isna() |\n",
    "        df['avg_tree_depth'].isna() |\n",
    "        (z_map.get('avg_dependency_distance', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['readability_anomaly'] = (\n",
    "        (z_map.get('flesch_reading_ease', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('gunning_fog', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('automated_readability_index', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['lexical_anomaly'] = (\n",
    "        (z_map.get('mtld', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('yules_k', pd.Series(0, index=df.index)) > n_std) |\n",
    "        (z_map.get('type_token_ratio', pd.Series(0, index=df.index)) > n_std)\n",
    "    )\n",
    "\n",
    "    df['quality_score'] = (3 - (\n",
    "        df['parse_quality_issue'].astype(int) +\n",
    "        df['readability_anomaly'].astype(int) +\n",
    "        df['lexical_anomaly'].astype(int)\n",
    "    )).clip(lower=0, upper=3)\n",
    "\n",
    "    print(f\"Using {n_std} standard deviations as threshold\")\n",
    "    print(f\"Parse issues: {int(df['parse_quality_issue'].sum())} texts\")\n",
    "    print(f\"Readability anomalies: {int(df['readability_anomaly'].sum())} texts\")\n",
    "    print(f\"Lexical anomalies: {int(df['lexical_anomaly'].sum())} texts\")\n",
    "    print(f\"Quality score distribution:\\n{df['quality_score'].value_counts().sort_index()}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "df_with_features = add_quality_flags_statistical(df_with_features, n_std=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a3c1bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Impute] Dropping duplicate columns: ['avg_word_length', 'digit_ratio', 'had_urls', 'had_html', 'had_code', 'had_table']\n",
      "\n",
      "[Impute] Dropping 2 features with >40% missing:\n",
      "  - not_text_reason: 100.0% missing\n",
      "  - mtld: 49.7% missing\n",
      "\n",
      "[Impute] Filled 2567 missing values across 13 features\n",
      "Top 10 imputed features:\n",
      "yules_k                        439\n",
      "smog_index                     250\n",
      "gunning_fog                    234\n",
      "automated_readability_index    234\n",
      "flesch_reading_ease            234\n",
      "trigram_entropy                204\n",
      "trigram_diversity              204\n",
      "bigram_entropy                 200\n",
      "bigram_diversity               200\n",
      "unigram_diversity              193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def impute_after_capping(df: pd.DataFrame, max_missing_pct: float = 0.4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing values with group medians, dropping features with excessive missingness.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    max_missing_pct : float\n",
    "        Maximum proportion of missing values allowed (default 0.4 = 40%)\n",
    "        Features exceeding this threshold are dropped before imputation\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop duplicate columns (keep first)\n",
    "    if df.columns.duplicated().any():\n",
    "        dup_mask = df.columns.duplicated()\n",
    "        print(f\"[Impute] Dropping duplicate columns: {df.columns[dup_mask].tolist()}\")\n",
    "        df = df.loc[:, ~dup_mask]\n",
    "\n",
    "    # Numeric features except target\n",
    "    num_feats = [c for c in df.select_dtypes(include=[np.number]).columns if c != \"is_ai\"]\n",
    "    if not num_feats:\n",
    "        return df\n",
    "\n",
    "    # Check missingness and drop high-missing features FIRST\n",
    "    missing_pct = df[num_feats].isna().mean()\n",
    "    high_missing = missing_pct[missing_pct > max_missing_pct].sort_values(ascending=False)\n",
    "    \n",
    "    if not high_missing.empty:\n",
    "        print(f\"\\n[Impute] Dropping {len(high_missing)} features with >{max_missing_pct*100:.0f}% missing:\")\n",
    "        for feat, pct in high_missing.items():\n",
    "            print(f\"  - {feat}: {pct*100:.1f}% missing\")\n",
    "        \n",
    "        # Remove high-missing features\n",
    "        num_feats = [f for f in num_feats if f not in high_missing.index]\n",
    "        df.drop(columns=high_missing.index, inplace=True)\n",
    "    \n",
    "    if not num_feats:\n",
    "        print(\"[Impute] No features remaining after missingness filter\")\n",
    "        return df\n",
    "\n",
    "    # Group keys for stratified imputation\n",
    "    group_keys = []\n",
    "    if \"source_type\" in df.columns:\n",
    "        group_keys.append(\"source_type\")\n",
    "    if \"n_tokens_doc\" in df.columns:\n",
    "        bins = [0, 100, 250, 500, 10_000]\n",
    "        labels = [\"S\", \"M\", \"L\", \"XL\"]\n",
    "        df[\"__len_bin__\"] = pd.cut(df[\"n_tokens_doc\"], bins=bins, right=False, labels=labels)\n",
    "        group_keys.append(\"__len_bin__\")\n",
    "\n",
    "    before_missing = df[num_feats].isna().sum()\n",
    "    values = df[num_feats].to_numpy(dtype=float)\n",
    "    mask = np.isnan(values)\n",
    "\n",
    "    # Group-wise median imputation\n",
    "    if group_keys:\n",
    "        med_df = df.groupby(group_keys, dropna=False, observed=False)[num_feats].transform(\"median\")\n",
    "        med_vals = med_df.to_numpy(dtype=float)\n",
    "        values = np.where(mask, med_vals, values)\n",
    "\n",
    "    # Global per-column median fallback\n",
    "    still_nan = np.isnan(values)\n",
    "    if still_nan.any():\n",
    "        # Check for columns that are entirely NaN after group fill\n",
    "        all_nan_cols = np.where(np.isnan(values).all(axis=0))[0]\n",
    "        if len(all_nan_cols):\n",
    "            drop_cols = [num_feats[i] for i in all_nan_cols]\n",
    "            print(f\"[Impute] Dropping {len(drop_cols)} all-NaN features: {drop_cols}\")\n",
    "            \n",
    "            keep_idx = [i for i in range(values.shape[1]) if i not in all_nan_cols]\n",
    "            values = values[:, keep_idx]\n",
    "            num_feats = [num_feats[i] for i in keep_idx]\n",
    "            df.drop(columns=drop_cols, inplace=True)\n",
    "            still_nan = np.isnan(values)\n",
    "\n",
    "        if still_nan.any():\n",
    "            col_medians = np.nanmedian(values, axis=0)\n",
    "            row_idx, col_idx = np.where(still_nan)\n",
    "            values[row_idx, col_idx] = col_medians[col_idx]\n",
    "\n",
    "    # Write back\n",
    "    df.loc[:, num_feats] = values\n",
    "    df.drop(columns=[\"__len_bin__\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "    # Report imputation summary\n",
    "    after_missing = df[num_feats].isna().sum()\n",
    "    imputed_counts = (before_missing.reindex(num_feats).fillna(0).astype(int)\n",
    "                      - after_missing.reindex(num_feats).fillna(0).astype(int))\n",
    "    total_imputed = int(imputed_counts.sum())\n",
    "    \n",
    "    if total_imputed:\n",
    "        print(f\"\\n[Impute] Filled {total_imputed} missing values across {(imputed_counts > 0).sum()} features\")\n",
    "        top_imputed = imputed_counts[imputed_counts > 0].sort_values(ascending=False).head(10)\n",
    "        print(\"Top 10 imputed features:\")\n",
    "        print(top_imputed.to_string())\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "df_with_features = impute_after_capping(df_with_features, max_missing_pct=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9480101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Analysis] Found 20 highly correlated pairs (>0.85):\n",
      "                  feature_1              feature_2  correlation\n",
      "     right_dependency_ratio  left_dependency_ratio     1.000000\n",
      "               n_tokens_doc                  n_tok     0.999482\n",
      "               n_tokens_doc            n_tokens_ws     0.997968\n",
      "                n_tokens_ws                  n_tok     0.997806\n",
      "            trigram_entropy         bigram_entropy     0.990520\n",
      "              bits_per_char      compression_ratio     0.990003\n",
      "                n_tokens_ws                n_chars     0.987497\n",
      "                      n_tok                n_chars     0.987274\n",
      "               n_tokens_doc                n_chars     0.986993\n",
      "          trigram_diversity       bigram_diversity     0.968799\n",
      "                 smog_index    flesch_reading_ease     0.936907\n",
      "                gunning_fog    flesch_reading_ease     0.933960\n",
      "                 smog_index            gunning_fog     0.932124\n",
      "           whitespace_ratio        avg_word_length     0.923660\n",
      "          unigram_diversity       type_token_ratio     0.908058\n",
      "          compression_ratio char_trigram_diversity     0.905012\n",
      "              bits_per_char char_trigram_diversity     0.903687\n",
      "automated_readability_index            gunning_fog     0.903287\n",
      "           hapax_type_ratio   hapax_legomena_ratio     0.880699\n",
      "     char_trigram_diversity       type_token_ratio     0.866276\n",
      "\n",
      "======================================================================\n",
      "  [Recommendation] Drop 'left_dependency_ratio', keep 'right_dependency_ratio' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'n_tok', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'n_tokens_ws', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'bigram_entropy', keep 'trigram_entropy' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'compression_ratio', keep 'bits_per_char' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'n_chars', keep 'n_tokens_doc' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'bigram_diversity', keep 'trigram_diversity' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'flesch_reading_ease', keep 'smog_index' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'gunning_fog', keep 'smog_index' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'avg_word_length', keep 'whitespace_ratio' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'type_token_ratio', keep 'unigram_diversity' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'char_trigram_diversity', keep 'bits_per_char' (missing: 0.0% vs 0.0%)\n",
      "  [Recommendation] Drop 'hapax_legomena_ratio', keep 'hapax_type_ratio' (missing: 0.0% vs 0.0%)\n",
      "\n",
      "[Summary] 13 features recommended for dropping\n",
      "  Current features: 57\n",
      "  After dropping: 44\n",
      "\n",
      "[Note] This is analysis only - no changes have been made to the dataframe\n"
     ]
    }
   ],
   "source": [
    "def analyze_correlated_features(\n",
    "    df: pd.DataFrame,\n",
    "    feature_cols: List[str] = None,\n",
    "    threshold: float = 0.85,\n",
    "    method: str = 'pearson',\n",
    "    keep_strategy: str = 'lower_missing'\n",
    ") -> Tuple[List[str], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Analyze highly correlated features WITHOUT modifying the dataframe.\n",
    "    Provides recommendations only.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature_cols : List[str], optional\n",
    "        List of feature columns to check. If None, uses all numeric columns except 'is_ai'\n",
    "    threshold : float\n",
    "        Correlation threshold (default 0.85). Pairs above this are considered redundant\n",
    "    method : str\n",
    "        Correlation method: 'pearson', 'spearman', or 'kendall'\n",
    "    keep_strategy : str\n",
    "        Which feature to keep from correlated pairs:\n",
    "        - 'lower_missing': Keep feature with less missing data\n",
    "        - 'higher_variance': Keep feature with higher variance\n",
    "        - 'first': Keep the first feature alphabetically\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    recommended_drops : List[str]\n",
    "        List of features recommended for dropping\n",
    "    corr_pairs : pd.DataFrame\n",
    "        DataFrame showing correlated pairs and correlation values\n",
    "    \"\"\"\n",
    "    # Identify feature columns\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                       if c != 'is_ai']\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df[feature_cols].corr(method=method).abs()\n",
    "    \n",
    "    # Get upper triangle (avoid double-counting pairs)\n",
    "    upper_tri = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Find correlated pairs\n",
    "    correlated_pairs = []\n",
    "    for col in upper_tri.columns:\n",
    "        high_corr = upper_tri[col][upper_tri[col] > threshold]\n",
    "        for idx, corr_val in high_corr.items():\n",
    "            correlated_pairs.append({\n",
    "                'feature_1': col,\n",
    "                'feature_2': idx,\n",
    "                'correlation': corr_val\n",
    "            })\n",
    "    \n",
    "    if not correlated_pairs:\n",
    "        print(f\"[Analysis] No feature pairs with correlation > {threshold}\")\n",
    "        return [], pd.DataFrame()\n",
    "    \n",
    "    # Create dataframe of correlated pairs\n",
    "    corr_df = pd.DataFrame(correlated_pairs).sort_values('correlation', ascending=False)\n",
    "    \n",
    "    print(f\"\\n[Analysis] Found {len(corr_df)} highly correlated pairs (>{threshold}):\")\n",
    "    print(corr_df.to_string(index=False))\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    \n",
    "    # Determine which features would be recommended for dropping\n",
    "    to_drop: Set[str] = set()\n",
    "    \n",
    "    for _, row in corr_df.iterrows():\n",
    "        feat1, feat2 = row['feature_1'], row['feature_2']\n",
    "        \n",
    "        # Skip if either already marked for dropping\n",
    "        if feat1 in to_drop or feat2 in to_drop:\n",
    "            continue\n",
    "        \n",
    "        # Decide which to keep based on strategy\n",
    "        if keep_strategy == 'lower_missing':\n",
    "            miss1 = df[feat1].isna().mean()\n",
    "            miss2 = df[feat2].isna().mean()\n",
    "            drop_feat = feat1 if miss1 > miss2 else feat2\n",
    "            keep_feat = feat2 if miss1 > miss2 else feat1\n",
    "            reason = f\"missing: {miss1:.1%} vs {miss2:.1%}\"\n",
    "            \n",
    "        elif keep_strategy == 'higher_variance':\n",
    "            var1 = df[feat1].var()\n",
    "            var2 = df[feat2].var()\n",
    "            drop_feat = feat1 if var1 < var2 else feat2\n",
    "            keep_feat = feat2 if var1 < var2 else feat1\n",
    "            reason = f\"variance: {var1:.2f} vs {var2:.2f}\"\n",
    "            \n",
    "        else:  # 'first'\n",
    "            drop_feat = max(feat1, feat2)  # Drop lexicographically later\n",
    "            keep_feat = min(feat1, feat2)\n",
    "            reason = \"alphabetical\"\n",
    "        \n",
    "        to_drop.add(drop_feat)\n",
    "        print(f\"  [Recommendation] Drop '{drop_feat}', keep '{keep_feat}' ({reason})\")\n",
    "    \n",
    "    # Summary\n",
    "    recommended_list = sorted(to_drop)\n",
    "    \n",
    "    print(f\"\\n[Summary] {len(recommended_list)} features recommended for dropping\")\n",
    "    print(f\"  Current features: {len(feature_cols)}\")\n",
    "    print(f\"  After dropping: {len(feature_cols) - len(recommended_list)}\")\n",
    "    print(f\"\\n[Note] This is analysis only - no changes have been made to the dataframe\")\n",
    "    \n",
    "    return recommended_list, corr_df\n",
    "\n",
    "\n",
    "# Usage - ANALYSIS ONLY (no changes to data)\n",
    "recommended_drops, correlation_pairs = analyze_correlated_features(\n",
    "    df_with_features,\n",
    "    threshold=0.85,\n",
    "    keep_strategy='lower_missing'\n",
    ")\n",
    "\n",
    "# Optional: Visualize correlation matrix\n",
    "def plot_correlation_heatmap(df, feature_cols=None, title=\"Feature Correlations\"):\n",
    "    \"\"\"Plot correlation heatmap for visual inspection\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "                       if c != 'is_ai']\n",
    "    \n",
    "    corr = df[feature_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool), k=1)\n",
    "    sns.heatmap(corr, mask=mask, annot=False, cmap='coolwarm', \n",
    "                center=0, vmin=-1, vmax=1, square=True,\n",
    "                linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title(title, fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize current correlations (optional)\n",
    "# plot_correlation_heatmap(df_with_features, title=\"Current Feature Correlations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5695e6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_correlated_features_strategic(df: pd.DataFrame, threshold: float = 0.85):\n",
    "    \"\"\"Enhanced version that addresses ALL correlation pairs\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    drops = {\n",
    "        # === Token counting (choose ONE representative) ===\n",
    "        'n_tok': 'Near-duplicate of n_tokens_doc (0.9997)',\n",
    "        'n_tokens_ws': 'Near-duplicate of n_tokens_doc (0.9991)', \n",
    "        'n_chars': 'Redundant with token counts (0.9937)',\n",
    "        # KEEP: n_tokens_doc (most explicit naming)\n",
    "        \n",
    "        # === Dependency ratios (perfect inverses) ===\n",
    "        'right_dependency_ratio': 'Perfect inverse of left_dependency_ratio (1.000)',\n",
    "        # KEEP: left_dependency_ratio\n",
    "        \n",
    "        # === N-gram entropy vs diversity ===\n",
    "        'bigram_entropy': 'Redundant with trigram_entropy (0.991)',\n",
    "        'trigram_entropy': 'Redundant with trigram_diversity (0.969)',\n",
    "        'bigram_diversity': 'Redundant with trigram_diversity (0.969)',\n",
    "        # KEEP: trigram_diversity (most granular + interpretable)\n",
    "        \n",
    "        # === Compression metrics ===\n",
    "        'bits_per_char': 'Redundant with compression_ratio (0.985)',\n",
    "        # But char_trigram_diversity correlates with both (0.90, 0.90)\n",
    "        # Keep compression_ratio + char_trigram_diversity (different concepts)\n",
    "        \n",
    "        # === Readability indices (keep 1-2 most standard) ===\n",
    "        'smog_index': 'Redundant with flesch_reading_ease (0.940)',\n",
    "        'gunning_fog': 'Redundant with flesch_reading_ease (0.934)',\n",
    "        'automated_readability_index': 'Redundant with gunning_fog (0.914)',\n",
    "        # KEEP: flesch_reading_ease (most widely used)\n",
    "        \n",
    "        # === Lexical diversity ===\n",
    "        'unigram_diversity': 'Redundant with type_token_ratio (0.903)',\n",
    "        # KEEP: type_token_ratio (standard metric)\n",
    "        \n",
    "        # === Hapax metrics ===\n",
    "        'hapax_type_ratio': 'Redundant with hapax_legomena_ratio (0.874)',\n",
    "        # KEEP: hapax_legomena_ratio (more standard terminology)\n",
    "        \n",
    "        # === Word length metrics ===\n",
    "        'whitespace_ratio': 'Redundant with avg_word_length (0.928)',\n",
    "        # KEEP: avg_word_length (more interpretable)\n",
    "    }\n",
    "    \n",
    "    # Note: n_sentences_doc correlates with token counts (0.86)\n",
    "    # but represents different information (document structure vs size)\n",
    "    # Decision: KEEP both\n",
    "    \n",
    "    existing_drops = {k: v for k, v in drops.items() if k in df.columns}\n",
    "    \n",
    "    print(f\"[Strategic Drop] Removing {len(existing_drops)} redundant features:\\n\")\n",
    "    for feat, reason in existing_drops.items():\n",
    "        print(f\"  ✗ {feat}\")\n",
    "        print(f\"    → {reason}\\n\")\n",
    "    \n",
    "    df_filtered = df.drop(columns=list(existing_drops.keys()))\n",
    "    \n",
    "    # Verification\n",
    "    feature_cols = [c for c in df_filtered.select_dtypes(include=[np.number]).columns \n",
    "                   if c != 'is_ai']\n",
    "    corr_matrix = df_filtered[feature_cols].corr().abs()\n",
    "    upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    \n",
    "    remaining_high = []\n",
    "    for col in upper_tri.columns:\n",
    "        high_corr = upper_tri[col][upper_tri[col] > threshold]\n",
    "        if not high_corr.empty:\n",
    "            for idx, val in high_corr.items():\n",
    "                remaining_high.append((col, idx, val))\n",
    "    \n",
    "    if remaining_high:\n",
    "        print(f\"[Warning] {len(remaining_high)} pairs still exceed threshold:\")\n",
    "        for f1, f2, corr in sorted(remaining_high, key=lambda x: x[2], reverse=True):\n",
    "            print(f\"  {f1} <-> {f2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(f\"[Success] No correlations > {threshold} remain\")\n",
    "    \n",
    "    print(f\"\\n[Final] {len(feature_cols)} features retained\")\n",
    "    \n",
    "    return df_filtered, existing_drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b25a5469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Strategic Drop] Removing 14 redundant features:\n",
      "\n",
      "  ✗ n_tok\n",
      "    → Near-duplicate of n_tokens_doc (0.9997)\n",
      "\n",
      "  ✗ n_tokens_ws\n",
      "    → Near-duplicate of n_tokens_doc (0.9991)\n",
      "\n",
      "  ✗ n_chars\n",
      "    → Redundant with token counts (0.9937)\n",
      "\n",
      "  ✗ right_dependency_ratio\n",
      "    → Perfect inverse of left_dependency_ratio (1.000)\n",
      "\n",
      "  ✗ bigram_entropy\n",
      "    → Redundant with trigram_entropy (0.991)\n",
      "\n",
      "  ✗ trigram_entropy\n",
      "    → Redundant with trigram_diversity (0.969)\n",
      "\n",
      "  ✗ bigram_diversity\n",
      "    → Redundant with trigram_diversity (0.969)\n",
      "\n",
      "  ✗ bits_per_char\n",
      "    → Redundant with compression_ratio (0.985)\n",
      "\n",
      "  ✗ smog_index\n",
      "    → Redundant with flesch_reading_ease (0.940)\n",
      "\n",
      "  ✗ gunning_fog\n",
      "    → Redundant with flesch_reading_ease (0.934)\n",
      "\n",
      "  ✗ automated_readability_index\n",
      "    → Redundant with gunning_fog (0.914)\n",
      "\n",
      "  ✗ unigram_diversity\n",
      "    → Redundant with type_token_ratio (0.903)\n",
      "\n",
      "  ✗ hapax_type_ratio\n",
      "    → Redundant with hapax_legomena_ratio (0.874)\n",
      "\n",
      "  ✗ whitespace_ratio\n",
      "    → Redundant with avg_word_length (0.928)\n",
      "\n",
      "[Warning] 2 pairs still exceed threshold:\n",
      "  compression_ratio <-> char_trigram_diversity: 0.905\n",
      "  char_trigram_diversity <-> type_token_ratio: 0.866\n",
      "\n",
      "[Final] 43 features retained\n"
     ]
    }
   ],
   "source": [
    "# Apply strategic dropping\n",
    "df_with_features, drop_log = drop_correlated_features_strategic(df_with_features, threshold=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5581ceaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quality distribution by source_type:\n",
      "quality_score  0   1   2     3\n",
      "model                         \n",
      "chatgpt        0   0   0   139\n",
      "cohere         0   0   4   133\n",
      "cohere-chat    0   0   0   136\n",
      "gpt2           1   4  24   107\n",
      "gpt3           0   2   3   131\n",
      "gpt4           0   0   0   136\n",
      "human          0   4  19  1477\n",
      "llama-chat     0   0   7   129\n",
      "mistral        0   1  11   124\n",
      "mistral-chat   0   0   1   135\n",
      "mpt            1  21  43    71\n",
      "mpt-chat       4  11  12   109\n",
      "Saved enriched dataset: raid_sample_small_with_features_CLEANED.csv (rows: 3000)\n"
     ]
    }
   ],
   "source": [
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_CLEANED.csv\"\n",
    "\n",
    "# Ensure balanced quality across train/test when you split later\n",
    "print(f\"\\nQuality distribution by source_type:\")\n",
    "print(df_with_features.groupby(['model', 'quality_score']).size().unstack(fill_value=0))\n",
    "\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS} (rows: {len(df_with_features)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebee79f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'adv_source_id', 'source_id', 'model', 'decoding', 'repetition_penalty', 'attack', 'domain', 'title', 'prompt', 'generation', 'is_ai', 'source_type', 'generation_raw', 'had_urls', 'had_html', 'had_code', 'had_table', 'alpha_ratio', 'digit_ratio', 'punct_ratio', 'avg_word_length', 'std_word_length', 'entropy_bits', 'entropy_norm', 'is_text_like', 'length_bin', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'n_tokens_doc', 'n_sentences_doc', 'resegmented', 'flesch_reading_ease', 'trigram_diversity', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'uppercase_ratio', 'unique_char_count', 'compression_ratio', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'depth_check_passed', 'hapax_legomena_ratio', 'yules_k', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio', 'features_capped', 'parse_quality_issue', 'readability_anomaly', 'lexical_anomaly', 'quality_score']\n"
     ]
    }
   ],
   "source": [
    "print(df_with_features.columns.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
