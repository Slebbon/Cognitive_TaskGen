{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\cmudict.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded small dataset with 3000 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>std_word_length</th>\n",
       "      <th>entropy_bits</th>\n",
       "      <th>entropy_norm</th>\n",
       "      <th>is_text_like</th>\n",
       "      <th>not_text_reason</th>\n",
       "      <th>n_tokens_ws</th>\n",
       "      <th>length_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>404002f2-1cdb-4021-9215-976801dbf032</td>\n",
       "      <td>0333dff5-5fb3-4c06-a892-3ae170dfae75</td>\n",
       "      <td>0333dff5-5fb3-4c06-a892-3ae170dfae75</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>whitespace</td>\n",
       "      <td>reddit</td>\n",
       "      <td>Anxiety over naptime and transitioning to daycare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>0.022454</td>\n",
       "      <td>4.008163</td>\n",
       "      <td>2.136164</td>\n",
       "      <td>4.252406</td>\n",
       "      <td>0.793722</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>248</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eedc3d68-f646-425b-bb9c-9ce469d1a348</td>\n",
       "      <td>07c99297-5b54-4240-987f-91cb2e6bbe45</td>\n",
       "      <td>07c99297-5b54-4240-987f-91cb2e6bbe45</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insert_paragraphs</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Love And War</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050992</td>\n",
       "      <td>3.852518</td>\n",
       "      <td>1.865006</td>\n",
       "      <td>4.475145</td>\n",
       "      <td>0.767226</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>270</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146c7064-c894-4290-b79e-daa517c3dec7</td>\n",
       "      <td>b5faf769-2be4-4428-9cde-6a6fd3fdc80a</td>\n",
       "      <td>b5faf769-2be4-4428-9cde-6a6fd3fdc80a</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>perplexity_misspelling</td>\n",
       "      <td>reddit</td>\n",
       "      <td>Socialists have more interest in controlling y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.032476</td>\n",
       "      <td>4.210145</td>\n",
       "      <td>2.178461</td>\n",
       "      <td>4.284333</td>\n",
       "      <td>0.794525</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  404002f2-1cdb-4021-9215-976801dbf032  0333dff5-5fb3-4c06-a892-3ae170dfae75   \n",
       "1  eedc3d68-f646-425b-bb9c-9ce469d1a348  07c99297-5b54-4240-987f-91cb2e6bbe45   \n",
       "2  146c7064-c894-4290-b79e-daa517c3dec7  b5faf769-2be4-4428-9cde-6a6fd3fdc80a   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  0333dff5-5fb3-4c06-a892-3ae170dfae75  human      NaN                NaN   \n",
       "1  07c99297-5b54-4240-987f-91cb2e6bbe45  human      NaN                NaN   \n",
       "2  b5faf769-2be4-4428-9cde-6a6fd3fdc80a  human      NaN                NaN   \n",
       "\n",
       "                   attack  domain  \\\n",
       "0              whitespace  reddit   \n",
       "1       insert_paragraphs  poetry   \n",
       "2  perplexity_misspelling  reddit   \n",
       "\n",
       "                                               title prompt  ... digit_ratio  \\\n",
       "0  Anxiety over naptime and transitioning to daycare    NaN  ...    0.003208   \n",
       "1                                       Love And War    NaN  ...    0.000000   \n",
       "2  Socialists have more interest in controlling y...    NaN  ...    0.004060   \n",
       "\n",
       "   punct_ratio avg_word_length  std_word_length  entropy_bits  entropy_norm  \\\n",
       "0     0.022454        4.008163         2.136164      4.252406      0.793722   \n",
       "1     0.050992        3.852518         1.865006      4.475145      0.767226   \n",
       "2     0.032476        4.210145         2.178461      4.284333      0.794525   \n",
       "\n",
       "   is_text_like  not_text_reason  n_tokens_ws  length_bin  \n",
       "0          True              NaN          248        long  \n",
       "1          True              NaN          270        long  \n",
       "2          True              NaN          132       short  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuration and dataset selection\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths to the three samples\n",
    "SAMPLE_PATHS = {\n",
    "    \"small\":  \"raid_sample_small.csv\",\n",
    "    \"medium\": \"raid_sample_medium.csv\",\n",
    "    \"large\":  \"raid_sample_large.csv\",\n",
    "}\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Set this to one of: \"small\", \"medium\", \"large\"\n",
    "SELECTED_DATASET = \"small\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "TEXT_COL = \"generation\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATHS[SELECTED_DATASET])\n",
    "print(f\"Loaded {SELECTED_DATASET} dataset with {len(df)} rows.\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6afd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy, CMUdict (NLTK), and g2p_en fallback\n",
    "\n",
    "# Ensure CMUdict is available\n",
    "nltk.download('cmudict', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "CMU = cmudict.dict()  # key: lowercase word, value: list of pronunciations (list of ARPAbet tokens)\n",
    "\n",
    "# Load spaCy English model (use 'en_core_web_sm' unless you already have md/lg installed)\n",
    "try:\n",
    "    nlp: Language = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure sentence boundaries are available (parser usually handles this; add sentencizer if needed)\n",
    "if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# g2p_en for OOV coverage\n",
    "G2P = G2p()\n",
    "\n",
    "# ARPAbet vowel bases (stress digits removed when checking)\n",
    "ARPA_VOWELS = {\n",
    "    \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
    "    \"EH\", \"ER\", \"EY\",\n",
    "    \"IH\", \"IY\",\n",
    "    \"OW\", \"OY\",\n",
    "    \"UH\", \"UW\"\n",
    "}\n",
    "\n",
    "# Cache syllable counts for speed\n",
    "_SYLL_CACHE: Dict[str, int] = {}\n",
    "\n",
    "def cmu_syllables(word: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Returns syllable count using CMUdict if available; else None.\n",
    "    Policy: use the first pronunciation variant.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w not in CMU:\n",
    "        return None\n",
    "    phones = CMU[w][0]\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    return max(count, 1)  # at least one for non-empty alphabetic words\n",
    "\n",
    "def g2p_syllables(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns syllable count using neural g2p_en; counts vowel phonemes.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w in _SYLL_CACHE:\n",
    "        return _SYLL_CACHE[w]\n",
    "    phones = G2P(w)\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    # Guard: ensure >=1 for alphabetic tokens\n",
    "    if count == 0 and re.search(r\"[A-Za-z]\", w):\n",
    "        count = 1\n",
    "    _SYLL_CACHE[w] = count\n",
    "    return count\n",
    "\n",
    "def syllables_hybrid(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Hybrid policy: try CMUdict first; if OOV, fall back to g2p_en.\n",
    "    \"\"\"\n",
    "    c = cmu_syllables(word)\n",
    "    if c is not None:\n",
    "        return c\n",
    "    return g2p_syllables(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3020ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation utilities using spaCy + CMUdict with g2p_en fallback\n",
    "\n",
    "def _word_like(tok) -> bool:\n",
    "    \"\"\"\n",
    "    Select lexical tokens (alphabetic, not space).\n",
    "    spaCy's tok.is_alpha ensures letter-only tokens; change if you want alphanumerics.\n",
    "    \"\"\"\n",
    "    return tok.is_alpha and not tok.is_space\n",
    "\n",
    "def _alnum_char_count(token_text: str) -> int:\n",
    "    \"\"\"Count alphanumeric characters for ARI; excludes whitespace and punctuation.\"\"\"\n",
    "    return sum(ch.isalnum() for ch in token_text)\n",
    "\n",
    "def features_from_doc(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - avg_word_length\n",
    "      - type_token_ratio\n",
    "      - stopword_ratio\n",
    "      - punctuation_ratio       (punct chars / non-space chars)\n",
    "      - avg_sentence_length     (words per sentence)\n",
    "      - sentence_length_std     (std of sentence word counts)\n",
    "      - flesch_reading_ease\n",
    "      - gunning_fog\n",
    "      - smog_index\n",
    "      - automated_readability_index\n",
    "    \"\"\"\n",
    "    # Sentences\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    n_sents = max(len(sents), 1)\n",
    "\n",
    "    # Token groups\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    punct_toks = [t for t in doc if t.is_punct]\n",
    "    nonspace_toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "    W = len(word_toks)\n",
    "\n",
    "    # Characters for ARI and punctuation ratio\n",
    "    chars_alnum = sum(_alnum_char_count(t.text) for t in nonspace_toks)\n",
    "    punct_chars = sum(len(t.text) for t in punct_toks)\n",
    "    nonspace_chars = sum(len(t.text) for t in nonspace_toks)\n",
    "\n",
    "    # Sentence-level word counts\n",
    "    sent_word_counts = [sum(1 for t in s if _word_like(t)) for s in sents]\n",
    "    avg_sentence_length = float(np.mean(sent_word_counts)) if sent_word_counts else 0.0\n",
    "    sentence_length_std  = float(np.std(sent_word_counts, ddof=0)) if len(sent_word_counts) > 1 else 0.0\n",
    "\n",
    "    # Word-level lengths\n",
    "    word_lengths = [len(t.text) for t in word_toks]\n",
    "    avg_word_length = float(np.mean(word_lengths)) if word_lengths else 0.0\n",
    "\n",
    "    # Type-token ratio (lowercased forms)\n",
    "    vocab = {t.text.lower() for t in word_toks}\n",
    "    type_token_ratio = (len(vocab) / W) if W > 0 else 0.0\n",
    "\n",
    "    # Stopword ratio via spaCy stop flags\n",
    "    stop_count = sum(1 for t in word_toks if t.is_stop)\n",
    "    stopword_ratio = (stop_count / W) if W > 0 else 0.0\n",
    "\n",
    "    # Punctuation ratio over non-space characters\n",
    "    punctuation_ratio = (punct_chars / nonspace_chars) if nonspace_chars > 0 else 0.0\n",
    "\n",
    "    # Syllables (hybrid)\n",
    "    syll_per_word = [syllables_hybrid(t.text) for t in word_toks] if W > 0 else []\n",
    "    syll_total = int(np.sum(syll_per_word)) if syll_per_word else 0\n",
    "    polysyllables = int(np.sum([syl >= 3 for syl in syll_per_word])) if syll_per_word else 0\n",
    "    complex_words = polysyllables  # standard: >= 3 syllables\n",
    "\n",
    "    # Rates for readability\n",
    "    words_per_sentence = (W / n_sents) if n_sents > 0 else 0.0\n",
    "    syllables_per_word = (syll_total / W) if W > 0 else 0.0\n",
    "    chars_per_word_ari = (chars_alnum / W) if W > 0 else 0.0\n",
    "\n",
    "    # Readability indices\n",
    "    # Flesch Reading Ease\n",
    "    flesch = 206.835 - 1.015 * words_per_sentence - 84.6 * syllables_per_word\n",
    "\n",
    "    # Gunning Fog\n",
    "    fog = 0.4 * (words_per_sentence + 100.0 * (complex_words / W if W > 0 else 0.0))\n",
    "\n",
    "    # SMOG\n",
    "    smog = (1.043 * math.sqrt(polysyllables * (30.0 / n_sents)) + 3.1291) if (polysyllables > 0 and n_sents > 0) else 0.0\n",
    "\n",
    "    # Automated Readability Index\n",
    "    ari = 4.71 * chars_per_word_ari + 0.5 * words_per_sentence - 21.43\n",
    "\n",
    "    return {\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"punctuation_ratio\": punctuation_ratio,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"sentence_length_std\": sentence_length_std,\n",
    "        \"flesch_reading_ease\": flesch,\n",
    "        \"gunning_fog\": fog,\n",
    "        \"smog_index\": smog,\n",
    "        \"automated_readability_index\": ari,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74acf093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram and burstiness utility functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# N-gram feature extraction utilities\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def ngram_diversity(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate n-gram diversity (unique n-grams / total n-grams).\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def ngram_entropy(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of n-gram distribution.\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    \n",
    "    counts = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_burstiness(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate burstiness coefficient based on word frequency distribution.\n",
    "    Burstiness = (sigma - mu) / (sigma + mu)\n",
    "    where mu is mean frequency and sigma is standard deviation.\n",
    "    Returns 0 if only one unique token.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    frequencies = list(word_counts.values())\n",
    "    \n",
    "    if len(frequencies) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    mu = np.mean(frequencies)\n",
    "    sigma = np.std(frequencies, ddof=0)\n",
    "    \n",
    "    if mu + sigma == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (sigma - mu) / (sigma + mu)\n",
    "\n",
    "print(\"N-gram and burstiness utility functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9638c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Character-level feature extraction\n",
    "\n",
    "import gzip\n",
    "\n",
    "def character_ngram_features(text: str, n: int = 3) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract character n-gram diversity and entropy.\n",
    "    Returns diversity ratio and entropy for character n-grams.\n",
    "    \"\"\"\n",
    "    if len(text) < n:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    char_ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    \n",
    "    if not char_ngrams:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Diversity\n",
    "    diversity = len(set(char_ngrams)) / len(char_ngrams)\n",
    "    \n",
    "    # Entropy\n",
    "    counts = Counter(char_ngrams)\n",
    "    total = len(char_ngrams)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "    \n",
    "    return diversity, entropy\n",
    "\n",
    "def compression_ratio(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate gzip compression ratio: compressed_size / original_size.\n",
    "    Lower values indicate more compressible (potentially more predictable) text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 1.0\n",
    "    \n",
    "    original_bytes = text.encode('utf-8')\n",
    "    compressed_bytes = gzip.compress(original_bytes)\n",
    "    \n",
    "    return len(compressed_bytes) / len(original_bytes)\n",
    "\n",
    "def character_statistics(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract surface-level character statistics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            \"uppercase_ratio\": 0.0,\n",
    "            \"digit_ratio\": 0.0,\n",
    "            \"whitespace_ratio\": 0.0,\n",
    "            \"unique_char_count\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total_chars = len(text)\n",
    "    \n",
    "    return {\n",
    "        \"uppercase_ratio\": sum(1 for c in text if c.isupper()) / total_chars,\n",
    "        \"digit_ratio\": sum(1 for c in text if c.isdigit()) / total_chars,\n",
    "        \"whitespace_ratio\": sum(1 for c in text if c.isspace()) / total_chars,\n",
    "        \"unique_char_count\": float(len(set(text))),\n",
    "    }\n",
    "\n",
    "print(\"Character-level feature functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74b9e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency tree feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Dependency tree and syntactic structure features\n",
    "\n",
    "def get_tree_depth(token) -> int:\n",
    "    \"\"\"\n",
    "    Recursively calculate the depth of the dependency subtree rooted at token.\n",
    "    \"\"\"\n",
    "    if not list(token.children):\n",
    "        return 1\n",
    "    return 1 + max(get_tree_depth(child) for child in token.children)\n",
    "\n",
    "def dependency_tree_features(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract dependency tree structural features.\n",
    "    \"\"\"\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    \n",
    "    if not word_toks:\n",
    "        return {\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0.0,\n",
    "            \"avg_dependency_distance\": 0.0,\n",
    "            \"left_dependency_ratio\": 0.0,\n",
    "            \"right_dependency_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Find sentence roots and calculate tree depths\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    tree_depths = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        root = [token for token in sent if token.head == token]\n",
    "        if root:\n",
    "            tree_depths.append(get_tree_depth(root[0]))\n",
    "    \n",
    "    avg_tree_depth = float(np.mean(tree_depths)) if tree_depths else 0.0\n",
    "    max_tree_depth = float(np.max(tree_depths)) if tree_depths else 0.0\n",
    "    \n",
    "    # Dependency distances (how far apart head and dependent are)\n",
    "    dep_distances = []\n",
    "    left_deps = 0\n",
    "    right_deps = 0\n",
    "    \n",
    "    for token in word_toks:\n",
    "        if token.head != token:  # Not root\n",
    "            distance = abs(token.i - token.head.i)\n",
    "            dep_distances.append(distance)\n",
    "            \n",
    "            # Track direction\n",
    "            if token.i < token.head.i:\n",
    "                left_deps += 1\n",
    "            else:\n",
    "                right_deps += 1\n",
    "    \n",
    "    avg_dep_distance = float(np.mean(dep_distances)) if dep_distances else 0.0\n",
    "    total_deps = left_deps + right_deps\n",
    "    \n",
    "    left_ratio = left_deps / total_deps if total_deps > 0 else 0.0\n",
    "    right_ratio = right_deps / total_deps if total_deps > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"avg_tree_depth\": avg_tree_depth,\n",
    "        \"max_tree_depth\": max_tree_depth,\n",
    "        \"avg_dependency_distance\": avg_dep_distance,\n",
    "        \"left_dependency_ratio\": left_ratio,\n",
    "        \"right_dependency_ratio\": right_ratio,\n",
    "    }\n",
    "\n",
    "print(\"Dependency tree feature functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89de6ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sophistication functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary sophistication and lexical diversity measures\n",
    "\n",
    "def hapax_legomena_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate ratio of words appearing exactly once (hapax legomena).\n",
    "    Indicates vocabulary richness.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    hapax_count = sum(1 for count in word_counts.values() if count == 1)\n",
    "    \n",
    "    return hapax_count / len(tokens)\n",
    "\n",
    "def yules_k(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Yule's K measure of lexical diversity.\n",
    "    More robust to text length than TTR.\n",
    "    K = 10000 * (M2 - M1) / (M1 * M1)\n",
    "    where M1 = number of tokens, M2 = sum of (frequency^2 * num_words_with_that_frequency)\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    M1 = len(tokens)\n",
    "    \n",
    "    frequency_spectrum = Counter(word_counts.values())\n",
    "    M2 = sum(freq * freq * count for freq, count in frequency_spectrum.items())\n",
    "    \n",
    "    if M1 <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    K = 10000 * (M2 - M1) / (M1 * M1)\n",
    "    return K\n",
    "\n",
    "def mtld(tokens: List[str], threshold: float = 0.72) -> float:\n",
    "    \"\"\"\n",
    "    Calculate MTLD (Measure of Textual Lexical Diversity).\n",
    "    Counts how many sequential word segments maintain TTR above threshold.\n",
    "    \"\"\"\n",
    "    if len(tokens) < 10:\n",
    "        return float(len(set(tokens)))\n",
    "    \n",
    "    def compute_factor(token_list):\n",
    "        ttr = 1.0\n",
    "        word_set = set()\n",
    "        factor_count = 0.0\n",
    "        \n",
    "        for i, token in enumerate(token_list, 1):\n",
    "            word_set.add(token)\n",
    "            ttr = len(word_set) / i\n",
    "            \n",
    "            if ttr < threshold:\n",
    "                factor_count += 1\n",
    "                word_set = set()\n",
    "        \n",
    "        if len(word_set) > 0:\n",
    "            factor_count += (1.0 - ttr) / (1.0 - threshold)\n",
    "        \n",
    "        return factor_count\n",
    "    \n",
    "    # Calculate forward and backward\n",
    "    forward = compute_factor(tokens)\n",
    "    backward = compute_factor(list(reversed(tokens)))\n",
    "    \n",
    "    if forward == 0 and backward == 0:\n",
    "        return float(len(tokens))\n",
    "    \n",
    "    factors = [f for f in [forward, backward] if f > 0]\n",
    "    mean_factor = np.mean(factors)\n",
    "    \n",
    "    return len(tokens) / mean_factor if mean_factor > 0 else float(len(tokens))\n",
    "\n",
    "def vocabulary_sophistication_features(tokens: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Aggregate vocabulary sophistication measures.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"hapax_legomena_ratio\": hapax_legomena_ratio(tokens),\n",
    "        \"yules_k\": yules_k(tokens),\n",
    "        \"mtld\": mtld(tokens),\n",
    "    }\n",
    "\n",
    "print(\"Vocabulary sophistication functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dfb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation pattern functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Punctuation pattern analysis\n",
    "\n",
    "def punctuation_patterns(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detailed punctuation pattern features beyond simple ratio.\n",
    "    \"\"\"\n",
    "    all_tokens = [t for t in doc if not t.is_space]\n",
    "    punct_tokens = [t for t in doc if t.is_punct]\n",
    "    \n",
    "    if not all_tokens:\n",
    "        return {\n",
    "            \"comma_ratio\": 0.0,\n",
    "            \"period_ratio\": 0.0,\n",
    "            \"question_ratio\": 0.0,\n",
    "            \"exclamation_ratio\": 0.0,\n",
    "            \"semicolon_ratio\": 0.0,\n",
    "            \"colon_ratio\": 0.0,\n",
    "            \"quote_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total = len(all_tokens)\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punct_text = ''.join([t.text for t in punct_tokens])\n",
    "    \n",
    "    return {\n",
    "        \"comma_ratio\": punct_text.count(',') / total,\n",
    "        \"period_ratio\": punct_text.count('.') / total,\n",
    "        \"question_ratio\": punct_text.count('?') / total,\n",
    "        \"exclamation_ratio\": punct_text.count('!') / total,\n",
    "        \"semicolon_ratio\": punct_text.count(';') / total,\n",
    "        \"colon_ratio\": punct_text.count(':') / total,\n",
    "        \"quote_ratio\": (punct_text.count('\"') + punct_text.count(\"'\")) / total,\n",
    "    }\n",
    "\n",
    "print(\"Punctuation pattern functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcd67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_features(text: str, doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract sentiment and emotional tone features.\n",
    "    Uses TextBlob for polarity and subjectivity.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"sentiment_subjectivity\": 0.0,\n",
    "            \"sentiment_polarity_variance\": 0.0,\n",
    "            \"positive_word_ratio\": 0.0,\n",
    "            \"negative_word_ratio\": 0.0,\n",
    "            \"neutral_sentence_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Overall document sentiment\n",
    "    blob = TextBlob(text)\n",
    "    features = {\n",
    "        \"sentiment_polarity\": blob.sentiment.polarity,  # -1 (negative) to 1 (positive)\n",
    "        \"sentiment_subjectivity\": blob.sentiment.subjectivity,  # 0 (objective) to 1 (subjective)\n",
    "    }\n",
    "    \n",
    "    # Sentence-level sentiment variance\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    sent_polarities = []\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_blob = TextBlob(sent.text)\n",
    "        polarity = sent_blob.sentiment.polarity\n",
    "        sent_polarities.append(polarity)\n",
    "        \n",
    "        # Count neutral sentences (polarity close to 0)\n",
    "        if abs(polarity) < 0.1:\n",
    "            neutral_count += 1\n",
    "    \n",
    "    features[\"sentiment_polarity_variance\"] = float(np.var(sent_polarities)) if len(sent_polarities) > 1 else 0.0\n",
    "    features[\"neutral_sentence_ratio\"] = neutral_count / len(sents) if sents else 0.0\n",
    "    \n",
    "    # Positive/negative word ratios using spaCy tokens\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    if word_toks:\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for token in word_toks:\n",
    "            word_blob = TextBlob(token.text.lower())\n",
    "            polarity = word_blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                positive_count += 1\n",
    "            elif polarity < -0.1:\n",
    "                negative_count += 1\n",
    "        \n",
    "        features[\"positive_word_ratio\"] = positive_count / len(word_toks)\n",
    "        features[\"negative_word_ratio\"] = negative_count / len(word_toks)\n",
    "    else:\n",
    "        features[\"positive_word_ratio\"] = 0.0\n",
    "        features[\"negative_word_ratio\"] = 0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa1f44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature columns:\n",
      "['avg_word_length', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'compression_ratio', 'uppercase_ratio', 'digit_ratio', 'whitespace_ratio', 'unique_char_count', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'hapax_legomena_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <th>colon_ratio</th>\n",
       "      <th>quote_ratio</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <th>negative_word_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>404002f2-1cdb-4021-9215-976801dbf032</td>\n",
       "      <td>0333dff5-5fb3-4c06-a892-3ae170dfae75</td>\n",
       "      <td>0333dff5-5fb3-4c06-a892-3ae170dfae75</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>whitespace</td>\n",
       "      <td>reddit</td>\n",
       "      <td>Anxiety over naptime and transitioning to daycare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.473184</td>\n",
       "      <td>0.070465</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.048980</td>\n",
       "      <td>0.02449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>eedc3d68-f646-425b-bb9c-9ce469d1a348</td>\n",
       "      <td>07c99297-5b54-4240-987f-91cb2e6bbe45</td>\n",
       "      <td>07c99297-5b54-4240-987f-91cb2e6bbe45</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>insert_paragraphs</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Love And War</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>0.002915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.225272</td>\n",
       "      <td>0.601691</td>\n",
       "      <td>0.049464</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.051661</td>\n",
       "      <td>0.01476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>146c7064-c894-4290-b79e-daa517c3dec7</td>\n",
       "      <td>b5faf769-2be4-4428-9cde-6a6fd3fdc80a</td>\n",
       "      <td>b5faf769-2be4-4428-9cde-6a6fd3fdc80a</td>\n",
       "      <td>human</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>perplexity_misspelling</td>\n",
       "      <td>reddit</td>\n",
       "      <td>Socialists have more interest in controlling y...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.378333</td>\n",
       "      <td>0.341667</td>\n",
       "      <td>0.055892</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 70 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  404002f2-1cdb-4021-9215-976801dbf032  0333dff5-5fb3-4c06-a892-3ae170dfae75   \n",
       "1  eedc3d68-f646-425b-bb9c-9ce469d1a348  07c99297-5b54-4240-987f-91cb2e6bbe45   \n",
       "2  146c7064-c894-4290-b79e-daa517c3dec7  b5faf769-2be4-4428-9cde-6a6fd3fdc80a   \n",
       "\n",
       "                              source_id  model decoding repetition_penalty  \\\n",
       "0  0333dff5-5fb3-4c06-a892-3ae170dfae75  human      NaN                NaN   \n",
       "1  07c99297-5b54-4240-987f-91cb2e6bbe45  human      NaN                NaN   \n",
       "2  b5faf769-2be4-4428-9cde-6a6fd3fdc80a  human      NaN                NaN   \n",
       "\n",
       "                   attack  domain  \\\n",
       "0              whitespace  reddit   \n",
       "1       insert_paragraphs  poetry   \n",
       "2  perplexity_misspelling  reddit   \n",
       "\n",
       "                                               title prompt  ...  \\\n",
       "0  Anxiety over naptime and transitioning to daycare    NaN  ...   \n",
       "1                                       Love And War    NaN  ...   \n",
       "2  Socialists have more interest in controlling y...    NaN  ...   \n",
       "\n",
       "  exclamation_ratio  semicolon_ratio colon_ratio  quote_ratio  \\\n",
       "0          0.000000         0.000000    0.000000          0.0   \n",
       "1          0.011662         0.011662    0.002915          0.0   \n",
       "2          0.000000         0.000000    0.000000          0.0   \n",
       "\n",
       "   sentiment_polarity  sentiment_subjectivity  sentiment_polarity_variance  \\\n",
       "0            0.025321                0.473184                     0.070465   \n",
       "1            0.225272                0.601691                     0.049464   \n",
       "2            0.378333                0.341667                     0.055892   \n",
       "\n",
       "   neutral_sentence_ratio  positive_word_ratio  negative_word_ratio  \n",
       "0                0.437500             0.048980              0.02449  \n",
       "1                0.545455             0.051661              0.01476  \n",
       "2                0.700000             0.045455              0.00000  \n",
       "\n",
       "[3 rows x 70 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application NLP.pipe based\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# 1 for deterministic ordering in some environments; -1  all available cores\n",
    "N_PROCESS = -1\n",
    "\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "feature_rows: List[Dict[str, float]] = []\n",
    "for text, doc in zip(texts, nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESS)):\n",
    "    doc_features: Dict[str, float] = {}\n",
    "\n",
    "    tokens = [t.text for t in doc if _word_like(t)]\n",
    "    doc_features.update(features_from_doc(doc))\n",
    "\n",
    "    # Word n-gram diversity/entropy and burstiness\n",
    "    doc_features[\"unigram_diversity\"] = ngram_diversity(tokens, 1)\n",
    "    doc_features[\"bigram_diversity\"] = ngram_diversity(tokens, 2)\n",
    "    doc_features[\"trigram_diversity\"] = ngram_diversity(tokens, 3)\n",
    "    doc_features[\"bigram_entropy\"] = ngram_entropy(tokens, 2)\n",
    "    doc_features[\"trigram_entropy\"] = ngram_entropy(tokens, 3)\n",
    "    doc_features[\"token_burstiness\"] = calculate_burstiness(tokens)\n",
    "\n",
    "    # Character-level features\n",
    "    trigram_diversity, trigram_entropy = character_ngram_features(text, n=3)\n",
    "    doc_features[\"char_trigram_diversity\"] = trigram_diversity\n",
    "    doc_features[\"char_trigram_entropy\"] = trigram_entropy\n",
    "    doc_features[\"compression_ratio\"] = compression_ratio(text)\n",
    "    doc_features.update(character_statistics(text))\n",
    "\n",
    "    # Syntactic, lexical, punctuation, and sentiment features\n",
    "    doc_features.update(dependency_tree_features(doc))\n",
    "    doc_features.update(vocabulary_sophistication_features(tokens))\n",
    "    doc_features.update(punctuation_patterns(doc))\n",
    "    doc_features.update(sentiment_features(text, doc))\n",
    "\n",
    "    feature_rows.append(doc_features)\n",
    "\n",
    "feat_df = pd.DataFrame(feature_rows)\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Computed feature columns:\")\n",
    "print(list(feat_df.columns))\n",
    "df_with_features.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01c5cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched dataset: raid_sample_small_with_features_PREPOS.csv  (rows: 3000)\n",
      "Feature columns saved (42 total):\n",
      "['type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'compression_ratio', 'uppercase_ratio', 'whitespace_ratio', 'unique_char_count', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'hapax_legomena_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.584479</td>\n",
       "      <td>0.145419</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.445572</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.747368</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopword_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.439500</td>\n",
       "      <td>0.170037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178825</td>\n",
       "      <td>0.474031</td>\n",
       "      <td>0.610283</td>\n",
       "      <td>0.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.029414</td>\n",
       "      <td>0.011826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016418</td>\n",
       "      <td>0.028056</td>\n",
       "      <td>0.044707</td>\n",
       "      <td>0.100962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>22.980522</td>\n",
       "      <td>21.406023</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>19.538462</td>\n",
       "      <td>30.620513</td>\n",
       "      <td>356.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_length_std</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>10.242729</td>\n",
       "      <td>10.866627</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.049981</td>\n",
       "      <td>7.789765</td>\n",
       "      <td>16.690021</td>\n",
       "      <td>173.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>53.267720</td>\n",
       "      <td>30.376332</td>\n",
       "      <td>-268.334775</td>\n",
       "      <td>20.280795</td>\n",
       "      <td>58.412607</td>\n",
       "      <td>82.008107</td>\n",
       "      <td>110.639455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>14.388530</td>\n",
       "      <td>9.321022</td>\n",
       "      <td>2.542922</td>\n",
       "      <td>7.591747</td>\n",
       "      <td>12.949603</td>\n",
       "      <td>20.922971</td>\n",
       "      <td>148.242697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>12.103777</td>\n",
       "      <td>4.356894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.447530</td>\n",
       "      <td>11.690004</td>\n",
       "      <td>17.122413</td>\n",
       "      <td>66.486521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>12.898040</td>\n",
       "      <td>11.296721</td>\n",
       "      <td>-1.387872</td>\n",
       "      <td>5.709764</td>\n",
       "      <td>11.077639</td>\n",
       "      <td>19.216023</td>\n",
       "      <td>175.471105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_diversity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.607818</td>\n",
       "      <td>0.147812</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.469693</td>\n",
       "      <td>0.601504</td>\n",
       "      <td>0.775775</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_diversity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.901888</td>\n",
       "      <td>0.137606</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.834251</td>\n",
       "      <td>0.932267</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_diversity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.953813</td>\n",
       "      <td>0.132801</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.930390</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_entropy</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>7.448463</td>\n",
       "      <td>0.946466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.517439</td>\n",
       "      <td>7.537923</td>\n",
       "      <td>8.407882</td>\n",
       "      <td>12.512292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_entropy</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>7.567761</td>\n",
       "      <td>0.961578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.623660</td>\n",
       "      <td>7.678888</td>\n",
       "      <td>8.538942</td>\n",
       "      <td>12.725889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_burstiness</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>-0.031803</td>\n",
       "      <td>0.222421</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.250039</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.190335</td>\n",
       "      <td>0.635500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_diversity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.522612</td>\n",
       "      <td>0.125451</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.391105</td>\n",
       "      <td>0.526049</td>\n",
       "      <td>0.662993</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_entropy</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>8.867496</td>\n",
       "      <td>0.652699</td>\n",
       "      <td>3.385698</td>\n",
       "      <td>8.282219</td>\n",
       "      <td>8.935230</td>\n",
       "      <td>9.529029</td>\n",
       "      <td>10.582828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.516534</td>\n",
       "      <td>0.097049</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>0.440681</td>\n",
       "      <td>0.520331</td>\n",
       "      <td>0.597724</td>\n",
       "      <td>1.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.026421</td>\n",
       "      <td>0.015223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010825</td>\n",
       "      <td>0.023677</td>\n",
       "      <td>0.045295</td>\n",
       "      <td>0.221140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.170044</td>\n",
       "      <td>0.017904</td>\n",
       "      <td>0.090237</td>\n",
       "      <td>0.145891</td>\n",
       "      <td>0.170740</td>\n",
       "      <td>0.191247</td>\n",
       "      <td>0.276190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_char_count</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>46.659000</td>\n",
       "      <td>10.326903</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>105.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_tree_depth</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>6.996630</td>\n",
       "      <td>4.412224</td>\n",
       "      <td>2.651163</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>6.597297</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tree_depth</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>10.939333</td>\n",
       "      <td>6.232383</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>2.763527</td>\n",
       "      <td>0.743597</td>\n",
       "      <td>1.008197</td>\n",
       "      <td>2.192045</td>\n",
       "      <td>2.608843</td>\n",
       "      <td>3.396813</td>\n",
       "      <td>17.703030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left_dependency_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.519958</td>\n",
       "      <td>0.088203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.430657</td>\n",
       "      <td>0.506697</td>\n",
       "      <td>0.640264</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_dependency_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.480042</td>\n",
       "      <td>0.088203</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.359736</td>\n",
       "      <td>0.493303</td>\n",
       "      <td>0.569343</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_legomena_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.460123</td>\n",
       "      <td>0.167312</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297521</td>\n",
       "      <td>0.442002</td>\n",
       "      <td>0.648882</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yules_k</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>140.469612</td>\n",
       "      <td>259.961608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.139592</td>\n",
       "      <td>109.757724</td>\n",
       "      <td>189.850586</td>\n",
       "      <td>9918.699187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtld</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>195.026807</td>\n",
       "      <td>1481.737435</td>\n",
       "      <td>1.004202</td>\n",
       "      <td>1.187775</td>\n",
       "      <td>1.719601</td>\n",
       "      <td>139.017647</td>\n",
       "      <td>34890.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comma_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.045777</td>\n",
       "      <td>0.029951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.043070</td>\n",
       "      <td>0.078217</td>\n",
       "      <td>0.384091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.042498</td>\n",
       "      <td>0.019876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021970</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.063830</td>\n",
       "      <td>0.265537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>0.060403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.068627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.006206</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004223</td>\n",
       "      <td>0.199324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.064343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.006459</td>\n",
       "      <td>0.012018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023262</td>\n",
       "      <td>0.100559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.095400</td>\n",
       "      <td>0.134386</td>\n",
       "      <td>-0.937500</td>\n",
       "      <td>-0.036080</td>\n",
       "      <td>0.085773</td>\n",
       "      <td>0.252790</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.421622</td>\n",
       "      <td>0.175737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183788</td>\n",
       "      <td>0.449665</td>\n",
       "      <td>0.601106</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.040499</td>\n",
       "      <td>0.038801</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>0.090219</td>\n",
       "      <td>0.298884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.554458</td>\n",
       "      <td>0.247745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.034409</td>\n",
       "      <td>0.025272</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031532</td>\n",
       "      <td>0.065217</td>\n",
       "      <td>0.222826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>0.016379</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.036232</td>\n",
       "      <td>0.297468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              count        mean          std         min  \\\n",
       "type_token_ratio             3000.0    0.584479     0.145419    0.004184   \n",
       "stopword_ratio               3000.0    0.439500     0.170037    0.000000   \n",
       "punctuation_ratio            3000.0    0.029414     0.011826    0.000000   \n",
       "avg_sentence_length          3000.0   22.980522    21.406023    2.333333   \n",
       "sentence_length_std          3000.0   10.242729    10.866627    0.000000   \n",
       "flesch_reading_ease          3000.0   53.267720    30.376332 -268.334775   \n",
       "gunning_fog                  3000.0   14.388530     9.321022    2.542922   \n",
       "smog_index                   3000.0   12.103777     4.356894    0.000000   \n",
       "automated_readability_index  3000.0   12.898040    11.296721   -1.387872   \n",
       "unigram_diversity            3000.0    0.607818     0.147812    0.004184   \n",
       "bigram_diversity             3000.0    0.901888     0.137606    0.008197   \n",
       "trigram_diversity            3000.0    0.953813     0.132801    0.008264   \n",
       "bigram_entropy               3000.0    7.448463     0.946466    0.000000   \n",
       "trigram_entropy              3000.0    7.567761     0.961578    0.000000   \n",
       "token_burstiness             3000.0   -0.031803     0.222421   -1.000000   \n",
       "char_trigram_diversity       3000.0    0.522612     0.125451    0.005935   \n",
       "char_trigram_entropy         3000.0    8.867496     0.652699    3.385698   \n",
       "compression_ratio            3000.0    0.516534     0.097049    0.031805   \n",
       "uppercase_ratio              3000.0    0.026421     0.015223    0.000000   \n",
       "whitespace_ratio             3000.0    0.170044     0.017904    0.090237   \n",
       "unique_char_count            3000.0   46.659000    10.326903    8.000000   \n",
       "avg_tree_depth               3000.0    6.996630     4.412224    2.651163   \n",
       "max_tree_depth               3000.0   10.939333     6.232383    4.000000   \n",
       "avg_dependency_distance      3000.0    2.763527     0.743597    1.008197   \n",
       "left_dependency_ratio        3000.0    0.519958     0.088203    0.000000   \n",
       "right_dependency_ratio       3000.0    0.480042     0.088203    0.000000   \n",
       "hapax_legomena_ratio         3000.0    0.460123     0.167312    0.000000   \n",
       "yules_k                      3000.0  140.469612   259.961608    0.000000   \n",
       "mtld                         3000.0  195.026807  1481.737435    1.004202   \n",
       "comma_ratio                  3000.0    0.045777     0.029951    0.000000   \n",
       "period_ratio                 3000.0    0.042498     0.019876    0.000000   \n",
       "question_ratio               3000.0    0.001536     0.004905    0.000000   \n",
       "exclamation_ratio            3000.0    0.001038     0.004002    0.000000   \n",
       "semicolon_ratio              3000.0    0.001480     0.006206    0.000000   \n",
       "colon_ratio                  3000.0    0.001770     0.003977    0.000000   \n",
       "quote_ratio                  3000.0    0.006459     0.012018    0.000000   \n",
       "sentiment_polarity           3000.0    0.095400     0.134386   -0.937500   \n",
       "sentiment_subjectivity       3000.0    0.421622     0.175737    0.000000   \n",
       "sentiment_polarity_variance  3000.0    0.040499     0.038801    0.000000   \n",
       "neutral_sentence_ratio       3000.0    0.554458     0.247745    0.000000   \n",
       "positive_word_ratio          3000.0    0.034409     0.025272    0.000000   \n",
       "negative_word_ratio          3000.0    0.016379     0.016394    0.000000   \n",
       "\n",
       "                                   10%         50%         90%           max  \n",
       "type_token_ratio              0.445572    0.578947    0.747368      1.000000  \n",
       "stopword_ratio                0.178825    0.474031    0.610283      0.898438  \n",
       "punctuation_ratio             0.016418    0.028056    0.044707      0.100962  \n",
       "avg_sentence_length          12.500000   19.538462   30.620513    356.000000  \n",
       "sentence_length_std           4.049981    7.789765   16.690021    173.500000  \n",
       "flesch_reading_ease          20.280795   58.412607   82.008107    110.639455  \n",
       "gunning_fog                   7.591747   12.949603   20.922971    148.242697  \n",
       "smog_index                    7.447530   11.690004   17.122413     66.486521  \n",
       "automated_readability_index   5.709764   11.077639   19.216023    175.471105  \n",
       "unigram_diversity             0.469693    0.601504    0.775775      1.000000  \n",
       "bigram_diversity              0.834251    0.932267    0.988506      1.000000  \n",
       "trigram_diversity             0.930390    0.987500    1.000000      1.000000  \n",
       "bigram_entropy                6.517439    7.537923    8.407882     12.512292  \n",
       "trigram_entropy               6.623660    7.678888    8.538942     12.725889  \n",
       "token_burstiness             -0.250039    0.003325    0.190335      0.635500  \n",
       "char_trigram_diversity        0.391105    0.526049    0.662993      1.000000  \n",
       "char_trigram_entropy          8.282219    8.935230    9.529029     10.582828  \n",
       "compression_ratio             0.440681    0.520331    0.597724      1.555556  \n",
       "uppercase_ratio               0.010825    0.023677    0.045295      0.221140  \n",
       "whitespace_ratio              0.145891    0.170740    0.191247      0.276190  \n",
       "unique_char_count            34.000000   46.000000   60.000000    105.000000  \n",
       "avg_tree_depth                4.857143    6.597297    8.800000    122.000000  \n",
       "max_tree_depth                7.000000   10.000000   14.000000    166.000000  \n",
       "avg_dependency_distance       2.192045    2.608843    3.396813     17.703030  \n",
       "left_dependency_ratio         0.430657    0.506697    0.640264      1.000000  \n",
       "right_dependency_ratio        0.359736    0.493303    0.569343      1.000000  \n",
       "hapax_legomena_ratio          0.297521    0.442002    0.648882      1.000000  \n",
       "yules_k                      58.139592  109.757724  189.850586   9918.699187  \n",
       "mtld                          1.187775    1.719601  139.017647  34890.520000  \n",
       "comma_ratio                   0.010526    0.043070    0.078217      0.384091  \n",
       "period_ratio                  0.021970    0.040816    0.063830      0.265537  \n",
       "question_ratio                0.000000    0.000000    0.005656      0.060403  \n",
       "exclamation_ratio             0.000000    0.000000    0.003186      0.068627  \n",
       "semicolon_ratio               0.000000    0.000000    0.004223      0.199324  \n",
       "colon_ratio                   0.000000    0.000000    0.006452      0.064343  \n",
       "quote_ratio                   0.000000    0.000000    0.023262      0.100559  \n",
       "sentiment_polarity           -0.036080    0.085773    0.252790      1.000000  \n",
       "sentiment_subjectivity        0.183788    0.449665    0.601106      1.000000  \n",
       "sentiment_polarity_variance   0.000107    0.030687    0.090219      0.298884  \n",
       "neutral_sentence_ratio        0.250000    0.545455    1.000000      1.000000  \n",
       "positive_word_ratio           0.000000    0.031532    0.065217      0.222826  \n",
       "negative_word_ratio           0.000000    0.013699    0.036232      0.297468  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Save enriched dataset and basic descriptive statistics\n",
    "\n",
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_PREPOS.csv\"\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS}  (rows: {len(df_with_features)})\")\n",
    "\n",
    "feature_cols = [col for col in df_with_features.columns if col not in df.columns]\n",
    "print(f\"Feature columns saved ({len(feature_cols)} total):\")\n",
    "print(feature_cols)\n",
    "\n",
    "display(df_with_features[feature_cols].describe(percentiles=[0.1, 0.5, 0.9]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "125c533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b190a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length min: 2.3333333333333335 max: 356.0\n",
      "flesch_reading_ease min: -268.33477528089884 max: 110.63945543603536\n",
      "gunning_fog min: 2.542922374429224 max: 148.2426966292135\n",
      "automated_readability_index min: -1.3878716865155596 max: 175.471104815864\n",
      "mtld min: 1.004201680672269 max: 34890.519999999386\n",
      "yules_k min: 0.0 max: 9918.69918699187\n",
      "max_tree_depth min: 4.0 max: 166.0\n"
     ]
    }
   ],
   "source": [
    "for col in [\"avg_sentence_length\",\"flesch_reading_ease\",\"gunning_fog\",\n",
    "            \"automated_readability_index\",\"mtld\",\"yules_k\",\"max_tree_depth\"]:\n",
    "    if col in df.columns:\n",
    "        print(col, \"min:\", df[col].min(), \"max:\", df[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b72a3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed thresholds:\n",
      "  asl_hi: 105.66999999999993\n",
      "  sls_hi: 80.0\n",
      "  fog_hi: 69.58179761904763\n",
      "  ari_hi: 87.17153812500003\n",
      "  fre_lo: -88.94751031717743\n",
      "  mtld_hi: 9246.999300000036\n",
      "  yk_hi: 1515.6763993614138\n",
      "  depth_max_hi: 50.0\n",
      "  depth_avg_hi: 25.0\n",
      "  depdist_hi: 8.0\n",
      "  comp_hi: 1.05\n",
      "  upper_hi: 0.15\n",
      "  uniq_hi: 80.0\n",
      "  ws_lo: 0.12\n",
      "  ws_hi: 0.25\n",
      "\n",
      "Cause counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lexical_metric_instability_or_length_effects</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependency_depth_computation_bug</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_segmentation_failure</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_or_code_noise</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              count\n",
       "cause                                              \n",
       "lexical_metric_instability_or_length_effects     30\n",
       "dependency_depth_computation_bug                 18\n",
       "sentence_segmentation_failure                    18\n",
       "markup_or_code_noise                             16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flag counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_instability</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_outlier</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth_implausible</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_noise</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_overhead</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count\n",
       "severity                    121\n",
       "seg_len_extreme              38\n",
       "lexical_instability          30\n",
       "readability_outlier          18\n",
       "depth_implausible            12\n",
       "markup_noise                 10\n",
       "compression_overhead          6\n",
       "dep_distance_implausible      6\n",
       "ngram_edge_effects            1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top offenders (with original feature values):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <th>readability_outlier</th>\n",
       "      <th>lexical_instability</th>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <th>depth_implausible</th>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <th>compression_overhead</th>\n",
       "      <th>markup_noise</th>\n",
       "      <th>suspected_causes</th>\n",
       "      <th>severity</th>\n",
       "      <th>...</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>unique_char_count</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>bigram_entropy</th>\n",
       "      <th>trigram_entropy</th>\n",
       "      <th>bigram_diversity</th>\n",
       "      <th>trigram_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2131</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.008197</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.090237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1.804035</td>\n",
       "      <td>0.049206</td>\n",
       "      <td>0.057937</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>2.387079</td>\n",
       "      <td>2.387206</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.023055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.238693</td>\n",
       "      <td>0.497942</td>\n",
       "      <td>0.009053</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.109739</td>\n",
       "      <td>8.647458</td>\n",
       "      <td>8.643856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>3.480000</td>\n",
       "      <td>0.033784</td>\n",
       "      <td>0.050369</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.199631</td>\n",
       "      <td>1.999980</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.012346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.874525</td>\n",
       "      <td>0.057904</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.241728</td>\n",
       "      <td>2.999926</td>\n",
       "      <td>2.999873</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.030534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1381</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.292994</td>\n",
       "      <td>0.133734</td>\n",
       "      <td>0.000751</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.157025</td>\n",
       "      <td>3.659705</td>\n",
       "      <td>4.227868</td>\n",
       "      <td>0.216561</td>\n",
       "      <td>0.269231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1.890196</td>\n",
       "      <td>0.064407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.216102</td>\n",
       "      <td>3.321651</td>\n",
       "      <td>3.321660</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.039370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.874317</td>\n",
       "      <td>0.165308</td>\n",
       "      <td>0.005930</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.167532</td>\n",
       "      <td>4.316971</td>\n",
       "      <td>5.107096</td>\n",
       "      <td>0.273224</td>\n",
       "      <td>0.346154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>17.0</td>\n",
       "      <td>12.525568</td>\n",
       "      <td>0.058041</td>\n",
       "      <td>0.041124</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.183238</td>\n",
       "      <td>3.612247</td>\n",
       "      <td>3.829640</td>\n",
       "      <td>0.039773</td>\n",
       "      <td>0.048433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.422535</td>\n",
       "      <td>0.084525</td>\n",
       "      <td>0.031697</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.220634</td>\n",
       "      <td>4.226473</td>\n",
       "      <td>4.841327</td>\n",
       "      <td>0.095775</td>\n",
       "      <td>0.129944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1858</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.242038</td>\n",
       "      <td>0.251867</td>\n",
       "      <td>0.142430</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.159861</td>\n",
       "      <td>4.683959</td>\n",
       "      <td>4.828932</td>\n",
       "      <td>0.408228</td>\n",
       "      <td>0.438095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2123</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>4.023952</td>\n",
       "      <td>0.145778</td>\n",
       "      <td>0.007916</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.147098</td>\n",
       "      <td>4.246247</td>\n",
       "      <td>5.092543</td>\n",
       "      <td>0.257485</td>\n",
       "      <td>0.337349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2679</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure, lexical_metric...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.156977</td>\n",
       "      <td>0.151028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.150386</td>\n",
       "      <td>4.188403</td>\n",
       "      <td>4.858197</td>\n",
       "      <td>0.273256</td>\n",
       "      <td>0.321637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>68.0</td>\n",
       "      <td>3.617021</td>\n",
       "      <td>0.117944</td>\n",
       "      <td>0.002016</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.210685</td>\n",
       "      <td>2.554693</td>\n",
       "      <td>2.603711</td>\n",
       "      <td>0.113475</td>\n",
       "      <td>0.121429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2.332386</td>\n",
       "      <td>0.160082</td>\n",
       "      <td>0.040872</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.239782</td>\n",
       "      <td>5.009381</td>\n",
       "      <td>5.763516</td>\n",
       "      <td>0.247159</td>\n",
       "      <td>0.327635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure]</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>4.470588</td>\n",
       "      <td>0.528388</td>\n",
       "      <td>0.031657</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.189944</td>\n",
       "      <td>7.487257</td>\n",
       "      <td>7.565510</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.955665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1618</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, markup_or_c...</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>166.0</td>\n",
       "      <td>2.266160</td>\n",
       "      <td>0.475276</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.108296</td>\n",
       "      <td>8.040245</td>\n",
       "      <td>8.049849</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1629</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug]</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>19.0</td>\n",
       "      <td>8.401003</td>\n",
       "      <td>0.507967</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.130732</td>\n",
       "      <td>8.651052</td>\n",
       "      <td>8.647458</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[sentence_segmentation_failure]</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>3.441767</td>\n",
       "      <td>0.501109</td>\n",
       "      <td>0.019956</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.184035</td>\n",
       "      <td>7.702817</td>\n",
       "      <td>7.910830</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.979839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug]</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>11.0</td>\n",
       "      <td>8.489051</td>\n",
       "      <td>0.591644</td>\n",
       "      <td>0.035040</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.184636</td>\n",
       "      <td>7.004931</td>\n",
       "      <td>7.072757</td>\n",
       "      <td>0.956204</td>\n",
       "      <td>0.992647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      seg_len_extreme  readability_outlier  lexical_instability  \\\n",
       "2131             True                 True                 True   \n",
       "1731             True                 True                 True   \n",
       "369              True                 True                 True   \n",
       "2007             True                 True                 True   \n",
       "2363             True                 True                 True   \n",
       "1381             True                 True                 True   \n",
       "225              True                 True                False   \n",
       "1462             True                 True                 True   \n",
       "1105             True                 True                False   \n",
       "1746             True                 True                False   \n",
       "1858             True                False                 True   \n",
       "2123             True                 True                 True   \n",
       "2679             True                 True                 True   \n",
       "2389             True                False                 True   \n",
       "2695             True                 True                False   \n",
       "84               True                 True                False   \n",
       "1618            False                False                False   \n",
       "1629             True                False                False   \n",
       "272              True                 True                False   \n",
       "1300             True                False                False   \n",
       "\n",
       "      ngram_edge_effects  depth_implausible  dep_distance_implausible  \\\n",
       "2131                True               True                     False   \n",
       "1731               False               True                     False   \n",
       "369                False              False                     False   \n",
       "2007               False               True                     False   \n",
       "2363               False               True                     False   \n",
       "1381               False              False                     False   \n",
       "225                False               True                     False   \n",
       "1462               False              False                     False   \n",
       "1105               False              False                      True   \n",
       "1746               False              False                      True   \n",
       "1858               False               True                     False   \n",
       "2123               False              False                     False   \n",
       "2679               False              False                     False   \n",
       "2389               False               True                     False   \n",
       "2695               False               True                     False   \n",
       "84                 False              False                     False   \n",
       "1618               False               True                     False   \n",
       "1629               False              False                      True   \n",
       "272                False              False                     False   \n",
       "1300               False              False                      True   \n",
       "\n",
       "      compression_overhead  markup_noise  \\\n",
       "2131                 False          True   \n",
       "1731                 False          True   \n",
       "369                  False          True   \n",
       "2007                 False         False   \n",
       "2363                 False         False   \n",
       "1381                 False         False   \n",
       "225                  False         False   \n",
       "1462                 False         False   \n",
       "1105                 False         False   \n",
       "1746                 False         False   \n",
       "1858                 False         False   \n",
       "2123                 False         False   \n",
       "2679                 False         False   \n",
       "2389                 False         False   \n",
       "2695                 False         False   \n",
       "84                   False         False   \n",
       "1618                 False          True   \n",
       "1629                 False         False   \n",
       "272                  False         False   \n",
       "1300                 False         False   \n",
       "\n",
       "                                       suspected_causes  severity  ...  \\\n",
       "2131  [dependency_depth_computation_bug, sentence_se...         6  ...   \n",
       "1731  [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "369   [sentence_segmentation_failure, lexical_metric...         4  ...   \n",
       "2007  [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "2363  [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "1381  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "225   [dependency_depth_computation_bug, sentence_se...         3  ...   \n",
       "1462  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "1105  [dependency_depth_computation_bug, sentence_se...         3  ...   \n",
       "1746  [dependency_depth_computation_bug, sentence_se...         3  ...   \n",
       "1858  [dependency_depth_computation_bug, lexical_met...         3  ...   \n",
       "2123  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "2679  [sentence_segmentation_failure, lexical_metric...         3  ...   \n",
       "2389  [dependency_depth_computation_bug, lexical_met...         3  ...   \n",
       "2695  [dependency_depth_computation_bug, sentence_se...         3  ...   \n",
       "84                      [sentence_segmentation_failure]         2  ...   \n",
       "1618  [dependency_depth_computation_bug, markup_or_c...         2  ...   \n",
       "1629                 [dependency_depth_computation_bug]         2  ...   \n",
       "272                     [sentence_segmentation_failure]         2  ...   \n",
       "1300                 [dependency_depth_computation_bug]         2  ...   \n",
       "\n",
       "      max_tree_depth  avg_dependency_distance  compression_ratio  \\\n",
       "2131           122.0                 1.008197           0.031805   \n",
       "1731           139.0                 1.804035           0.049206   \n",
       "369             16.0                 4.238693           0.497942   \n",
       "2007            83.0                 3.480000           0.033784   \n",
       "2363           100.0                 1.874525           0.057904   \n",
       "1381            13.0                 6.292994           0.133734   \n",
       "225             79.0                 1.890196           0.064407   \n",
       "1462            12.0                 7.874317           0.165308   \n",
       "1105            17.0                12.525568           0.058041   \n",
       "1746             8.0                14.422535           0.084525   \n",
       "1858            94.0                 2.242038           0.251867   \n",
       "2123            18.0                 4.023952           0.145778   \n",
       "2679            19.0                 4.156977           0.151028   \n",
       "2389            68.0                 3.617021           0.117944   \n",
       "2695            60.0                 2.332386           0.160082   \n",
       "84              15.0                 4.470588           0.528388   \n",
       "1618           166.0                 2.266160           0.475276   \n",
       "1629            19.0                 8.401003           0.507967   \n",
       "272             14.0                 3.441767           0.501109   \n",
       "1300            11.0                 8.489051           0.591644   \n",
       "\n",
       "      uppercase_ratio  unique_char_count  whitespace_ratio  bigram_entropy  \\\n",
       "2131         0.090976               10.0          0.090237        0.000000   \n",
       "1731         0.057937               15.0          0.276190        2.387079   \n",
       "369          0.009053               42.0          0.109739        8.647458   \n",
       "2007         0.050369               13.0          0.199631        1.999980   \n",
       "2363         0.000000               14.0          0.241728        2.999926   \n",
       "1381         0.000751               29.0          0.157025        3.659705   \n",
       "225          0.000000               19.0          0.216102        3.321651   \n",
       "1462         0.005930               35.0          0.167532        4.316971   \n",
       "1105         0.041124               24.0          0.183238        3.612247   \n",
       "1746         0.031697               21.0          0.220634        4.226473   \n",
       "1858         0.142430               54.0          0.159861        4.683959   \n",
       "2123         0.007916               36.0          0.147098        4.246247   \n",
       "2679         0.000000               33.0          0.150386        4.188403   \n",
       "2389         0.002016               27.0          0.210685        2.554693   \n",
       "2695         0.040872               27.0          0.239782        5.009381   \n",
       "84           0.031657               40.0          0.189944        7.487257   \n",
       "1618         0.006947               40.0          0.108296        8.040245   \n",
       "1629         0.004878               38.0          0.130732        8.651052   \n",
       "272          0.019956               37.0          0.184035        7.702817   \n",
       "1300         0.035040               37.0          0.184636        7.004931   \n",
       "\n",
       "      trigram_entropy  bigram_diversity  trigram_diversity  \n",
       "2131         0.000000          0.008197           0.008264  \n",
       "1731         2.387206          0.022989           0.023055  \n",
       "369          8.643856          1.000000           1.000000  \n",
       "2007         2.000000          0.012308           0.012346  \n",
       "2363         2.999873          0.030418           0.030534  \n",
       "1381         4.227868          0.216561           0.269231  \n",
       "225          3.321660          0.039216           0.039370  \n",
       "1462         5.107096          0.273224           0.346154  \n",
       "1105         3.829640          0.039773           0.048433  \n",
       "1746         4.841327          0.095775           0.129944  \n",
       "1858         4.828932          0.408228           0.438095  \n",
       "2123         5.092543          0.257485           0.337349  \n",
       "2679         4.858197          0.273256           0.321637  \n",
       "2389         2.603711          0.113475           0.121429  \n",
       "2695         5.763516          0.247159           0.327635  \n",
       "84           7.565510          0.916667           0.955665  \n",
       "1618         8.049849          0.992481           1.000000  \n",
       "1629         8.647458          1.000000           1.000000  \n",
       "272          7.910830          0.903614           0.979839  \n",
       "1300         7.072757          0.956204           0.992647  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"bigram_entropy\",\"trigram_entropy\",\"bigram_diversity\",\"trigram_diversity\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\"whitespace_ratio\"\n",
    "]\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols=NUMERIC_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Coerce known feature columns to numeric (if present).\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _q(df, col, q):\n",
    "    return float(np.nanquantile(df[col].values, q)) if col in df else np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# Thresholds and Diagnostics\n",
    "# -------------------------------\n",
    "\n",
    "def compute_diagnostic_thresholds(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Data-driven thresholds (quantile-based) + a few hard guards.\"\"\"\n",
    "    thr = {}\n",
    "    thr[\"asl_hi\"]       = max(80.0, _q(df, \"avg_sentence_length\", 0.99))\n",
    "    thr[\"sls_hi\"]       = max(80.0, _q(df, \"sentence_length_std\", 0.99))\n",
    "    thr[\"fog_hi\"]       = max(60.0, _q(df, \"gunning_fog\", 0.995))\n",
    "    thr[\"ari_hi\"]       = max(60.0, _q(df, \"automated_readability_index\", 0.995))\n",
    "    thr[\"fre_lo\"]       = min(-50.0, _q(df, \"flesch_reading_ease\", 0.005))\n",
    "    thr[\"mtld_hi\"]      = max(500.0, _q(df, \"mtld\", 0.995))\n",
    "    thr[\"yk_hi\"]        = max(1000.0, _q(df, \"yules_k\", 0.995))\n",
    "    thr[\"depth_max_hi\"] = max(50.0, _q(df, \"max_tree_depth\", 0.995))\n",
    "    thr[\"depth_avg_hi\"] = max(25.0, _q(df, \"avg_tree_depth\", 0.995))\n",
    "    thr[\"depdist_hi\"]   = max(8.0, _q(df, \"avg_dependency_distance\", 0.995))\n",
    "    thr[\"comp_hi\"]      = 1.05\n",
    "    thr[\"upper_hi\"]     = max(0.15, _q(df, \"uppercase_ratio\", 0.995))\n",
    "    thr[\"uniq_hi\"]      = max(80.0, _q(df, \"unique_char_count\", 0.995))\n",
    "    thr[\"ws_lo\"]        = min(0.12, _q(df, \"whitespace_ratio\", 0.005))\n",
    "    thr[\"ws_hi\"]        = max(0.25, _q(df, \"whitespace_ratio\", 0.995))\n",
    "    return thr\n",
    "\n",
    "def diagnose_feature_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return diagnostics dataframe with boolean flags and suspected causes list.\"\"\"\n",
    "    # Ensure numeric so comparisons fire correctly\n",
    "    df = ensure_numeric(df)\n",
    "\n",
    "    thr = compute_diagnostic_thresholds(df)\n",
    "    D = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Flag definitions\n",
    "    D[\"seg_len_extreme\"] = (\n",
    "        (df.get(\"avg_sentence_length\", np.nan) > thr[\"asl_hi\"]) |\n",
    "        (df.get(\"sentence_length_std\", np.nan) > thr[\"sls_hi\"])\n",
    "    )\n",
    "    D[\"readability_outlier\"] = (\n",
    "        (df.get(\"flesch_reading_ease\", np.nan) < thr[\"fre_lo\"]) |\n",
    "        (df.get(\"gunning_fog\", np.nan) > thr[\"fog_hi\"]) |\n",
    "        (df.get(\"automated_readability_index\", np.nan) > thr[\"ari_hi\"])\n",
    "    )\n",
    "    D[\"lexical_instability\"] = (\n",
    "        (df.get(\"mtld\", np.nan) > thr[\"mtld_hi\"]) |\n",
    "        (df.get(\"yules_k\", np.nan) > thr[\"yk_hi\"])\n",
    "    )\n",
    "    D[\"ngram_edge_effects\"] = (\n",
    "        (df.get(\"bigram_entropy\", np.nan) == 0) |\n",
    "        (df.get(\"trigram_entropy\", np.nan) == 0) |\n",
    "        ((df.get(\"trigram_diversity\", np.nan) >= 0.99) & (df.get(\"trigram_entropy\", np.nan) < 1.0)) |\n",
    "        ((df.get(\"bigram_diversity\", np.nan)   >= 0.99) & (df.get(\"bigram_entropy\", np.nan)   < 1.0))\n",
    "    )\n",
    "    D[\"depth_implausible\"] = (\n",
    "        (df.get(\"max_tree_depth\", np.nan) > thr[\"depth_max_hi\"]) |\n",
    "        (df.get(\"avg_tree_depth\", np.nan) > thr[\"depth_avg_hi\"])\n",
    "    )\n",
    "    D[\"dep_distance_implausible\"] = (df.get(\"avg_dependency_distance\", np.nan) > thr[\"depdist_hi\"])\n",
    "    D[\"compression_overhead\"] = (df.get(\"compression_ratio\", np.nan) > thr[\"comp_hi\"])\n",
    "    D[\"markup_noise\"] = (\n",
    "        ((df.get(\"uppercase_ratio\", np.nan) > thr[\"upper_hi\"]) & (df.get(\"unique_char_count\", np.nan) > thr[\"uniq_hi\"])) |\n",
    "        (df.get(\"whitespace_ratio\", np.nan) < thr[\"ws_lo\"]) |\n",
    "        (df.get(\"whitespace_ratio\", np.nan) > thr[\"ws_hi\"])\n",
    "    )\n",
    "\n",
    "    # Suspected causes column (pre-allocate + .at)\n",
    "    D[\"suspected_causes\"] = pd.Series([[] for _ in range(len(D))], index=D.index, dtype=object)\n",
    "    for i in D.index:\n",
    "        c = []\n",
    "        if bool(D.at[i, \"depth_implausible\"]) or bool(D.at[i, \"dep_distance_implausible\"]):\n",
    "            c.append(\"dependency_depth_computation_bug\")\n",
    "        if bool(D.at[i, \"seg_len_extreme\"]) and bool(D.at[i, \"readability_outlier\"]):\n",
    "            c.append(\"sentence_segmentation_failure\")\n",
    "        if bool(D.at[i, \"lexical_instability\"]) or bool(D.at[i, \"ngram_edge_effects\"]):\n",
    "            c.append(\"lexical_metric_instability_or_length_effects\")\n",
    "        if bool(D.at[i, \"compression_overhead\"]) or bool(D.at[i, \"markup_noise\"]):\n",
    "            c.append(\"markup_or_code_noise\")\n",
    "        D.at[i, \"suspected_causes\"] = c\n",
    "\n",
    "    # Severity score\n",
    "    flag_cols = [c for c in D.columns if c != \"suspected_causes\"]\n",
    "    for c in flag_cols:\n",
    "        D[c] = D[c].fillna(False).astype(bool)\n",
    "    D[\"severity\"] = D[flag_cols].sum(axis=1).astype(int)\n",
    "\n",
    "    # Attach thresholds for later inspection\n",
    "    D.attrs[\"thresholds\"] = thr\n",
    "    return D\n",
    "\n",
    "def build_diagnostic_report(diag: pd.DataFrame, top_k: int = 15):\n",
    "    \"\"\"Summarize counts by cause and return top offending rows by severity.\"\"\"\n",
    "    cause_counts = (\n",
    "        diag[\"suspected_causes\"].explode().value_counts(dropna=True)\n",
    "        .rename_axis(\"cause\").to_frame(\"count\")\n",
    "    )\n",
    "    flag_counts = (\n",
    "        diag.drop(columns=[\"suspected_causes\"])\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .to_frame(\"count\")\n",
    "    )\n",
    "    top_offenders = diag.sort_values(\"severity\", ascending=False).head(top_k)\n",
    "    return {\"cause_counts\": cause_counts, \"flag_counts\": flag_counts, \"top_offenders\": top_offenders}\n",
    "\n",
    "# -------------------------------\n",
    "# Runner / Example usage\n",
    "# -------------------------------\n",
    "\n",
    "# Assume you already have `df` with your features\n",
    "# If your features were just computed and may be strings, the coercion inside\n",
    "# diagnose_feature_outliers will handle them; coercing here is optional:\n",
    "# df = ensure_numeric(df)\n",
    "\n",
    "diag = diagnose_feature_outliers(df)\n",
    "rep  = build_diagnostic_report(diag, top_k=20)\n",
    "\n",
    "print(\"Computed thresholds:\")\n",
    "for k, v in diag.attrs.get(\"thresholds\", {}).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nCause counts:\")\n",
    "display(rep[\"cause_counts\"])\n",
    "\n",
    "print(\"\\nFlag counts:\")\n",
    "display(rep[\"flag_counts\"])\n",
    "\n",
    "print(\"\\nTop offenders (with original feature values):\")\n",
    "cols_to_show = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\n",
    "    \"whitespace_ratio\",\"bigram_entropy\",\"trigram_entropy\",\n",
    "    \"bigram_diversity\",\"trigram_diversity\"\n",
    "]\n",
    "existing_cols = [c for c in cols_to_show if c in df.columns]\n",
    "display(rep[\"top_offenders\"].join(df[existing_cols], how=\"left\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
