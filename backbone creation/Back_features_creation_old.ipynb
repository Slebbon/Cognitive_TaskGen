{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6aff351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded medium dataset with 12000 rows.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>digit_ratio</th>\n",
       "      <th>punct_ratio</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>std_word_length</th>\n",
       "      <th>entropy_bits</th>\n",
       "      <th>entropy_norm</th>\n",
       "      <th>is_text_like</th>\n",
       "      <th>not_text_reason</th>\n",
       "      <th>n_tokens_ws</th>\n",
       "      <th>length_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31dac42c-d620-48ef-9f4e-30a072594c4a</td>\n",
       "      <td>cd90a5fb-6aee-4a78-a32d-e34016718deb</td>\n",
       "      <td>0499bbab-4652-479a-8409-95f5ee077c0a</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>number</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Or From That Sea Of Time</td>\n",
       "      <td>The following is the full text of a poem title...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>1.610466</td>\n",
       "      <td>4.247145</td>\n",
       "      <td>0.828019</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49</td>\n",
       "      <td>short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec269ad5-b494-4269-aff4-9d2b4023db56</td>\n",
       "      <td>5890f7ff-7153-49ba-b4f0-11cb0fc502fa</td>\n",
       "      <td>3a482121-a841-4cc5-b071-058785c10525</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>upper_lower</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Petit Fours</td>\n",
       "      <td>Write a recipe for \"Petit Fours\".</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>0.024169</td>\n",
       "      <td>5.390572</td>\n",
       "      <td>2.193266</td>\n",
       "      <td>4.566332</td>\n",
       "      <td>0.750121</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>313</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86644bab-0ba0-4b90-bba6-344df0e58ea7</td>\n",
       "      <td>e6d6a10d-6a5b-4801-b160-68ec66db6f9c</td>\n",
       "      <td>e16d6eac-e79b-439d-8f6c-673dcebaecad</td>\n",
       "      <td>cohere-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>no</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Formal specification</td>\n",
       "      <td>Write the body of a Wikipedia article titled \"...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.015865</td>\n",
       "      <td>5.599291</td>\n",
       "      <td>3.701504</td>\n",
       "      <td>4.285282</td>\n",
       "      <td>0.763224</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>280</td>\n",
       "      <td>long</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  31dac42c-d620-48ef-9f4e-30a072594c4a  cd90a5fb-6aee-4a78-a32d-e34016718deb   \n",
       "1  ec269ad5-b494-4269-aff4-9d2b4023db56  5890f7ff-7153-49ba-b4f0-11cb0fc502fa   \n",
       "2  86644bab-0ba0-4b90-bba6-344df0e58ea7  e6d6a10d-6a5b-4801-b160-68ec66db6f9c   \n",
       "\n",
       "                              source_id        model  decoding  \\\n",
       "0  0499bbab-4652-479a-8409-95f5ee077c0a         gpt2  sampling   \n",
       "1  3a482121-a841-4cc5-b071-058785c10525     mpt-chat  sampling   \n",
       "2  e16d6eac-e79b-439d-8f6c-673dcebaecad  cohere-chat  sampling   \n",
       "\n",
       "  repetition_penalty            attack   domain                     title  \\\n",
       "0                yes            number   poetry  Or From That Sea Of Time   \n",
       "1                yes       upper_lower  recipes               Petit Fours   \n",
       "2                 no  zero_width_space     wiki      Formal specification   \n",
       "\n",
       "                                              prompt  ... digit_ratio  \\\n",
       "0  The following is the full text of a poem title...  ...    0.000000   \n",
       "1                  Write a recipe for \"Petit Fours\".  ...    0.012588   \n",
       "2  Write the body of a Wikipedia article titled \"...  ...    0.002115   \n",
       "\n",
       "   punct_ratio avg_word_length std_word_length  entropy_bits  entropy_norm  \\\n",
       "0     0.015625        4.080000        1.610466      4.247145      0.828019   \n",
       "1     0.024169        5.390572        2.193266      4.566332      0.750121   \n",
       "2     0.015865        5.599291        3.701504      4.285282      0.763224   \n",
       "\n",
       "   is_text_like  not_text_reason  n_tokens_ws  length_bin  \n",
       "0          True              NaN           49       short  \n",
       "1          True              NaN          313        long  \n",
       "2          True              NaN          280        long  \n",
       "\n",
       "[3 rows x 31 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Configuration and dataset selection\n",
    "\n",
    "import sys\n",
    "import math\n",
    "import re\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Paths to the three samples\n",
    "SAMPLE_PATHS = {\n",
    "    \"small\":  \"raid_sample_small.csv\",\n",
    "    \"medium\": \"raid_sample_medium.csv\",\n",
    "    \"large\":  \"raid_sample_large.csv\",\n",
    "}\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# Set this to one of: \"small\", \"medium\", \"large\"\n",
    "SELECTED_DATASET = \"medium\"\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "TEXT_COL = \"generation\"\n",
    "\n",
    "df = pd.read_csv(SAMPLE_PATHS[SELECTED_DATASET])\n",
    "print(f\"Loaded {SELECTED_DATASET} dataset with {len(df)} rows.\")\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b6afd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy, CMUdict (NLTK), and g2p_en fallback\n",
    "\n",
    "# Ensure CMUdict is available\n",
    "nltk.download('cmudict', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "CMU = cmudict.dict()  # key: lowercase word, value: list of pronunciations (list of ARPAbet tokens)\n",
    "\n",
    "# Load spaCy English model (use 'en_core_web_sm' unless you already have md/lg installed)\n",
    "try:\n",
    "    nlp: Language = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    from spacy.cli import download\n",
    "    download(\"en_core_web_sm\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Ensure sentence boundaries are available (parser usually handles this; add sentencizer if needed)\n",
    "if \"sentencizer\" not in nlp.pipe_names and \"parser\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# g2p_en for OOV coverage\n",
    "G2P = G2p()\n",
    "\n",
    "# ARPAbet vowel bases (stress digits removed when checking)\n",
    "ARPA_VOWELS = {\n",
    "    \"AA\", \"AE\", \"AH\", \"AO\", \"AW\", \"AY\",\n",
    "    \"EH\", \"ER\", \"EY\",\n",
    "    \"IH\", \"IY\",\n",
    "    \"OW\", \"OY\",\n",
    "    \"UH\", \"UW\"\n",
    "}\n",
    "\n",
    "# Cache syllable counts for speed\n",
    "_SYLL_CACHE: Dict[str, int] = {}\n",
    "\n",
    "def cmu_syllables(word: str) -> int | None:\n",
    "    \"\"\"\n",
    "    Returns syllable count using CMUdict if available; else None.\n",
    "    Policy: use the first pronunciation variant.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w not in CMU:\n",
    "        return None\n",
    "    phones = CMU[w][0]\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    return max(count, 1)  # at least one for non-empty alphabetic words\n",
    "\n",
    "def g2p_syllables(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns syllable count using neural g2p_en; counts vowel phonemes.\n",
    "    \"\"\"\n",
    "    w = word.lower()\n",
    "    if w in _SYLL_CACHE:\n",
    "        return _SYLL_CACHE[w]\n",
    "    phones = G2P(w)\n",
    "    count = 0\n",
    "    for ph in phones:\n",
    "        base = re.sub(r\"\\d\", \"\", ph)\n",
    "        if base in ARPA_VOWELS:\n",
    "            count += 1\n",
    "    # Guard: ensure >=1 for alphabetic tokens\n",
    "    if count == 0 and re.search(r\"[A-Za-z]\", w):\n",
    "        count = 1\n",
    "    _SYLL_CACHE[w] = count\n",
    "    return count\n",
    "\n",
    "def syllables_hybrid(word: str) -> int:\n",
    "    \"\"\"\n",
    "    Hybrid policy: try CMUdict first; if OOV, fall back to g2p_en.\n",
    "    \"\"\"\n",
    "    c = cmu_syllables(word)\n",
    "    if c is not None:\n",
    "        return c\n",
    "    return g2p_syllables(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3020ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature computation utilities using spaCy + CMUdict with g2p_en fallback\n",
    "\n",
    "def _word_like(tok) -> bool:\n",
    "    \"\"\"\n",
    "    Select lexical tokens (alphabetic, not space).\n",
    "    spaCy's tok.is_alpha ensures letter-only tokens; change if you want alphanumerics.\n",
    "    \"\"\"\n",
    "    return tok.is_alpha and not tok.is_space\n",
    "\n",
    "def _alnum_char_count(token_text: str) -> int:\n",
    "    \"\"\"Count alphanumeric characters for ARI; excludes whitespace and punctuation.\"\"\"\n",
    "    return sum(ch.isalnum() for ch in token_text)\n",
    "\n",
    "def features_from_doc(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Computes:\n",
    "      - avg_word_length\n",
    "      - type_token_ratio\n",
    "      - stopword_ratio\n",
    "      - punctuation_ratio       (punct chars / non-space chars)\n",
    "      - avg_sentence_length     (words per sentence)\n",
    "      - sentence_length_std     (std of sentence word counts)\n",
    "      - flesch_reading_ease\n",
    "      - gunning_fog\n",
    "      - smog_index\n",
    "      - automated_readability_index\n",
    "    \"\"\"\n",
    "    # Sentences\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    n_sents = max(len(sents), 1)\n",
    "\n",
    "    # Token groups\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    punct_toks = [t for t in doc if t.is_punct]\n",
    "    nonspace_toks = [t for t in doc if not t.is_space]\n",
    "\n",
    "    W = len(word_toks)\n",
    "\n",
    "    # Characters for ARI and punctuation ratio\n",
    "    chars_alnum = sum(_alnum_char_count(t.text) for t in nonspace_toks)\n",
    "    punct_chars = sum(len(t.text) for t in punct_toks)\n",
    "    nonspace_chars = sum(len(t.text) for t in nonspace_toks)\n",
    "\n",
    "    # Sentence-level word counts\n",
    "    sent_word_counts = [sum(1 for t in s if _word_like(t)) for s in sents]\n",
    "    avg_sentence_length = float(np.mean(sent_word_counts)) if sent_word_counts else 0.0\n",
    "    sentence_length_std  = float(np.std(sent_word_counts, ddof=0)) if len(sent_word_counts) > 1 else 0.0\n",
    "\n",
    "    # Word-level lengths\n",
    "    word_lengths = [len(t.text) for t in word_toks]\n",
    "    avg_word_length = float(np.mean(word_lengths)) if word_lengths else 0.0\n",
    "\n",
    "    # Type-token ratio (lowercased forms)\n",
    "    vocab = {t.text.lower() for t in word_toks}\n",
    "    type_token_ratio = (len(vocab) / W) if W > 0 else 0.0\n",
    "\n",
    "    # Stopword ratio via spaCy stop flags\n",
    "    stop_count = sum(1 for t in word_toks if t.is_stop)\n",
    "    stopword_ratio = (stop_count / W) if W > 0 else 0.0\n",
    "\n",
    "    # Punctuation ratio over non-space characters\n",
    "    punctuation_ratio = (punct_chars / nonspace_chars) if nonspace_chars > 0 else 0.0\n",
    "\n",
    "    # Syllables (hybrid)\n",
    "    syll_per_word = [syllables_hybrid(t.text) for t in word_toks] if W > 0 else []\n",
    "    syll_total = int(np.sum(syll_per_word)) if syll_per_word else 0\n",
    "    polysyllables = int(np.sum([syl >= 3 for syl in syll_per_word])) if syll_per_word else 0\n",
    "    complex_words = polysyllables  # standard: >= 3 syllables\n",
    "\n",
    "    # Rates for readability\n",
    "    words_per_sentence = (W / n_sents) if n_sents > 0 else 0.0\n",
    "    syllables_per_word = (syll_total / W) if W > 0 else 0.0\n",
    "    chars_per_word_ari = (chars_alnum / W) if W > 0 else 0.0\n",
    "\n",
    "    # Readability indices\n",
    "    # Flesch Reading Ease\n",
    "    flesch = 206.835 - 1.015 * words_per_sentence - 84.6 * syllables_per_word\n",
    "\n",
    "    # Gunning Fog\n",
    "    fog = 0.4 * (words_per_sentence + 100.0 * (complex_words / W if W > 0 else 0.0))\n",
    "\n",
    "    # SMOG\n",
    "    smog = (1.043 * math.sqrt(polysyllables * (30.0 / n_sents)) + 3.1291) if (polysyllables > 0 and n_sents > 0) else 0.0\n",
    "\n",
    "    # Automated Readability Index\n",
    "    ari = 4.71 * chars_per_word_ari + 0.5 * words_per_sentence - 21.43\n",
    "\n",
    "    return {\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "        \"type_token_ratio\": type_token_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"punctuation_ratio\": punctuation_ratio,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"sentence_length_std\": sentence_length_std,\n",
    "        \"flesch_reading_ease\": flesch,\n",
    "        \"gunning_fog\": fog,\n",
    "        \"smog_index\": smog,\n",
    "        \"automated_readability_index\": ari,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74acf093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram and burstiness utility functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# N-gram feature extraction utilities\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "import math\n",
    "\n",
    "def extract_ngrams(tokens: List[str], n: int) -> List[Tuple[str, ...]]:\n",
    "    \"\"\"Extract n-grams from a list of tokens.\"\"\"\n",
    "    if len(tokens) < n:\n",
    "        return []\n",
    "    return [tuple(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def ngram_diversity(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate n-gram diversity (unique n-grams / total n-grams).\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    return len(set(ngrams)) / len(ngrams)\n",
    "\n",
    "def ngram_entropy(tokens: List[str], n: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Shannon entropy of n-gram distribution.\n",
    "    Returns 0 if no n-grams possible.\n",
    "    \"\"\"\n",
    "    ngrams = extract_ngrams(tokens, n)\n",
    "    if not ngrams:\n",
    "        return 0.0\n",
    "    \n",
    "    counts = Counter(ngrams)\n",
    "    total = len(ngrams)\n",
    "    entropy = 0.0\n",
    "    \n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def calculate_burstiness(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate burstiness coefficient based on word frequency distribution.\n",
    "    Burstiness = (sigma - mu) / (sigma + mu)\n",
    "    where mu is mean frequency and sigma is standard deviation.\n",
    "    Returns 0 if only one unique token.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    frequencies = list(word_counts.values())\n",
    "    \n",
    "    if len(frequencies) < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    mu = np.mean(frequencies)\n",
    "    sigma = np.std(frequencies, ddof=0)\n",
    "    \n",
    "    if mu + sigma == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (sigma - mu) / (sigma + mu)\n",
    "\n",
    "print(\"N-gram and burstiness utility functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9638c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-level feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Character-level feature extraction\n",
    "\n",
    "import gzip\n",
    "\n",
    "def character_ngram_features(text: str, n: int = 3) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract character n-gram diversity and entropy.\n",
    "    Returns diversity ratio and entropy for character n-grams.\n",
    "    \"\"\"\n",
    "    if len(text) < n:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    char_ngrams = [text[i:i+n] for i in range(len(text) - n + 1)]\n",
    "    \n",
    "    if not char_ngrams:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Diversity\n",
    "    diversity = len(set(char_ngrams)) / len(char_ngrams)\n",
    "    \n",
    "    # Entropy\n",
    "    counts = Counter(char_ngrams)\n",
    "    total = len(char_ngrams)\n",
    "    entropy = 0.0\n",
    "    for count in counts.values():\n",
    "        prob = count / total\n",
    "        entropy -= prob * math.log2(prob)\n",
    "    \n",
    "    return diversity, entropy\n",
    "\n",
    "def compression_ratio(text: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate gzip compression ratio: compressed_size / original_size.\n",
    "    Lower values indicate more compressible (potentially more predictable) text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 1.0\n",
    "    \n",
    "    original_bytes = text.encode('utf-8')\n",
    "    compressed_bytes = gzip.compress(original_bytes)\n",
    "    \n",
    "    return len(compressed_bytes) / len(original_bytes)\n",
    "\n",
    "def character_statistics(text: str) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract surface-level character statistics.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return {\n",
    "            \"uppercase_ratio\": 0.0,\n",
    "            \"digit_ratio\": 0.0,\n",
    "            \"whitespace_ratio\": 0.0,\n",
    "            \"unique_char_count\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total_chars = len(text)\n",
    "    \n",
    "    return {\n",
    "        \"uppercase_ratio\": sum(1 for c in text if c.isupper()) / total_chars,\n",
    "        \"digit_ratio\": sum(1 for c in text if c.isdigit()) / total_chars,\n",
    "        \"whitespace_ratio\": sum(1 for c in text if c.isspace()) / total_chars,\n",
    "        \"unique_char_count\": float(len(set(text))),\n",
    "    }\n",
    "\n",
    "print(\"Character-level feature functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74b9e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependency tree feature functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Dependency tree and syntactic structure features\n",
    "\n",
    "def get_tree_depth(token) -> int:\n",
    "    \"\"\"\n",
    "    Recursively calculate the depth of the dependency subtree rooted at token.\n",
    "    \"\"\"\n",
    "    if not list(token.children):\n",
    "        return 1\n",
    "    return 1 + max(get_tree_depth(child) for child in token.children)\n",
    "\n",
    "def dependency_tree_features(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract dependency tree structural features.\n",
    "    \"\"\"\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    \n",
    "    if not word_toks:\n",
    "        return {\n",
    "            \"avg_tree_depth\": 0.0,\n",
    "            \"max_tree_depth\": 0.0,\n",
    "            \"avg_dependency_distance\": 0.0,\n",
    "            \"left_dependency_ratio\": 0.0,\n",
    "            \"right_dependency_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Find sentence roots and calculate tree depths\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    tree_depths = []\n",
    "    \n",
    "    for sent in sents:\n",
    "        root = [token for token in sent if token.head == token]\n",
    "        if root:\n",
    "            tree_depths.append(get_tree_depth(root[0]))\n",
    "    \n",
    "    avg_tree_depth = float(np.mean(tree_depths)) if tree_depths else 0.0\n",
    "    max_tree_depth = float(np.max(tree_depths)) if tree_depths else 0.0\n",
    "    \n",
    "    # Dependency distances (how far apart head and dependent are)\n",
    "    dep_distances = []\n",
    "    left_deps = 0\n",
    "    right_deps = 0\n",
    "    \n",
    "    for token in word_toks:\n",
    "        if token.head != token:  # Not root\n",
    "            distance = abs(token.i - token.head.i)\n",
    "            dep_distances.append(distance)\n",
    "            \n",
    "            # Track direction\n",
    "            if token.i < token.head.i:\n",
    "                left_deps += 1\n",
    "            else:\n",
    "                right_deps += 1\n",
    "    \n",
    "    avg_dep_distance = float(np.mean(dep_distances)) if dep_distances else 0.0\n",
    "    total_deps = left_deps + right_deps\n",
    "    \n",
    "    left_ratio = left_deps / total_deps if total_deps > 0 else 0.0\n",
    "    right_ratio = right_deps / total_deps if total_deps > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"avg_tree_depth\": avg_tree_depth,\n",
    "        \"max_tree_depth\": max_tree_depth,\n",
    "        \"avg_dependency_distance\": avg_dep_distance,\n",
    "        \"left_dependency_ratio\": left_ratio,\n",
    "        \"right_dependency_ratio\": right_ratio,\n",
    "    }\n",
    "\n",
    "print(\"Dependency tree feature functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89de6ce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary sophistication functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary sophistication and lexical diversity measures\n",
    "\n",
    "def hapax_legomena_ratio(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate ratio of words appearing exactly once (hapax legomena).\n",
    "    Indicates vocabulary richness.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    hapax_count = sum(1 for count in word_counts.values() if count == 1)\n",
    "    \n",
    "    return hapax_count / len(tokens)\n",
    "\n",
    "def yules_k(tokens: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Yule's K measure of lexical diversity.\n",
    "    More robust to text length than TTR.\n",
    "    K = 10000 * (M2 - M1) / (M1 * M1)\n",
    "    where M1 = number of tokens, M2 = sum of (frequency^2 * num_words_with_that_frequency)\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return 0.0\n",
    "    \n",
    "    word_counts = Counter(tokens)\n",
    "    M1 = len(tokens)\n",
    "    \n",
    "    frequency_spectrum = Counter(word_counts.values())\n",
    "    M2 = sum(freq * freq * count for freq, count in frequency_spectrum.items())\n",
    "    \n",
    "    if M1 <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    K = 10000 * (M2 - M1) / (M1 * M1)\n",
    "    return K\n",
    "\n",
    "def mtld(tokens: List[str], threshold: float = 0.72) -> float:\n",
    "    \"\"\"\n",
    "    Calculate MTLD (Measure of Textual Lexical Diversity).\n",
    "    Counts how many sequential word segments maintain TTR above threshold.\n",
    "    \"\"\"\n",
    "    if len(tokens) < 10:\n",
    "        return float(len(set(tokens)))\n",
    "    \n",
    "    def compute_factor(token_list):\n",
    "        ttr = 1.0\n",
    "        word_set = set()\n",
    "        factor_count = 0.0\n",
    "        \n",
    "        for i, token in enumerate(token_list, 1):\n",
    "            word_set.add(token)\n",
    "            ttr = len(word_set) / i\n",
    "            \n",
    "            if ttr < threshold:\n",
    "                factor_count += 1\n",
    "                word_set = set()\n",
    "        \n",
    "        if len(word_set) > 0:\n",
    "            factor_count += (1.0 - ttr) / (1.0 - threshold)\n",
    "        \n",
    "        return factor_count\n",
    "    \n",
    "    # Calculate forward and backward\n",
    "    forward = compute_factor(tokens)\n",
    "    backward = compute_factor(list(reversed(tokens)))\n",
    "    \n",
    "    if forward == 0 and backward == 0:\n",
    "        return float(len(tokens))\n",
    "    \n",
    "    factors = [f for f in [forward, backward] if f > 0]\n",
    "    mean_factor = np.mean(factors)\n",
    "    \n",
    "    return len(tokens) / mean_factor if mean_factor > 0 else float(len(tokens))\n",
    "\n",
    "def vocabulary_sophistication_features(tokens: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Aggregate vocabulary sophistication measures.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"hapax_legomena_ratio\": hapax_legomena_ratio(tokens),\n",
    "        \"yules_k\": yules_k(tokens),\n",
    "        \"mtld\": mtld(tokens),\n",
    "    }\n",
    "\n",
    "print(\"Vocabulary sophistication functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dfb021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation pattern functions loaded.\n"
     ]
    }
   ],
   "source": [
    "# Punctuation pattern analysis\n",
    "\n",
    "def punctuation_patterns(doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Detailed punctuation pattern features beyond simple ratio.\n",
    "    \"\"\"\n",
    "    all_tokens = [t for t in doc if not t.is_space]\n",
    "    punct_tokens = [t for t in doc if t.is_punct]\n",
    "    \n",
    "    if not all_tokens:\n",
    "        return {\n",
    "            \"comma_ratio\": 0.0,\n",
    "            \"period_ratio\": 0.0,\n",
    "            \"question_ratio\": 0.0,\n",
    "            \"exclamation_ratio\": 0.0,\n",
    "            \"semicolon_ratio\": 0.0,\n",
    "            \"colon_ratio\": 0.0,\n",
    "            \"quote_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    total = len(all_tokens)\n",
    "    \n",
    "    # Count specific punctuation marks\n",
    "    punct_text = ''.join([t.text for t in punct_tokens])\n",
    "    \n",
    "    return {\n",
    "        \"comma_ratio\": punct_text.count(',') / total,\n",
    "        \"period_ratio\": punct_text.count('.') / total,\n",
    "        \"question_ratio\": punct_text.count('?') / total,\n",
    "        \"exclamation_ratio\": punct_text.count('!') / total,\n",
    "        \"semicolon_ratio\": punct_text.count(';') / total,\n",
    "        \"colon_ratio\": punct_text.count(':') / total,\n",
    "        \"quote_ratio\": (punct_text.count('\"') + punct_text.count(\"'\")) / total,\n",
    "    }\n",
    "\n",
    "print(\"Punctuation pattern functions loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd67107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment_features(text: str, doc: Doc) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract sentiment and emotional tone features.\n",
    "    Uses TextBlob for polarity and subjectivity.\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return {\n",
    "            \"sentiment_polarity\": 0.0,\n",
    "            \"sentiment_subjectivity\": 0.0,\n",
    "            \"sentiment_polarity_variance\": 0.0,\n",
    "            \"positive_word_ratio\": 0.0,\n",
    "            \"negative_word_ratio\": 0.0,\n",
    "            \"neutral_sentence_ratio\": 0.0,\n",
    "        }\n",
    "    \n",
    "    # Overall document sentiment\n",
    "    blob = TextBlob(text)\n",
    "    features = {\n",
    "        \"sentiment_polarity\": blob.sentiment.polarity,  # -1 (negative) to 1 (positive)\n",
    "        \"sentiment_subjectivity\": blob.sentiment.subjectivity,  # 0 (objective) to 1 (subjective)\n",
    "    }\n",
    "    \n",
    "    # Sentence-level sentiment variance\n",
    "    sents = list(doc.sents) if doc.has_annotation(\"SENT_START\") else [doc]\n",
    "    sent_polarities = []\n",
    "    neutral_count = 0\n",
    "    \n",
    "    for sent in sents:\n",
    "        sent_blob = TextBlob(sent.text)\n",
    "        polarity = sent_blob.sentiment.polarity\n",
    "        sent_polarities.append(polarity)\n",
    "        \n",
    "        # Count neutral sentences (polarity close to 0)\n",
    "        if abs(polarity) < 0.1:\n",
    "            neutral_count += 1\n",
    "    \n",
    "    features[\"sentiment_polarity_variance\"] = float(np.var(sent_polarities)) if len(sent_polarities) > 1 else 0.0\n",
    "    features[\"neutral_sentence_ratio\"] = neutral_count / len(sents) if sents else 0.0\n",
    "    \n",
    "    # Positive/negative word ratios using spaCy tokens\n",
    "    word_toks = [t for t in doc if _word_like(t)]\n",
    "    if word_toks:\n",
    "        positive_count = 0\n",
    "        negative_count = 0\n",
    "        \n",
    "        for token in word_toks:\n",
    "            word_blob = TextBlob(token.text.lower())\n",
    "            polarity = word_blob.sentiment.polarity\n",
    "            \n",
    "            if polarity > 0.1:\n",
    "                positive_count += 1\n",
    "            elif polarity < -0.1:\n",
    "                negative_count += 1\n",
    "        \n",
    "        features[\"positive_word_ratio\"] = positive_count / len(word_toks)\n",
    "        features[\"negative_word_ratio\"] = negative_count / len(word_toks)\n",
    "    else:\n",
    "        features[\"positive_word_ratio\"] = 0.0\n",
    "        features[\"negative_word_ratio\"] = 0.0\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa1f44dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed feature columns:\n",
      "['avg_word_length', 'type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'compression_ratio', 'uppercase_ratio', 'digit_ratio', 'whitespace_ratio', 'unique_char_count', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'hapax_legomena_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adv_source_id</th>\n",
       "      <th>source_id</th>\n",
       "      <th>model</th>\n",
       "      <th>decoding</th>\n",
       "      <th>repetition_penalty</th>\n",
       "      <th>attack</th>\n",
       "      <th>domain</th>\n",
       "      <th>title</th>\n",
       "      <th>prompt</th>\n",
       "      <th>...</th>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <th>colon_ratio</th>\n",
       "      <th>quote_ratio</th>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <th>negative_word_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31dac42c-d620-48ef-9f4e-30a072594c4a</td>\n",
       "      <td>cd90a5fb-6aee-4a78-a32d-e34016718deb</td>\n",
       "      <td>0499bbab-4652-479a-8409-95f5ee077c0a</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>number</td>\n",
       "      <td>poetry</td>\n",
       "      <td>Or From That Sea Of Time</td>\n",
       "      <td>The following is the full text of a poem title...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.050625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ec269ad5-b494-4269-aff4-9d2b4023db56</td>\n",
       "      <td>5890f7ff-7153-49ba-b4f0-11cb0fc502fa</td>\n",
       "      <td>3a482121-a841-4cc5-b071-058785c10525</td>\n",
       "      <td>mpt-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>yes</td>\n",
       "      <td>upper_lower</td>\n",
       "      <td>recipes</td>\n",
       "      <td>Petit Fours</td>\n",
       "      <td>Write a recipe for \"Petit Fours\".</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011696</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.014620</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.187625</td>\n",
       "      <td>0.533749</td>\n",
       "      <td>0.035866</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.072414</td>\n",
       "      <td>0.024138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86644bab-0ba0-4b90-bba6-344df0e58ea7</td>\n",
       "      <td>e6d6a10d-6a5b-4801-b160-68ec66db6f9c</td>\n",
       "      <td>e16d6eac-e79b-439d-8f6c-673dcebaecad</td>\n",
       "      <td>cohere-chat</td>\n",
       "      <td>sampling</td>\n",
       "      <td>no</td>\n",
       "      <td>zero_width_space</td>\n",
       "      <td>wiki</td>\n",
       "      <td>Formal specification</td>\n",
       "      <td>Write the body of a Wikipedia article titled \"...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.008333</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.013217</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.021429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                         adv_source_id  \\\n",
       "0  31dac42c-d620-48ef-9f4e-30a072594c4a  cd90a5fb-6aee-4a78-a32d-e34016718deb   \n",
       "1  ec269ad5-b494-4269-aff4-9d2b4023db56  5890f7ff-7153-49ba-b4f0-11cb0fc502fa   \n",
       "2  86644bab-0ba0-4b90-bba6-344df0e58ea7  e6d6a10d-6a5b-4801-b160-68ec66db6f9c   \n",
       "\n",
       "                              source_id        model  decoding  \\\n",
       "0  0499bbab-4652-479a-8409-95f5ee077c0a         gpt2  sampling   \n",
       "1  3a482121-a841-4cc5-b071-058785c10525     mpt-chat  sampling   \n",
       "2  e16d6eac-e79b-439d-8f6c-673dcebaecad  cohere-chat  sampling   \n",
       "\n",
       "  repetition_penalty            attack   domain                     title  \\\n",
       "0                yes            number   poetry  Or From That Sea Of Time   \n",
       "1                yes       upper_lower  recipes               Petit Fours   \n",
       "2                 no  zero_width_space     wiki      Formal specification   \n",
       "\n",
       "                                              prompt  ... exclamation_ratio  \\\n",
       "0  The following is the full text of a poem title...  ...          0.000000   \n",
       "1                  Write a recipe for \"Petit Fours\".  ...          0.011696   \n",
       "2  Write the body of a Wikipedia article titled \"...  ...          0.000000   \n",
       "\n",
       "   semicolon_ratio colon_ratio quote_ratio  sentiment_polarity  \\\n",
       "0         0.000000    0.018519    0.000000            0.450000   \n",
       "1         0.002924    0.014620    0.002924            0.187625   \n",
       "2         0.000000    0.000000    0.000000           -0.008333   \n",
       "\n",
       "   sentiment_subjectivity  sentiment_polarity_variance  \\\n",
       "0                0.650000                     0.050625   \n",
       "1                0.533749                     0.035866   \n",
       "2                0.287500                     0.013217   \n",
       "\n",
       "   neutral_sentence_ratio  positive_word_ratio  negative_word_ratio  \n",
       "0                0.500000             0.040000             0.000000  \n",
       "1                0.363636             0.072414             0.024138  \n",
       "2                0.636364             0.021429             0.021429  \n",
       "\n",
       "[3 rows x 75 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application NLP.pipe based\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "# 1 for deterministic ordering in some environments; -1  all available cores\n",
    "N_PROCESS = -1\n",
    "\n",
    "texts = df[TEXT_COL].astype(str).tolist()\n",
    "\n",
    "feature_rows: List[Dict[str, float]] = []\n",
    "for text, doc in zip(texts, nlp.pipe(texts, batch_size=BATCH_SIZE, n_process=N_PROCESS)):\n",
    "    doc_features: Dict[str, float] = {}\n",
    "\n",
    "    tokens = [t.text for t in doc if _word_like(t)]\n",
    "    doc_features.update(features_from_doc(doc))\n",
    "\n",
    "    # Word n-gram diversity/entropy and burstiness\n",
    "    doc_features[\"unigram_diversity\"] = ngram_diversity(tokens, 1)\n",
    "    doc_features[\"bigram_diversity\"] = ngram_diversity(tokens, 2)\n",
    "    doc_features[\"trigram_diversity\"] = ngram_diversity(tokens, 3)\n",
    "    doc_features[\"bigram_entropy\"] = ngram_entropy(tokens, 2)\n",
    "    doc_features[\"trigram_entropy\"] = ngram_entropy(tokens, 3)\n",
    "    doc_features[\"token_burstiness\"] = calculate_burstiness(tokens)\n",
    "\n",
    "    # Character-level features\n",
    "    trigram_diversity, trigram_entropy = character_ngram_features(text, n=3)\n",
    "    doc_features[\"char_trigram_diversity\"] = trigram_diversity\n",
    "    doc_features[\"char_trigram_entropy\"] = trigram_entropy\n",
    "    doc_features[\"compression_ratio\"] = compression_ratio(text)\n",
    "    doc_features.update(character_statistics(text))\n",
    "\n",
    "    # Syntactic, lexical, punctuation, and sentiment features\n",
    "    doc_features.update(dependency_tree_features(doc))\n",
    "    doc_features.update(vocabulary_sophistication_features(tokens))\n",
    "    doc_features.update(punctuation_patterns(doc))\n",
    "    doc_features.update(sentiment_features(text, doc))\n",
    "\n",
    "    feature_rows.append(doc_features)\n",
    "\n",
    "feat_df = pd.DataFrame(feature_rows)\n",
    "df_with_features = pd.concat([df.reset_index(drop=True), feat_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "print(\"Computed feature columns:\")\n",
    "print(list(feat_df.columns))\n",
    "df_with_features.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c5cd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved enriched dataset: raid_sample_medium_with_features_PREPOS.csv  (rows: 12000)\n",
      "Feature columns saved (42 total):\n",
      "['type_token_ratio', 'stopword_ratio', 'punctuation_ratio', 'avg_sentence_length', 'sentence_length_std', 'flesch_reading_ease', 'gunning_fog', 'smog_index', 'automated_readability_index', 'unigram_diversity', 'bigram_diversity', 'trigram_diversity', 'bigram_entropy', 'trigram_entropy', 'token_burstiness', 'char_trigram_diversity', 'char_trigram_entropy', 'compression_ratio', 'uppercase_ratio', 'whitespace_ratio', 'unique_char_count', 'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance', 'left_dependency_ratio', 'right_dependency_ratio', 'hapax_legomena_ratio', 'yules_k', 'mtld', 'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio', 'semicolon_ratio', 'colon_ratio', 'quote_ratio', 'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance', 'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>10%</th>\n",
       "      <th>50%</th>\n",
       "      <th>90%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>type_token_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.588571</td>\n",
       "      <td>0.145105</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.449778</td>\n",
       "      <td>0.581017</td>\n",
       "      <td>0.752700</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stopword_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.441644</td>\n",
       "      <td>0.167728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220585</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.609045</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>punctuation_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016749</td>\n",
       "      <td>0.028137</td>\n",
       "      <td>0.045290</td>\n",
       "      <td>0.229457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_sentence_length</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>23.261864</td>\n",
       "      <td>23.031781</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12.478054</td>\n",
       "      <td>19.777778</td>\n",
       "      <td>30.200000</td>\n",
       "      <td>427.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_length_std</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>10.124756</td>\n",
       "      <td>11.268769</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.029888</td>\n",
       "      <td>7.878088</td>\n",
       "      <td>16.025289</td>\n",
       "      <td>234.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>52.543699</td>\n",
       "      <td>31.866539</td>\n",
       "      <td>-345.375000</td>\n",
       "      <td>19.497500</td>\n",
       "      <td>58.332917</td>\n",
       "      <td>81.643773</td>\n",
       "      <td>193.729876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gunning_fog</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>14.574954</td>\n",
       "      <td>9.920579</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>7.747965</td>\n",
       "      <td>13.077524</td>\n",
       "      <td>20.919824</td>\n",
       "      <td>170.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smog_index</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>12.194485</td>\n",
       "      <td>4.486948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.447530</td>\n",
       "      <td>11.765961</td>\n",
       "      <td>17.269485</td>\n",
       "      <td>83.107548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>automated_readability_index</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>13.035883</td>\n",
       "      <td>12.400888</td>\n",
       "      <td>-5.367672</td>\n",
       "      <td>5.813742</td>\n",
       "      <td>11.199291</td>\n",
       "      <td>18.708750</td>\n",
       "      <td>299.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unigram_diversity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.612270</td>\n",
       "      <td>0.147626</td>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.471785</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.779412</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_diversity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.905261</td>\n",
       "      <td>0.132842</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.836535</td>\n",
       "      <td>0.933927</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_diversity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.956708</td>\n",
       "      <td>0.127377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.932584</td>\n",
       "      <td>0.987654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bigram_entropy</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>7.451990</td>\n",
       "      <td>0.956696</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.517041</td>\n",
       "      <td>7.545304</td>\n",
       "      <td>8.428517</td>\n",
       "      <td>13.143263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trigram_entropy</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>7.568515</td>\n",
       "      <td>0.978851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.599913</td>\n",
       "      <td>7.672425</td>\n",
       "      <td>8.553473</td>\n",
       "      <td>13.472103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_burstiness</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>-0.036750</td>\n",
       "      <td>0.227217</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.271018</td>\n",
       "      <td>-0.002330</td>\n",
       "      <td>0.185217</td>\n",
       "      <td>0.756181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_diversity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.525953</td>\n",
       "      <td>0.125661</td>\n",
       "      <td>0.006455</td>\n",
       "      <td>0.394140</td>\n",
       "      <td>0.526649</td>\n",
       "      <td>0.668577</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>char_trigram_entropy</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>8.871163</td>\n",
       "      <td>0.655246</td>\n",
       "      <td>3.007080</td>\n",
       "      <td>8.284809</td>\n",
       "      <td>8.957517</td>\n",
       "      <td>9.507826</td>\n",
       "      <td>10.686642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.519465</td>\n",
       "      <td>0.094092</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.446662</td>\n",
       "      <td>0.522857</td>\n",
       "      <td>0.600735</td>\n",
       "      <td>1.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.026928</td>\n",
       "      <td>0.018735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010781</td>\n",
       "      <td>0.023726</td>\n",
       "      <td>0.045428</td>\n",
       "      <td>0.520888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.170021</td>\n",
       "      <td>0.017826</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.145552</td>\n",
       "      <td>0.171186</td>\n",
       "      <td>0.191140</td>\n",
       "      <td>0.279001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_char_count</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>46.555917</td>\n",
       "      <td>10.280873</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>222.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_tree_depth</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>7.020740</td>\n",
       "      <td>5.591922</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.894737</td>\n",
       "      <td>6.571429</td>\n",
       "      <td>8.714286</td>\n",
       "      <td>195.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tree_depth</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>10.919417</td>\n",
       "      <td>8.293069</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>2.783334</td>\n",
       "      <td>1.395051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.206339</td>\n",
       "      <td>2.611111</td>\n",
       "      <td>3.388143</td>\n",
       "      <td>69.054286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>left_dependency_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.517509</td>\n",
       "      <td>0.084519</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.431296</td>\n",
       "      <td>0.504831</td>\n",
       "      <td>0.623279</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_dependency_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.482407</td>\n",
       "      <td>0.084501</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376615</td>\n",
       "      <td>0.495146</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>0.856354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hapax_legomena_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.465198</td>\n",
       "      <td>0.168309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.299723</td>\n",
       "      <td>0.446721</td>\n",
       "      <td>0.657420</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yules_k</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>139.618024</td>\n",
       "      <td>261.950277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>56.812687</td>\n",
       "      <td>108.838431</td>\n",
       "      <td>188.857726</td>\n",
       "      <td>9918.699187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mtld</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>184.654935</td>\n",
       "      <td>1475.403990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.188608</td>\n",
       "      <td>1.732707</td>\n",
       "      <td>145.674835</td>\n",
       "      <td>46381.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comma_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.046016</td>\n",
       "      <td>0.029343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.042813</td>\n",
       "      <td>0.078366</td>\n",
       "      <td>0.389401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>period_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.042403</td>\n",
       "      <td>0.022341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.063380</td>\n",
       "      <td>1.050725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005321</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exclamation_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.005099</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semicolon_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.005343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004348</td>\n",
       "      <td>0.296928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>colon_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006623</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quote_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.013118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>0.185771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.093871</td>\n",
       "      <td>0.130533</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.029865</td>\n",
       "      <td>0.085247</td>\n",
       "      <td>0.246476</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_subjectivity</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.421896</td>\n",
       "      <td>0.172734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.448516</td>\n",
       "      <td>0.597105</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_polarity_variance</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.040879</td>\n",
       "      <td>0.038963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.031655</td>\n",
       "      <td>0.090738</td>\n",
       "      <td>0.394180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral_sentence_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.547948</td>\n",
       "      <td>0.247804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive_word_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.035016</td>\n",
       "      <td>0.028085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.064220</td>\n",
       "      <td>0.811111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_word_ratio</th>\n",
       "      <td>12000.0</td>\n",
       "      <td>0.016371</td>\n",
       "      <td>0.016123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013761</td>\n",
       "      <td>0.035045</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count        mean          std         min  \\\n",
       "type_token_ratio             12000.0    0.588571     0.145105    0.006803   \n",
       "stopword_ratio               12000.0    0.441644     0.167728    0.000000   \n",
       "punctuation_ratio            12000.0    0.029730     0.012610    0.000000   \n",
       "avg_sentence_length          12000.0   23.261864    23.031781    0.500000   \n",
       "sentence_length_std          12000.0   10.124756    11.268769    0.000000   \n",
       "flesch_reading_ease          12000.0   52.543699    31.866539 -345.375000   \n",
       "gunning_fog                  12000.0   14.574954     9.920579    1.200000   \n",
       "smog_index                   12000.0   12.194485     4.486948    0.000000   \n",
       "automated_readability_index  12000.0   13.035883    12.400888   -5.367672   \n",
       "unigram_diversity            12000.0    0.612270     0.147626    0.006803   \n",
       "bigram_diversity             12000.0    0.905261     0.132842    0.000000   \n",
       "trigram_diversity            12000.0    0.956708     0.127377    0.000000   \n",
       "bigram_entropy               12000.0    7.451990     0.956696    0.000000   \n",
       "trigram_entropy              12000.0    7.568515     0.978851    0.000000   \n",
       "token_burstiness             12000.0   -0.036750     0.227217   -1.000000   \n",
       "char_trigram_diversity       12000.0    0.525953     0.125661    0.006455   \n",
       "char_trigram_entropy         12000.0    8.871163     0.655246    3.007080   \n",
       "compression_ratio            12000.0    0.519465     0.094092    0.027405   \n",
       "uppercase_ratio              12000.0    0.026928     0.018735    0.000000   \n",
       "whitespace_ratio             12000.0    0.170021     0.017826    0.022472   \n",
       "unique_char_count            12000.0   46.555917    10.280873    9.000000   \n",
       "avg_tree_depth               12000.0    7.020740     5.591922    2.000000   \n",
       "max_tree_depth               12000.0   10.919417     8.293069    2.000000   \n",
       "avg_dependency_distance      12000.0    2.783334     1.395051    0.000000   \n",
       "left_dependency_ratio        12000.0    0.517509     0.084519    0.000000   \n",
       "right_dependency_ratio       12000.0    0.482407     0.084501    0.000000   \n",
       "hapax_legomena_ratio         12000.0    0.465198     0.168309    0.000000   \n",
       "yules_k                      12000.0  139.618024   261.950277    0.000000   \n",
       "mtld                         12000.0  184.654935  1475.403990    1.000000   \n",
       "comma_ratio                  12000.0    0.046016     0.029343    0.000000   \n",
       "period_ratio                 12000.0    0.042403     0.022341    0.000000   \n",
       "question_ratio               12000.0    0.001547     0.005032    0.000000   \n",
       "exclamation_ratio            12000.0    0.001181     0.005099    0.000000   \n",
       "semicolon_ratio              12000.0    0.001401     0.005343    0.000000   \n",
       "colon_ratio                  12000.0    0.001837     0.004702    0.000000   \n",
       "quote_ratio                  12000.0    0.006592     0.013118    0.000000   \n",
       "sentiment_polarity           12000.0    0.093871     0.130533   -1.000000   \n",
       "sentiment_subjectivity       12000.0    0.421896     0.172734    0.000000   \n",
       "sentiment_polarity_variance  12000.0    0.040879     0.038963    0.000000   \n",
       "neutral_sentence_ratio       12000.0    0.547948     0.247804    0.000000   \n",
       "positive_word_ratio          12000.0    0.035016     0.028085    0.000000   \n",
       "negative_word_ratio          12000.0    0.016371     0.016123    0.000000   \n",
       "\n",
       "                                   10%         50%         90%           max  \n",
       "type_token_ratio              0.449778    0.581017    0.752700      1.000000  \n",
       "stopword_ratio                0.220585    0.473684    0.609045      1.000000  \n",
       "punctuation_ratio             0.016749    0.028137    0.045290      0.229457  \n",
       "avg_sentence_length          12.478054   19.777778   30.200000    427.000000  \n",
       "sentence_length_std           4.029888    7.878088   16.025289    234.000000  \n",
       "flesch_reading_ease          19.497500   58.332917   81.643773    193.729876  \n",
       "gunning_fog                   7.747965   13.077524   20.919824    170.800000  \n",
       "smog_index                    7.447530   11.765961   17.269485     83.107548  \n",
       "automated_readability_index   5.813742   11.199291   18.708750    299.100000  \n",
       "unigram_diversity             0.471785    0.605263    0.779412      1.000000  \n",
       "bigram_diversity              0.836535    0.933927    0.991150      1.000000  \n",
       "trigram_diversity             0.932584    0.987654    1.000000      1.000000  \n",
       "bigram_entropy                6.517041    7.545304    8.428517     13.143263  \n",
       "trigram_entropy               6.599913    7.672425    8.553473     13.472103  \n",
       "token_burstiness             -0.271018   -0.002330    0.185217      0.756181  \n",
       "char_trigram_diversity        0.394140    0.526649    0.668577      1.000000  \n",
       "char_trigram_entropy          8.284809    8.957517    9.507826     10.686642  \n",
       "compression_ratio             0.446662    0.522857    0.600735      1.666667  \n",
       "uppercase_ratio               0.010781    0.023726    0.045428      0.520888  \n",
       "whitespace_ratio              0.145552    0.171186    0.191140      0.279001  \n",
       "unique_char_count            34.000000   46.000000   60.000000    222.000000  \n",
       "avg_tree_depth                4.894737    6.571429    8.714286    195.000000  \n",
       "max_tree_depth                7.000000   10.000000   14.000000    332.000000  \n",
       "avg_dependency_distance       2.206339    2.611111    3.388143     69.054286  \n",
       "left_dependency_ratio         0.431296    0.504831    0.623279      1.000000  \n",
       "right_dependency_ratio        0.376615    0.495146    0.568635      0.856354  \n",
       "hapax_legomena_ratio          0.299723    0.446721    0.657420      1.000000  \n",
       "yules_k                      56.812687  108.838431  188.857726   9918.699187  \n",
       "mtld                          1.188608    1.732707  145.674835  46381.720000  \n",
       "comma_ratio                   0.012987    0.042813    0.078366      0.389401  \n",
       "period_ratio                  0.022727    0.040650    0.063380      1.050725  \n",
       "question_ratio                0.000000    0.000000    0.005321      0.083333  \n",
       "exclamation_ratio             0.000000    0.000000    0.003289      0.200000  \n",
       "semicolon_ratio               0.000000    0.000000    0.004348      0.296928  \n",
       "colon_ratio                   0.000000    0.000000    0.006623      0.166667  \n",
       "quote_ratio                   0.000000    0.000000    0.022727      0.185771  \n",
       "sentiment_polarity           -0.029865    0.085247    0.246476      1.000000  \n",
       "sentiment_subjectivity        0.200000    0.448516    0.597105      1.000000  \n",
       "sentiment_polarity_variance   0.000206    0.031655    0.090738      0.394180  \n",
       "neutral_sentence_ratio        0.250000    0.538462    1.000000      1.000000  \n",
       "positive_word_ratio           0.000000    0.031970    0.064220      0.811111  \n",
       "negative_word_ratio           0.000000    0.013761    0.035045      0.333333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Save enriched dataset and basic descriptive statistics\n",
    "\n",
    "OUT_WITH_FEATS = f\"raid_sample_{SELECTED_DATASET}_with_features_PREPOS.csv\"\n",
    "df_with_features.to_csv(OUT_WITH_FEATS, index=False)\n",
    "print(f\"Saved enriched dataset: {OUT_WITH_FEATS}  (rows: {len(df_with_features)})\")\n",
    "\n",
    "feature_cols = [col for col in df_with_features.columns if col not in df.columns]\n",
    "print(f\"Feature columns saved ({len(feature_cols)} total):\")\n",
    "print(feature_cols)\n",
    "\n",
    "display(df_with_features[feature_cols].describe(percentiles=[0.1, 0.5, 0.9]).T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "125c533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_with_features.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b190a66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_sentence_length min: 0.5 max: 427.0\n",
      "flesch_reading_ease min: -345.37499999999994 max: 193.72987554112555\n",
      "gunning_fog min: 1.2000000000000002 max: 170.8\n",
      "automated_readability_index min: -5.3676718336938265 max: 299.09999999999997\n",
      "mtld min: 1.0 max: 46381.72000000067\n",
      "yules_k min: 0.0 max: 9918.69918699187\n",
      "max_tree_depth min: 2.0 max: 332.0\n"
     ]
    }
   ],
   "source": [
    "for col in [\"avg_sentence_length\",\"flesch_reading_ease\",\"gunning_fog\",\n",
    "            \"automated_readability_index\",\"mtld\",\"yules_k\",\"max_tree_depth\"]:\n",
    "    if col in df.columns:\n",
    "        print(col, \"min:\", df[col].min(), \"max:\", df[col].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b72a3cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed thresholds:\n",
      "  asl_hi: 119.00500000000011\n",
      "  sls_hi: 80.0\n",
      "  fog_hi: 77.73378260869514\n",
      "  ari_hi: 97.3265883196227\n",
      "  fre_lo: -115.03797738095234\n",
      "  mtld_hi: 9027.516866666618\n",
      "  yk_hi: 1475.1035246435642\n",
      "  depth_max_hi: 50.0\n",
      "  depth_avg_hi: 25.0\n",
      "  depdist_hi: 8.0\n",
      "  comp_hi: 1.05\n",
      "  upper_hi: 0.15\n",
      "  uniq_hi: 80.0\n",
      "  ws_lo: 0.12\n",
      "  ws_hi: 0.25\n",
      "\n",
      "Cause counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cause</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lexical_metric_instability_or_length_effects</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dependency_depth_computation_bug</th>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence_segmentation_failure</th>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_or_code_noise</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              count\n",
       "cause                                              \n",
       "lexical_metric_instability_or_length_effects    121\n",
       "dependency_depth_computation_bug                 81\n",
       "sentence_segmentation_failure                    63\n",
       "markup_or_code_noise                             58"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Flag counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>severity</th>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lexical_instability</th>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>readability_outlier</th>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth_implausible</th>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>markup_noise</th>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compression_overhead</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          count\n",
       "severity                    491\n",
       "seg_len_extreme             164\n",
       "lexical_instability         120\n",
       "readability_outlier          65\n",
       "depth_implausible            53\n",
       "markup_noise                 45\n",
       "dep_distance_implausible     28\n",
       "compression_overhead         14\n",
       "ngram_edge_effects            2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top offenders (with original feature values):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seg_len_extreme</th>\n",
       "      <th>readability_outlier</th>\n",
       "      <th>lexical_instability</th>\n",
       "      <th>ngram_edge_effects</th>\n",
       "      <th>depth_implausible</th>\n",
       "      <th>dep_distance_implausible</th>\n",
       "      <th>compression_overhead</th>\n",
       "      <th>markup_noise</th>\n",
       "      <th>suspected_causes</th>\n",
       "      <th>severity</th>\n",
       "      <th>...</th>\n",
       "      <th>max_tree_depth</th>\n",
       "      <th>avg_dependency_distance</th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>uppercase_ratio</th>\n",
       "      <th>unique_char_count</th>\n",
       "      <th>whitespace_ratio</th>\n",
       "      <th>bigram_entropy</th>\n",
       "      <th>trigram_entropy</th>\n",
       "      <th>bigram_diversity</th>\n",
       "      <th>trigram_diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9906</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.008197</td>\n",
       "      <td>0.031805</td>\n",
       "      <td>0.090976</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.090237</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008197</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9547</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>155.0</td>\n",
       "      <td>2.012500</td>\n",
       "      <td>0.093366</td>\n",
       "      <td>0.003276</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.260442</td>\n",
       "      <td>2.992157</td>\n",
       "      <td>3.503450</td>\n",
       "      <td>0.079167</td>\n",
       "      <td>0.092050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5952</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>45.801724</td>\n",
       "      <td>0.067857</td>\n",
       "      <td>0.029390</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.262246</td>\n",
       "      <td>3.078176</td>\n",
       "      <td>3.392755</td>\n",
       "      <td>0.048851</td>\n",
       "      <td>0.057637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9057</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2.656325</td>\n",
       "      <td>0.035737</td>\n",
       "      <td>0.045484</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.272255</td>\n",
       "      <td>2.584942</td>\n",
       "      <td>2.584929</td>\n",
       "      <td>0.014320</td>\n",
       "      <td>0.014354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>19.563584</td>\n",
       "      <td>0.070344</td>\n",
       "      <td>0.063949</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.276579</td>\n",
       "      <td>2.872940</td>\n",
       "      <td>3.127343</td>\n",
       "      <td>0.040462</td>\n",
       "      <td>0.052174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8536</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1.503086</td>\n",
       "      <td>0.059816</td>\n",
       "      <td>0.062117</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.248466</td>\n",
       "      <td>2.167725</td>\n",
       "      <td>2.192122</td>\n",
       "      <td>0.033951</td>\n",
       "      <td>0.037152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.262987</td>\n",
       "      <td>0.542149</td>\n",
       "      <td>0.006203</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.127378</td>\n",
       "      <td>8.266787</td>\n",
       "      <td>8.262095</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>69.054286</td>\n",
       "      <td>0.056460</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.178026</td>\n",
       "      <td>2.580156</td>\n",
       "      <td>2.623900</td>\n",
       "      <td>0.048571</td>\n",
       "      <td>0.054441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>[lexical_metric_instability_or_length_effects,...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.202247</td>\n",
       "      <td>0.123596</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.022472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>140.0</td>\n",
       "      <td>2.789625</td>\n",
       "      <td>0.031739</td>\n",
       "      <td>0.030435</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.150870</td>\n",
       "      <td>2.321892</td>\n",
       "      <td>2.321904</td>\n",
       "      <td>0.014409</td>\n",
       "      <td>0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7922</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1.510769</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.047991</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.181362</td>\n",
       "      <td>2.167355</td>\n",
       "      <td>2.167781</td>\n",
       "      <td>0.033846</td>\n",
       "      <td>0.033951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4467</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.216138</td>\n",
       "      <td>0.033739</td>\n",
       "      <td>0.038717</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.191925</td>\n",
       "      <td>2.321892</td>\n",
       "      <td>2.321904</td>\n",
       "      <td>0.014409</td>\n",
       "      <td>0.014451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4588</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>35.802083</td>\n",
       "      <td>0.053608</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.098969</td>\n",
       "      <td>2.056118</td>\n",
       "      <td>2.119485</td>\n",
       "      <td>0.052083</td>\n",
       "      <td>0.063158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7828</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>26.0</td>\n",
       "      <td>3.338462</td>\n",
       "      <td>0.075648</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.093660</td>\n",
       "      <td>1.846188</td>\n",
       "      <td>2.342442</td>\n",
       "      <td>0.107692</td>\n",
       "      <td>0.116279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2.489164</td>\n",
       "      <td>0.033972</td>\n",
       "      <td>0.100062</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.199506</td>\n",
       "      <td>1.999979</td>\n",
       "      <td>1.999972</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>0.012422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10670</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>[dependency_depth_computation_bug, lexical_met...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1.664360</td>\n",
       "      <td>0.043152</td>\n",
       "      <td>0.092871</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.272045</td>\n",
       "      <td>1.688671</td>\n",
       "      <td>1.744479</td>\n",
       "      <td>0.020690</td>\n",
       "      <td>0.027682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>50.117371</td>\n",
       "      <td>0.031925</td>\n",
       "      <td>0.120188</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.358776</td>\n",
       "      <td>2.358839</td>\n",
       "      <td>0.016432</td>\n",
       "      <td>0.016471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7242</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1.662116</td>\n",
       "      <td>0.027405</td>\n",
       "      <td>0.105320</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.157442</td>\n",
       "      <td>1.584946</td>\n",
       "      <td>1.584946</td>\n",
       "      <td>0.010239</td>\n",
       "      <td>0.010274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>40.0</td>\n",
       "      <td>3.658703</td>\n",
       "      <td>0.051051</td>\n",
       "      <td>0.064865</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.175976</td>\n",
       "      <td>2.109911</td>\n",
       "      <td>2.354599</td>\n",
       "      <td>0.034130</td>\n",
       "      <td>0.047945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7115</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>[dependency_depth_computation_bug, sentence_se...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>20.230126</td>\n",
       "      <td>0.056548</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.239167</td>\n",
       "      <td>2.321877</td>\n",
       "      <td>2.321852</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.021008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       seg_len_extreme  readability_outlier  lexical_instability  \\\n",
       "9906              True                 True                 True   \n",
       "9547              True                 True                 True   \n",
       "5952              True                 True                 True   \n",
       "9057              True                 True                 True   \n",
       "8912              True                 True                 True   \n",
       "8536              True                 True                 True   \n",
       "1024              True                 True                 True   \n",
       "8521              True                 True                 True   \n",
       "8200             False                 True                False   \n",
       "4400              True                 True                 True   \n",
       "7922              True                 True                 True   \n",
       "4467              True                 True                 True   \n",
       "4588             False                 True                 True   \n",
       "7828              True                False                 True   \n",
       "10755             True                 True                 True   \n",
       "10670             True                False                 True   \n",
       "9994              True                 True                 True   \n",
       "7242              True                 True                 True   \n",
       "626               True                 True                 True   \n",
       "7115              True                 True                 True   \n",
       "\n",
       "       ngram_edge_effects  depth_implausible  dep_distance_implausible  \\\n",
       "9906                 True               True                     False   \n",
       "9547                False               True                     False   \n",
       "5952                False              False                      True   \n",
       "9057                False               True                     False   \n",
       "8912                False              False                      True   \n",
       "8536                False               True                     False   \n",
       "1024                False              False                      True   \n",
       "8521                False              False                      True   \n",
       "8200                 True              False                     False   \n",
       "4400                False               True                     False   \n",
       "7922                False               True                     False   \n",
       "4467                False              False                      True   \n",
       "4588                False              False                      True   \n",
       "7828                False               True                     False   \n",
       "10755               False               True                     False   \n",
       "10670               False               True                     False   \n",
       "9994                False              False                      True   \n",
       "7242                False               True                     False   \n",
       "626                 False               True                     False   \n",
       "7115                False              False                      True   \n",
       "\n",
       "       compression_overhead  markup_noise  \\\n",
       "9906                  False          True   \n",
       "9547                  False          True   \n",
       "5952                  False          True   \n",
       "9057                  False          True   \n",
       "8912                  False          True   \n",
       "8536                  False         False   \n",
       "1024                  False         False   \n",
       "8521                  False         False   \n",
       "8200                   True          True   \n",
       "4400                  False         False   \n",
       "7922                  False         False   \n",
       "4467                  False         False   \n",
       "4588                  False          True   \n",
       "7828                  False          True   \n",
       "10755                 False         False   \n",
       "10670                 False          True   \n",
       "9994                  False         False   \n",
       "7242                  False         False   \n",
       "626                   False         False   \n",
       "7115                  False         False   \n",
       "\n",
       "                                        suspected_causes  severity  ...  \\\n",
       "9906   [dependency_depth_computation_bug, sentence_se...         6  ...   \n",
       "9547   [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "5952   [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "9057   [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "8912   [dependency_depth_computation_bug, sentence_se...         5  ...   \n",
       "8536   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "1024   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "8521   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "8200   [lexical_metric_instability_or_length_effects,...         4  ...   \n",
       "4400   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "7922   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "4467   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "4588   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "7828   [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "10755  [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "10670  [dependency_depth_computation_bug, lexical_met...         4  ...   \n",
       "9994   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "7242   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "626    [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "7115   [dependency_depth_computation_bug, sentence_se...         4  ...   \n",
       "\n",
       "       max_tree_depth  avg_dependency_distance  compression_ratio  \\\n",
       "9906            122.0                 1.008197           0.031805   \n",
       "9547            155.0                 2.012500           0.093366   \n",
       "5952              8.0                45.801724           0.067857   \n",
       "9057             72.0                 2.656325           0.035737   \n",
       "8912              9.0                19.563584           0.070344   \n",
       "8536            161.0                 1.503086           0.059816   \n",
       "1024             14.0                10.262987           0.542149   \n",
       "8521              5.0                69.054286           0.056460   \n",
       "8200              2.0                 0.000000           1.202247   \n",
       "4400            140.0                 2.789625           0.031739   \n",
       "7922            161.0                 1.510769           0.046875   \n",
       "4467              6.0                35.216138           0.033739   \n",
       "4588              4.0                35.802083           0.053608   \n",
       "7828             26.0                 3.338462           0.075648   \n",
       "10755            83.0                 2.489164           0.033972   \n",
       "10670            97.0                 1.664360           0.043152   \n",
       "9994              7.0                50.117371           0.031925   \n",
       "7242            100.0                 1.662116           0.027405   \n",
       "626              40.0                 3.658703           0.051051   \n",
       "7115              6.0                20.230126           0.056548   \n",
       "\n",
       "       uppercase_ratio  unique_char_count  whitespace_ratio  bigram_entropy  \\\n",
       "9906          0.090976               10.0          0.090237        0.000000   \n",
       "9547          0.003276               24.0          0.260442        2.992157   \n",
       "5952          0.029390               20.0          0.262246        3.078176   \n",
       "9057          0.045484               13.0          0.272255        2.584942   \n",
       "8912          0.063949               16.0          0.276579        2.872940   \n",
       "8536          0.062117               21.0          0.248466        2.167725   \n",
       "1024          0.006203               44.0          0.127378        8.266787   \n",
       "8521          0.001526               23.0          0.178026        2.580156   \n",
       "8200          0.123596               43.0          0.022472        0.000000   \n",
       "4400          0.030435               18.0          0.150870        2.321892   \n",
       "7922          0.047991               22.0          0.181362        2.167355   \n",
       "4467          0.038717               15.0          0.191925        2.321892   \n",
       "4588          0.100000               14.0          0.098969        2.056118   \n",
       "7828          0.018732               27.0          0.093660        1.846188   \n",
       "10755         0.100062               15.0          0.199506        1.999979   \n",
       "10670         0.092871                9.0          0.272045        1.688671   \n",
       "9994          0.120188               19.0          0.200000        2.358776   \n",
       "7242          0.105320                9.0          0.157442        1.584946   \n",
       "626           0.064865               14.0          0.175976        2.109911   \n",
       "7115          0.040000               16.0          0.239167        2.321877   \n",
       "\n",
       "       trigram_entropy  bigram_diversity  trigram_diversity  \n",
       "9906          0.000000          0.008197           0.008264  \n",
       "9547          3.503450          0.079167           0.092050  \n",
       "5952          3.392755          0.048851           0.057637  \n",
       "9057          2.584929          0.014320           0.014354  \n",
       "8912          3.127343          0.040462           0.052174  \n",
       "8536          2.192122          0.033951           0.037152  \n",
       "1024          8.262095          1.000000           1.000000  \n",
       "8521          2.623900          0.048571           0.054441  \n",
       "8200          0.000000          0.000000           0.000000  \n",
       "4400          2.321904          0.014409           0.014451  \n",
       "7922          2.167781          0.033846           0.033951  \n",
       "4467          2.321904          0.014409           0.014451  \n",
       "4588          2.119485          0.052083           0.063158  \n",
       "7828          2.342442          0.107692           0.116279  \n",
       "10755         1.999972          0.012384           0.012422  \n",
       "10670         1.744479          0.020690           0.027682  \n",
       "9994          2.358839          0.016432           0.016471  \n",
       "7242          1.584946          0.010239           0.010274  \n",
       "626           2.354599          0.034130           0.047945  \n",
       "7115          2.321852          0.020921           0.021008  \n",
       "\n",
       "[20 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "\n",
    "NUMERIC_COLS = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"bigram_entropy\",\"trigram_entropy\",\"bigram_diversity\",\"trigram_diversity\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\"whitespace_ratio\"\n",
    "]\n",
    "\n",
    "def ensure_numeric(df: pd.DataFrame, cols=NUMERIC_COLS) -> pd.DataFrame:\n",
    "    \"\"\"Coerce known feature columns to numeric (if present).\"\"\"\n",
    "    df = df.copy()\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def _q(df, col, q):\n",
    "    return float(np.nanquantile(df[col].values, q)) if col in df else np.nan\n",
    "\n",
    "# -------------------------------\n",
    "# Thresholds and Diagnostics\n",
    "# -------------------------------\n",
    "\n",
    "def compute_diagnostic_thresholds(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Data-driven thresholds (quantile-based) + a few hard guards.\"\"\"\n",
    "    thr = {}\n",
    "    thr[\"asl_hi\"]       = max(80.0, _q(df, \"avg_sentence_length\", 0.99))\n",
    "    thr[\"sls_hi\"]       = max(80.0, _q(df, \"sentence_length_std\", 0.99))\n",
    "    thr[\"fog_hi\"]       = max(60.0, _q(df, \"gunning_fog\", 0.995))\n",
    "    thr[\"ari_hi\"]       = max(60.0, _q(df, \"automated_readability_index\", 0.995))\n",
    "    thr[\"fre_lo\"]       = min(-50.0, _q(df, \"flesch_reading_ease\", 0.005))\n",
    "    thr[\"mtld_hi\"]      = max(500.0, _q(df, \"mtld\", 0.995))\n",
    "    thr[\"yk_hi\"]        = max(1000.0, _q(df, \"yules_k\", 0.995))\n",
    "    thr[\"depth_max_hi\"] = max(50.0, _q(df, \"max_tree_depth\", 0.995))\n",
    "    thr[\"depth_avg_hi\"] = max(25.0, _q(df, \"avg_tree_depth\", 0.995))\n",
    "    thr[\"depdist_hi\"]   = max(8.0, _q(df, \"avg_dependency_distance\", 0.995))\n",
    "    thr[\"comp_hi\"]      = 1.05\n",
    "    thr[\"upper_hi\"]     = max(0.15, _q(df, \"uppercase_ratio\", 0.995))\n",
    "    thr[\"uniq_hi\"]      = max(80.0, _q(df, \"unique_char_count\", 0.995))\n",
    "    thr[\"ws_lo\"]        = min(0.12, _q(df, \"whitespace_ratio\", 0.005))\n",
    "    thr[\"ws_hi\"]        = max(0.25, _q(df, \"whitespace_ratio\", 0.995))\n",
    "    return thr\n",
    "\n",
    "def diagnose_feature_outliers(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return diagnostics dataframe with boolean flags and suspected causes list.\"\"\"\n",
    "    # Ensure numeric so comparisons fire correctly\n",
    "    df = ensure_numeric(df)\n",
    "\n",
    "    thr = compute_diagnostic_thresholds(df)\n",
    "    D = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Flag definitions\n",
    "    D[\"seg_len_extreme\"] = (\n",
    "        (df.get(\"avg_sentence_length\", np.nan) > thr[\"asl_hi\"]) |\n",
    "        (df.get(\"sentence_length_std\", np.nan) > thr[\"sls_hi\"])\n",
    "    )\n",
    "    D[\"readability_outlier\"] = (\n",
    "        (df.get(\"flesch_reading_ease\", np.nan) < thr[\"fre_lo\"]) |\n",
    "        (df.get(\"gunning_fog\", np.nan) > thr[\"fog_hi\"]) |\n",
    "        (df.get(\"automated_readability_index\", np.nan) > thr[\"ari_hi\"])\n",
    "    )\n",
    "    D[\"lexical_instability\"] = (\n",
    "        (df.get(\"mtld\", np.nan) > thr[\"mtld_hi\"]) |\n",
    "        (df.get(\"yules_k\", np.nan) > thr[\"yk_hi\"])\n",
    "    )\n",
    "    D[\"ngram_edge_effects\"] = (\n",
    "        (df.get(\"bigram_entropy\", np.nan) == 0) |\n",
    "        (df.get(\"trigram_entropy\", np.nan) == 0) |\n",
    "        ((df.get(\"trigram_diversity\", np.nan) >= 0.99) & (df.get(\"trigram_entropy\", np.nan) < 1.0)) |\n",
    "        ((df.get(\"bigram_diversity\", np.nan)   >= 0.99) & (df.get(\"bigram_entropy\", np.nan)   < 1.0))\n",
    "    )\n",
    "    D[\"depth_implausible\"] = (\n",
    "        (df.get(\"max_tree_depth\", np.nan) > thr[\"depth_max_hi\"]) |\n",
    "        (df.get(\"avg_tree_depth\", np.nan) > thr[\"depth_avg_hi\"])\n",
    "    )\n",
    "    D[\"dep_distance_implausible\"] = (df.get(\"avg_dependency_distance\", np.nan) > thr[\"depdist_hi\"])\n",
    "    D[\"compression_overhead\"] = (df.get(\"compression_ratio\", np.nan) > thr[\"comp_hi\"])\n",
    "    D[\"markup_noise\"] = (\n",
    "        ((df.get(\"uppercase_ratio\", np.nan) > thr[\"upper_hi\"]) & (df.get(\"unique_char_count\", np.nan) > thr[\"uniq_hi\"])) |\n",
    "        (df.get(\"whitespace_ratio\", np.nan) < thr[\"ws_lo\"]) |\n",
    "        (df.get(\"whitespace_ratio\", np.nan) > thr[\"ws_hi\"])\n",
    "    )\n",
    "\n",
    "    # Suspected causes column (pre-allocate + .at)\n",
    "    D[\"suspected_causes\"] = pd.Series([[] for _ in range(len(D))], index=D.index, dtype=object)\n",
    "    for i in D.index:\n",
    "        c = []\n",
    "        if bool(D.at[i, \"depth_implausible\"]) or bool(D.at[i, \"dep_distance_implausible\"]):\n",
    "            c.append(\"dependency_depth_computation_bug\")\n",
    "        if bool(D.at[i, \"seg_len_extreme\"]) and bool(D.at[i, \"readability_outlier\"]):\n",
    "            c.append(\"sentence_segmentation_failure\")\n",
    "        if bool(D.at[i, \"lexical_instability\"]) or bool(D.at[i, \"ngram_edge_effects\"]):\n",
    "            c.append(\"lexical_metric_instability_or_length_effects\")\n",
    "        if bool(D.at[i, \"compression_overhead\"]) or bool(D.at[i, \"markup_noise\"]):\n",
    "            c.append(\"markup_or_code_noise\")\n",
    "        D.at[i, \"suspected_causes\"] = c\n",
    "\n",
    "    # Severity score\n",
    "    flag_cols = [c for c in D.columns if c != \"suspected_causes\"]\n",
    "    for c in flag_cols:\n",
    "        D[c] = D[c].fillna(False).astype(bool)\n",
    "    D[\"severity\"] = D[flag_cols].sum(axis=1).astype(int)\n",
    "\n",
    "    # Attach thresholds for later inspection\n",
    "    D.attrs[\"thresholds\"] = thr\n",
    "    return D\n",
    "\n",
    "def build_diagnostic_report(diag: pd.DataFrame, top_k: int = 15):\n",
    "    \"\"\"Summarize counts by cause and return top offending rows by severity.\"\"\"\n",
    "    cause_counts = (\n",
    "        diag[\"suspected_causes\"].explode().value_counts(dropna=True)\n",
    "        .rename_axis(\"cause\").to_frame(\"count\")\n",
    "    )\n",
    "    flag_counts = (\n",
    "        diag.drop(columns=[\"suspected_causes\"])\n",
    "            .sum()\n",
    "            .sort_values(ascending=False)\n",
    "            .to_frame(\"count\")\n",
    "    )\n",
    "    top_offenders = diag.sort_values(\"severity\", ascending=False).head(top_k)\n",
    "    return {\"cause_counts\": cause_counts, \"flag_counts\": flag_counts, \"top_offenders\": top_offenders}\n",
    "\n",
    "# -------------------------------\n",
    "# Runner / Example usage\n",
    "# -------------------------------\n",
    "\n",
    "# Assume you already have `df` with your features\n",
    "# If your features were just computed and may be strings, the coercion inside\n",
    "# diagnose_feature_outliers will handle them; coercing here is optional:\n",
    "# df = ensure_numeric(df)\n",
    "\n",
    "diag = diagnose_feature_outliers(df)\n",
    "rep  = build_diagnostic_report(diag, top_k=20)\n",
    "\n",
    "print(\"Computed thresholds:\")\n",
    "for k, v in diag.attrs.get(\"thresholds\", {}).items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"\\nCause counts:\")\n",
    "display(rep[\"cause_counts\"])\n",
    "\n",
    "print(\"\\nFlag counts:\")\n",
    "display(rep[\"flag_counts\"])\n",
    "\n",
    "print(\"\\nTop offenders (with original feature values):\")\n",
    "cols_to_show = [\n",
    "    \"avg_sentence_length\",\"sentence_length_std\",\"flesch_reading_ease\",\n",
    "    \"gunning_fog\",\"automated_readability_index\",\"mtld\",\"yules_k\",\n",
    "    \"avg_tree_depth\",\"max_tree_depth\",\"avg_dependency_distance\",\n",
    "    \"compression_ratio\",\"uppercase_ratio\",\"unique_char_count\",\n",
    "    \"whitespace_ratio\",\"bigram_entropy\",\"trigram_entropy\",\n",
    "    \"bigram_diversity\",\"trigram_diversity\"\n",
    "]\n",
    "existing_cols = [c for c in cols_to_show if c in df.columns]\n",
    "display(rep[\"top_offenders\"].join(df[existing_cols], how=\"left\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
