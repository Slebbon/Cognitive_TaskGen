{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-m6ch17IVR3",
        "outputId": "29969f49-042d-49eb-f9fa-59f1cfed6495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "‚úì All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: INSTALLATIONS\n",
        "# ============================================================================\n",
        "print(\"Installing required packages...\")\n",
        "\n",
        "!pip install -q xgboost scikit-learn\n",
        "\n",
        "print(\"‚úì All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 2: IMPORTS\n",
        "# ============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import (\n",
        "    StratifiedKFold, cross_val_score, train_test_split\n",
        ")\n",
        "from sklearn.metrics import (\n",
        "    f1_score, accuracy_score, roc_auc_score,\n",
        "    precision_score, recall_score, classification_report,\n",
        "    confusion_matrix, roc_curve\n",
        ")\n",
        "import time\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"‚úì All imports successful!\")\n",
        "print(f\"XGBoost version: {xgb.__version__}\")\n",
        "print(f\"Running on: CPU (optimal for tree-based models)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uz2jCxlSIapP",
        "outputId": "8dd5bbc9-3b78-4b74-a98e-e6a30184e133"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All imports successful!\n",
            "XGBoost version: 3.0.5\n",
            "Running on: CPU (optimal for tree-based models)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 3: DEFINE FEATURE SET\n",
        "# ============================================================================\n",
        "\n",
        "expected_features = [\n",
        "    'trigram_diversity', 'yules_k', 'token_burstiness', 'char_trigram_entropy',\n",
        "    'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance',\n",
        "    'uppercase_ratio', 'whitespace_ratio', 'unique_char_count',\n",
        "    'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio',\n",
        "    'semicolon_ratio', 'colon_ratio', 'quote_ratio',\n",
        "    'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance',\n",
        "    'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio',\n",
        "    'pos_ratio_DET', 'pos_ratio_ADP', 'pos_ratio_AUX', 'pos_ratio_CCONJ',\n",
        "    'pos_ratio_PART', 'pos_ratio_NUM', 'pos_row_entropy_weighted',\n",
        "    'function_to_content_rate', 'noun_verb_alternation_rate', 'content_function_ratio',\n",
        "    'noun_verb_ratio', 'adj_adv_ratio', 'verbs_per_100_tok', 'nouns_per_100_tok',\n",
        "    'adj_per_100_tok', 'adv_per_100_tok', 'pron_per_100_tok', 'punct_per_100_tok',\n",
        "    'tokens_per_sentence_mean', 'mean_nouns_per_sent', 'mean_verbs_per_sent',\n",
        "    'mean_adjs_per_sent', 'mean_advs_per_sent', 'prop_sents_with_verb',\n",
        "    'unique_upos_per_sent_mean', 'max_runlen_NOUN', 'max_runlen_PUNCT',\n",
        "    'avg_sentence_length', 'sentence_length_std', 'n_sentences_doc'\n",
        "]\n",
        "\n",
        "print(f\"‚úì Feature set defined: {len(expected_features)} features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3A9UxuBIcfY",
        "outputId": "22e18226-d381-4a33-8e2c-f2bc1de8cf26"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Feature set defined: 53 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 4: LOAD DATA\n",
        "# ============================================================================\n",
        "\n",
        "# Adjust this to your actual data loading method\n",
        "df = pd.read_csv(r'/content/raid_sample_large_PostPOS_CLEAN.csv')\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Class distribution:\\n{df['is_ai'].value_counts()}\")\n",
        "\n",
        "# Prepare data\n",
        "X = df[expected_features].values\n",
        "y = df['is_ai'].astype(int).values\n",
        "\n",
        "print(f\"\\nData prepared:\")\n",
        "print(f\"  Features: {X.shape}\")\n",
        "print(f\"  Labels: {y.shape}\")\n",
        "print(f\"  Class distribution: AI={y.sum()}, Human={len(y)-y.sum()}\")\n",
        "\n",
        "# Create train/test split\n",
        "print(f\"\\nCreating 80/20 train/test split...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"  Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"  Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\n‚úì Data ready for optimization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIOQdzf1aR2G",
        "outputId": "ef9c6829-602a-45bb-9fee-fe578bf001bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (60000, 86)\n",
            "Class distribution:\n",
            "is_ai\n",
            "True     30000\n",
            "False    30000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data prepared:\n",
            "  Features: (60000, 53)\n",
            "  Labels: (60000,)\n",
            "  Class distribution: AI=30000, Human=30000\n",
            "\n",
            "Creating 80/20 train/test split...\n",
            "  Training set: 48000 samples\n",
            "  Test set: 12000 samples\n",
            "\n",
            "‚úì Data ready for optimization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 5: SETUP CHECKPOINT SYSTEM\n",
        "# ============================================================================\n",
        "\n",
        "# Create checkpoint directory\n",
        "checkpoint_dir = Path('xgb_optimization')\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"‚úì Checkpoint directory created: {checkpoint_dir}/\")\n",
        "print(\"  All progress will be saved automatically\")\n",
        "print(\"  You can safely resume if interrupted\\n\")\n",
        "\n",
        "# Global timer\n",
        "notebook_start_time = time.time()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFtaSkMYaT81",
        "outputId": "21254007-6e05-4c8f-8241-72b2da88653a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Checkpoint directory created: xgb_optimization/\n",
            "  All progress will be saved automatically\n",
            "  You can safely resume if interrupted\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 6: COMPREHENSIVE XGBOOST SEARCH (ALL ON FULL TRAINING SET)\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"COMPREHENSIVE XGBOOST HYPERPARAMETER SEARCH\")\n",
        "print(\"=\"*70)\n",
        "print(\"Strategy: XGBoost is fast enough to test 30+ configs on full training set\")\n",
        "print(\"Expected total time: 10-15 minutes\\n\")\n",
        "\n",
        "checkpoint_dir = Path('xgb_comprehensive_search')\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# COMPREHENSIVE GRID: 30 strategic configurations\n",
        "configs = [\n",
        "    # ===== BASELINE VARIATIONS =====\n",
        "    {'name': 'baseline_100', 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'baseline_300', 'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'baseline_500', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    # ===== TREE COUNT √ó LEARNING RATE COMBINATIONS =====\n",
        "    {'name': 'many_trees_low_lr', 'n_estimators': 800, 'max_depth': 6, 'learning_rate': 0.05,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'very_many_low_lr', 'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.03,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'few_high_lr', 'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.3,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    # ===== DEPTH VARIATIONS =====\n",
        "    {'name': 'very_shallow', 'n_estimators': 500, 'max_depth': 3, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'shallow', 'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'medium_deep', 'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'deep', 'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'very_deep', 'n_estimators': 300, 'max_depth': 12, 'learning_rate': 0.05,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    # ===== SUBSAMPLING VARIATIONS =====\n",
        "    {'name': 'low_subsample', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.6, 'colsample_bytree': 0.6, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'med_subsample', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'high_subsample', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.95, 'colsample_bytree': 0.95, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'full_sample', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 1.0, 'colsample_bytree': 1.0, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    # ===== REGULARIZATION VARIATIONS =====\n",
        "    {'name': 'l1_light', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0.3, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'l1_strong', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 1.0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'l2_strong', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 3.0},\n",
        "\n",
        "    {'name': 'elastic_net', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0.5, 'reg_lambda': 2.0},\n",
        "\n",
        "    # ===== MIN CHILD WEIGHT VARIATIONS =====\n",
        "    {'name': 'min_child_3', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 3, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'min_child_5', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 5, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'min_child_10', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 10, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    # ===== GAMMA VARIATIONS =====\n",
        "    {'name': 'gamma_01', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0.1,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'gamma_03', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0.3,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    {'name': 'gamma_05', 'n_estimators': 400, 'max_depth': 6, 'learning_rate': 0.1,\n",
        "     'subsample': 0.8, 'colsample_bytree': 0.8, 'min_child_weight': 1, 'gamma': 0.5,\n",
        "     'reg_alpha': 0, 'reg_lambda': 1},\n",
        "\n",
        "    # ===== BALANCED COMBINATIONS =====\n",
        "    {'name': 'aggressive', 'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.15,\n",
        "     'subsample': 0.9, 'colsample_bytree': 0.9, 'min_child_weight': 1, 'gamma': 0,\n",
        "     'reg_alpha': 0, 'reg_lambda': 0.5},\n",
        "\n",
        "    {'name': 'conservative', 'n_estimators': 500, 'max_depth': 4, 'learning_rate': 0.05,\n",
        "     'subsample': 0.7, 'colsample_bytree': 0.7, 'min_child_weight': 5, 'gamma': 0.2,\n",
        "     'reg_alpha': 0.3, 'reg_lambda': 2.0},\n",
        "\n",
        "    {'name': 'balanced_fast', 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.15,\n",
        "     'subsample': 0.85, 'colsample_bytree': 0.85, 'min_child_weight': 2, 'gamma': 0.05,\n",
        "     'reg_alpha': 0.1, 'reg_lambda': 1.0},\n",
        "\n",
        "    {'name': 'balanced_accurate', 'n_estimators': 600, 'max_depth': 7, 'learning_rate': 0.08,\n",
        "     'subsample': 0.85, 'colsample_bytree': 0.85, 'min_child_weight': 2, 'gamma': 0.1,\n",
        "     'reg_alpha': 0.1, 'reg_lambda': 1.2},\n",
        "\n",
        "    {'name': 'optimal_candidate', 'n_estimators': 500, 'max_depth': 7, 'learning_rate': 0.08,\n",
        "     'subsample': 0.85, 'colsample_bytree': 0.85, 'min_child_weight': 1, 'gamma': 0.05,\n",
        "     'reg_alpha': 0.05, 'reg_lambda': 1.0},\n",
        "]\n",
        "\n",
        "print(f\"Testing {len(configs)} configurations\")\n",
        "print(f\"Estimated time: {len(configs) * 0.34:.1f} minutes\\n\")\n",
        "\n",
        "# Checkpoint system\n",
        "checkpoint_file = checkpoint_dir / 'all_results.pkl'\n",
        "\n",
        "if checkpoint_file.exists():\n",
        "    with open(checkpoint_file, 'rb') as f:\n",
        "        results = pickle.load(f)\n",
        "    print(f\" Loaded {len(results)} completed configurations\\n\")\n",
        "else:\n",
        "    results = []\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Test all configurations\n",
        "for idx, config in enumerate(configs, 1):\n",
        "    name = config.pop('name')\n",
        "\n",
        "    # Skip if already done\n",
        "    if any(r['name'] == name for r in results):\n",
        "        print(f\"[{idx}/{len(configs)}] ‚è≠  '{name}' already completed\")\n",
        "        continue\n",
        "\n",
        "    print(f\"[{idx}/{len(configs)}] Testing: {name}\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        random_state=42, n_jobs=-1,\n",
        "        eval_metric='logloss', verbosity=0,\n",
        "        **config\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "    scores = cross_val_score(\n",
        "        model, X_train, y_train,\n",
        "        cv=cv, scoring='f1',\n",
        "        n_jobs=-1, verbose=0\n",
        "    )\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    result = {\n",
        "        'name': name,\n",
        "        'params': config,\n",
        "        'f1_mean': scores.mean(),\n",
        "        'f1_std': scores.std(),\n",
        "        'f1_scores': scores.tolist(),\n",
        "        'time_seconds': elapsed\n",
        "    }\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "    print(f\"  ‚úì F1: {scores.mean():.4f} ¬± {scores.std():.4f}  ({elapsed:.0f}s)\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    with open(checkpoint_file, 'wb') as f:\n",
        "        pickle.dump(results, f)\n",
        "\n",
        "    # Show progress\n",
        "    completed = len(results)\n",
        "    avg_time = (time.time() - start_time) / completed\n",
        "    remaining = (len(configs) - completed) * avg_time\n",
        "    if remaining > 0:\n",
        "        print(f\"  ‚è±  Est. {remaining/60:.1f} min remaining\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "\n",
        "# Display results\n",
        "df_results = pd.DataFrame(results).sort_values('f1_mean', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMPREHENSIVE SEARCH COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total time: {total_time/60:.1f} minutes\")\n",
        "print(f\"Configs tested: {len(results)}\\n\")\n",
        "\n",
        "print(\"TOP 10 CONFIGURATIONS:\")\n",
        "print(df_results[['name', 'f1_mean', 'f1_std']].head(10).to_string(index=False))\n",
        "\n",
        "best = df_results.iloc[0]\n",
        "print(f\"\\n WINNER: {best['name']}\")\n",
        "print(f\"   F1 Score: {best['f1_mean']:.4f} ¬± {best['f1_std']:.4f}\")\n",
        "print(f\"   Parameters: {best['params']}\")\n",
        "\n",
        "# Save full results\n",
        "df_results.to_csv('xgboost_comprehensive_results.csv', index=False)\n",
        "print(f\"\\n‚úì Full results saved to 'xgboost_comprehensive_results.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUljfAxfaGS-",
        "outputId": "75fdf2fe-e773-4d51-e1e9-9e765a135aa3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "COMPREHENSIVE XGBOOST HYPERPARAMETER SEARCH\n",
            "======================================================================\n",
            "Strategy: XGBoost is fast enough to test 30+ configs on full training set\n",
            "Expected total time: 10-15 minutes\n",
            "\n",
            "Testing 30 configurations\n",
            "Estimated time: 10.2 minutes\n",
            "\n",
            "[1/30] Testing: baseline_100\n",
            "  ‚úì F1: 0.8818 ¬± 0.0015  (13s)\n",
            "  ‚è±  Est. 6.5 min remaining\n",
            "[2/30] Testing: baseline_300\n",
            "  ‚úì F1: 0.9056 ¬± 0.0018  (17s)\n",
            "  ‚è±  Est. 7.1 min remaining\n",
            "[3/30] Testing: baseline_500\n",
            "  ‚úì F1: 0.9123 ¬± 0.0005  (28s)\n",
            "  ‚è±  Est. 8.8 min remaining\n",
            "[4/30] Testing: many_trees_low_lr\n",
            "  ‚úì F1: 0.9119 ¬± 0.0010  (44s)\n",
            "  ‚è±  Est. 11.1 min remaining\n",
            "[5/30] Testing: very_many_low_lr\n",
            "  ‚úì F1: 0.9077 ¬± 0.0006  (53s)\n",
            "  ‚è±  Est. 13.0 min remaining\n",
            "[6/30] Testing: few_high_lr\n",
            "  ‚úì F1: 0.8838 ¬± 0.0016  (3s)\n",
            "  ‚è±  Est. 10.6 min remaining\n",
            "[7/30] Testing: very_shallow\n",
            "  ‚úì F1: 0.8777 ¬± 0.0016  (10s)\n",
            "  ‚è±  Est. 9.2 min remaining\n",
            "[8/30] Testing: shallow\n",
            "  ‚úì F1: 0.8947 ¬± 0.0007  (13s)\n",
            "  ‚è±  Est. 8.4 min remaining\n",
            "[9/30] Testing: medium_deep\n",
            "  ‚úì F1: 0.9130 ¬± 0.0026  (31s)\n",
            "  ‚è±  Est. 8.3 min remaining\n",
            "[10/30] Testing: deep\n",
            "  ‚úì F1: 0.9143 ¬± 0.0021  (48s)\n",
            "  ‚è±  Est. 8.7 min remaining\n",
            "[11/30] Testing: very_deep\n",
            "  ‚úì F1: 0.9139 ¬± 0.0009  (81s)\n",
            "  ‚è±  Est. 9.9 min remaining\n",
            "[12/30] Testing: low_subsample\n",
            "  ‚úì F1: 0.9086 ¬± 0.0021  (21s)\n",
            "  ‚è±  Est. 9.1 min remaining\n",
            "[13/30] Testing: med_subsample\n",
            "  ‚úì F1: 0.9088 ¬± 0.0006  (22s)\n",
            "  ‚è±  Est. 8.4 min remaining\n",
            "[14/30] Testing: high_subsample\n",
            "  ‚úì F1: 0.9087 ¬± 0.0010  (24s)\n",
            "  ‚è±  Est. 7.8 min remaining\n",
            "[15/30] Testing: full_sample\n",
            "  ‚úì F1: 0.9085 ¬± 0.0011  (25s)\n",
            "  ‚è±  Est. 7.3 min remaining\n",
            "[16/30] Testing: l1_light\n",
            "  ‚úì F1: 0.9089 ¬± 0.0012  (22s)\n",
            "  ‚è±  Est. 6.7 min remaining\n",
            "[17/30] Testing: l1_strong\n",
            "  ‚úì F1: 0.9097 ¬± 0.0024  (24s)\n",
            "  ‚è±  Est. 6.1 min remaining\n",
            "[18/30] Testing: l2_strong\n",
            "  ‚úì F1: 0.9099 ¬± 0.0015  (23s)\n",
            "  ‚è±  Est. 5.6 min remaining\n",
            "[19/30] Testing: elastic_net\n",
            "  ‚úì F1: 0.9083 ¬± 0.0013  (23s)\n",
            "  ‚è±  Est. 5.1 min remaining\n",
            "[20/30] Testing: min_child_3\n",
            "  ‚úì F1: 0.9075 ¬± 0.0016  (20s)\n",
            "  ‚è±  Est. 4.6 min remaining\n",
            "[21/30] Testing: min_child_5\n",
            "  ‚úì F1: 0.9080 ¬± 0.0024  (18s)\n",
            "  ‚è±  Est. 4.0 min remaining\n",
            "[22/30] Testing: min_child_10\n",
            "  ‚úì F1: 0.9063 ¬± 0.0011  (17s)\n",
            "  ‚è±  Est. 3.5 min remaining\n",
            "[23/30] Testing: gamma_01\n",
            "  ‚úì F1: 0.9088 ¬± 0.0015  (22s)\n",
            "  ‚è±  Est. 3.1 min remaining\n",
            "[24/30] Testing: gamma_03\n",
            "  ‚úì F1: 0.9099 ¬± 0.0013  (23s)\n",
            "  ‚è±  Est. 2.6 min remaining\n",
            "[25/30] Testing: gamma_05\n",
            "  ‚úì F1: 0.9095 ¬± 0.0007  (22s)\n",
            "  ‚è±  Est. 2.2 min remaining\n",
            "[26/30] Testing: aggressive\n",
            "  ‚úì F1: 0.9170 ¬± 0.0012  (51s)\n",
            "  ‚è±  Est. 1.8 min remaining\n",
            "[27/30] Testing: conservative\n",
            "  ‚úì F1: 0.8783 ¬± 0.0016  (12s)\n",
            "  ‚è±  Est. 1.3 min remaining\n",
            "[28/30] Testing: balanced_fast\n",
            "  ‚úì F1: 0.8950 ¬± 0.0004  (9s)\n",
            "  ‚è±  Est. 0.9 min remaining\n",
            "[29/30] Testing: balanced_accurate\n",
            "  ‚úì F1: 0.9131 ¬± 0.0017  (42s)\n",
            "  ‚è±  Est. 0.4 min remaining\n",
            "[30/30] Testing: optimal_candidate\n",
            "  ‚úì F1: 0.9140 ¬± 0.0009  (40s)\n",
            "\n",
            "======================================================================\n",
            "COMPREHENSIVE SEARCH COMPLETE\n",
            "======================================================================\n",
            "Total time: 13.4 minutes\n",
            "Configs tested: 30\n",
            "\n",
            "TOP 10 CONFIGURATIONS:\n",
            "             name  f1_mean   f1_std\n",
            "       aggressive 0.916973 0.001189\n",
            "             deep 0.914289 0.002052\n",
            "optimal_candidate 0.913977 0.000895\n",
            "        very_deep 0.913925 0.000896\n",
            "balanced_accurate 0.913108 0.001671\n",
            "      medium_deep 0.913038 0.002574\n",
            "     baseline_500 0.912272 0.000478\n",
            "many_trees_low_lr 0.911864 0.000996\n",
            "         gamma_03 0.909883 0.001350\n",
            "        l2_strong 0.909867 0.001501\n",
            "\n",
            " WINNER: aggressive\n",
            "   F1 Score: 0.9170 ¬± 0.0012\n",
            "   Parameters: {'n_estimators': 500, 'max_depth': 9, 'learning_rate': 0.15, 'subsample': 0.9, 'colsample_bytree': 0.9, 'min_child_weight': 1, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 0.5}\n",
            "\n",
            "‚úì Full results saved to 'xgboost_comprehensive_results.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 7: FINAL VALIDATION WITH 5-FOLD CV\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL VALIDATION: TOP 3 CONFIGS WITH 5-FOLD CV\")\n",
        "print(\"=\"*70)\n",
        "print(\"Testing top 3 configurations with more robust 5-fold CV\\n\")\n",
        "\n",
        "# Get top 3 from comprehensive search\n",
        "top3 = df_results.head(3)\n",
        "\n",
        "cv_final = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "final_validation_results = []\n",
        "\n",
        "for idx, row in top3.iterrows():\n",
        "    name = row['name']\n",
        "    params = row['params']\n",
        "\n",
        "    print(f\"\\n{'‚îÄ'*70}\")\n",
        "    print(f\"Validating: {name}\")\n",
        "    print(f\"{'‚îÄ'*70}\")\n",
        "    print(f\"3-fold F1: {row['f1_mean']:.4f} ¬± {row['f1_std']:.4f}\")\n",
        "\n",
        "    model = XGBClassifier(\n",
        "        random_state=42, n_jobs=-1,\n",
        "        eval_metric='logloss', verbosity=0,\n",
        "        **params\n",
        "    )\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Get comprehensive metrics with 5-fold CV\n",
        "    scores_f1 = cross_val_score(model, X_train, y_train, cv=cv_final,\n",
        "                                scoring='f1', n_jobs=-1, verbose=0)\n",
        "    scores_acc = cross_val_score(model, X_train, y_train, cv=cv_final,\n",
        "                                 scoring='accuracy', n_jobs=-1, verbose=0)\n",
        "    scores_auc = cross_val_score(model, X_train, y_train, cv=cv_final,\n",
        "                                 scoring='roc_auc', n_jobs=-1, verbose=0)\n",
        "    scores_prec = cross_val_score(model, X_train, y_train, cv=cv_final,\n",
        "                                  scoring='precision', n_jobs=-1, verbose=0)\n",
        "    scores_rec = cross_val_score(model, X_train, y_train, cv=cv_final,\n",
        "                                 scoring='recall', n_jobs=-1, verbose=0)\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "\n",
        "    result = {\n",
        "        'name': name,\n",
        "        'params': params,\n",
        "        'f1_mean': scores_f1.mean(),\n",
        "        'f1_std': scores_f1.std(),\n",
        "        'accuracy_mean': scores_acc.mean(),\n",
        "        'accuracy_std': scores_acc.std(),\n",
        "        'roc_auc_mean': scores_auc.mean(),\n",
        "        'roc_auc_std': scores_auc.std(),\n",
        "        'precision_mean': scores_prec.mean(),\n",
        "        'precision_std': scores_prec.std(),\n",
        "        'recall_mean': scores_rec.mean(),\n",
        "        'recall_std': scores_rec.std(),\n",
        "        'time_seconds': elapsed\n",
        "    }\n",
        "\n",
        "    final_validation_results.append(result)\n",
        "\n",
        "    print(f\"\\n5-fold CV Results:\")\n",
        "    print(f\"  F1 Score:  {scores_f1.mean():.4f} ¬± {scores_f1.std():.4f}\")\n",
        "    print(f\"  Accuracy:  {scores_acc.mean():.4f} ¬± {scores_acc.std():.4f}\")\n",
        "    print(f\"  ROC-AUC:   {scores_auc.mean():.4f} ¬± {scores_auc.std():.4f}\")\n",
        "    print(f\"  Precision: {scores_prec.mean():.4f} ¬± {scores_prec.std():.4f}\")\n",
        "    print(f\"  Recall:    {scores_rec.mean():.4f} ¬± {scores_rec.std():.4f}\")\n",
        "    print(f\"  Time: {elapsed:.1f}s\")\n",
        "\n",
        "# Determine final winner\n",
        "final_df = pd.DataFrame(final_validation_results).sort_values('f1_mean', ascending=False)\n",
        "winner = final_df.iloc[0]\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL VALIDATION COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nRankings (5-fold CV):\")\n",
        "for idx, row in final_df.iterrows():\n",
        "    print(f\"  {row['name']:<20}: F1={row['f1_mean']:.4f} ¬± {row['f1_std']:.4f}\")\n",
        "\n",
        "print(f\"\\n FINAL WINNER: {winner['name']}\")\n",
        "print(f\"   F1 Score: {winner['f1_mean']:.4f} ¬± {winner['f1_std']:.4f}\")\n",
        "print(f\"   Accuracy: {winner['accuracy_mean']:.4f} ¬± {winner['accuracy_std']:.4f}\")\n",
        "print(f\"   ROC-AUC:  {winner['roc_auc_mean']:.4f} ¬± {winner['roc_auc_std']:.4f}\")\n",
        "\n",
        "# Save final validation results\n",
        "final_df.to_csv('xgboost_final_validation.csv', index=False)\n",
        "print(f\"\\n‚úì Final validation results saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P_ZwM80adF8",
        "outputId": "da2bf102-70bc-46df-fda8-ebb41b951b71"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FINAL VALIDATION: TOP 3 CONFIGS WITH 5-FOLD CV\n",
            "======================================================================\n",
            "Testing top 3 configurations with more robust 5-fold CV\n",
            "\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Validating: aggressive\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "3-fold F1: 0.9170 ¬± 0.0012\n",
            "\n",
            "5-fold CV Results:\n",
            "  F1 Score:  0.9206 ¬± 0.0030\n",
            "  Accuracy:  0.9221 ¬± 0.0029\n",
            "  ROC-AUC:   0.9787 ¬± 0.0010\n",
            "  Precision: 0.9390 ¬± 0.0031\n",
            "  Recall:    0.9028 ¬± 0.0035\n",
            "  Time: 482.9s\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Validating: deep\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "3-fold F1: 0.9143 ¬± 0.0021\n",
            "\n",
            "5-fold CV Results:\n",
            "  F1 Score:  0.9190 ¬± 0.0034\n",
            "  Accuracy:  0.9206 ¬± 0.0034\n",
            "  ROC-AUC:   0.9783 ¬± 0.0009\n",
            "  Precision: 0.9382 ¬± 0.0033\n",
            "  Recall:    0.9006 ¬± 0.0038\n",
            "  Time: 443.7s\n",
            "\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "Validating: optimal_candidate\n",
            "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
            "3-fold F1: 0.9140 ¬± 0.0009\n",
            "\n",
            "5-fold CV Results:\n",
            "  F1 Score:  0.9186 ¬± 0.0025\n",
            "  Accuracy:  0.9202 ¬± 0.0025\n",
            "  ROC-AUC:   0.9764 ¬± 0.0010\n",
            "  Precision: 0.9382 ¬± 0.0035\n",
            "  Recall:    0.8997 ¬± 0.0018\n",
            "  Time: 361.2s\n",
            "\n",
            "======================================================================\n",
            "FINAL VALIDATION COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Rankings (5-fold CV):\n",
            "  aggressive          : F1=0.9206 ¬± 0.0030\n",
            "  deep                : F1=0.9190 ¬± 0.0034\n",
            "  optimal_candidate   : F1=0.9186 ¬± 0.0025\n",
            "\n",
            " FINAL: aggressive\n",
            "   F1 Score: 0.9206 ¬± 0.0030\n",
            "   Accuracy: 0.9221 ¬± 0.0029\n",
            "   ROC-AUC:  0.9787 ¬± 0.0010\n",
            "\n",
            "‚úì Final validation results saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 8: TEST SET EVALUATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST SET EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Training final model: {winner['name']}\")\n",
        "print(f\"Parameters: {winner['params']}\\n\")\n",
        "\n",
        "# Train final model on full training set\n",
        "final_model = XGBClassifier(\n",
        "    random_state=42, n_jobs=-1,\n",
        "    eval_metric='logloss', verbosity=0,\n",
        "    **winner['params']\n",
        ")\n",
        "\n",
        "print(f\"Training on {len(X_train)} samples...\")\n",
        "final_model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Evaluating on {len(X_test)} test samples...\")\n",
        "y_pred = final_model.predict(X_test)\n",
        "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate all metrics\n",
        "test_results = {\n",
        "    'accuracy': accuracy_score(y_test, y_pred),\n",
        "    'f1': f1_score(y_test, y_pred),\n",
        "    'roc_auc': roc_auc_score(y_test, y_proba),\n",
        "    'precision': precision_score(y_test, y_pred),\n",
        "    'recall': recall_score(y_test, y_pred)\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TEST SET RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"  Accuracy:  {test_results['accuracy']:.4f}\")\n",
        "print(f\"  F1 Score:  {test_results['f1']:.4f}\")\n",
        "print(f\"  ROC-AUC:   {test_results['roc_auc']:.4f}\")\n",
        "print(f\"  Precision: {test_results['precision']:.4f}\")\n",
        "print(f\"  Recall:    {test_results['recall']:.4f}\")\n",
        "\n",
        "# Generalization check\n",
        "cv_test_gap = abs(winner['f1_mean'] - test_results['f1'])\n",
        "print(f\"\\nüìä Generalization Analysis:\")\n",
        "print(f\"  CV F1:     {winner['f1_mean']:.4f}\")\n",
        "print(f\"  Test F1:   {test_results['f1']:.4f}\")\n",
        "print(f\"  Gap:       {cv_test_gap:.4f} ({cv_test_gap/winner['f1_mean']*100:.2f}%)\")\n",
        "\n",
        "if cv_test_gap < 0.01:\n",
        "    print(f\"  ‚úì EXCELLENT: Model generalizes very well\")\n",
        "elif cv_test_gap < 0.02:\n",
        "    print(f\"  ‚úì GOOD: Acceptable generalization\")\n",
        "else:\n",
        "    print(f\"  ‚ö† WARNING: Significant generalization gap\")\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"Classification Report:\")\n",
        "print(\"-\"*70)\n",
        "print(classification_report(y_test, y_pred,\n",
        "                          target_names=['Human', 'AI'],\n",
        "                          digits=4))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(f\"                 Predicted\")\n",
        "print(f\"                 Human      AI\")\n",
        "print(f\"Actual Human   {cm[0,0]:6d}  {cm[0,1]:6d}\")\n",
        "print(f\"       AI      {cm[1,0]:6d}  {cm[1,1]:6d}\")\n",
        "\n",
        "# Error analysis\n",
        "human_as_ai = cm[0, 1]\n",
        "ai_as_human = cm[1, 0]\n",
        "total_human = cm[0].sum()\n",
        "total_ai = cm[1].sum()\n",
        "\n",
        "print(f\"\\nüìà Error Analysis:\")\n",
        "print(f\"  False Positives (Human‚ÜíAI): {human_as_ai:5d} ({human_as_ai/total_human*100:.2f}%)\")\n",
        "print(f\"  False Negatives (AI‚ÜíHuman): {ai_as_human:5d} ({ai_as_human/total_ai*100:.2f}%)\")\n",
        "print(f\"  Total errors: {human_as_ai + ai_as_human:5d} ({(human_as_ai + ai_as_human)/len(y_test)*100:.2f}%)\")\n",
        "\n",
        "# Save final model\n",
        "final_model_path = checkpoint_dir / 'xgboost_optimized_final.pkl'\n",
        "with open(final_model_path, 'wb') as f:\n",
        "    pickle.dump(final_model, f)\n",
        "\n",
        "print(f\"\\n‚úì Final model saved to: {final_model_path}\")\n",
        "\n",
        "# Save test predictions for analysis\n",
        "test_results_df = pd.DataFrame({\n",
        "    'y_true': y_test,\n",
        "    'y_pred': y_pred,\n",
        "    'y_proba': y_proba\n",
        "})\n",
        "test_results_df.to_csv('test_predictions.csv', index=False)\n",
        "print(f\"‚úì Test predictions saved to: test_predictions.csv\")"
      ],
      "metadata": {
        "id": "l9enxAHO4cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CELL 9: COMPREHENSIVE VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"GENERATING COMPREHENSIVE VISUALIZATIONS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "# 1. Hyperparameter exploration heatmap (top 15 configs)\n",
        "ax1 = plt.subplot(3, 4, 1)\n",
        "top15 = df_results.head(15)\n",
        "heatmap_data = []\n",
        "for _, row in top15.iterrows():\n",
        "    heatmap_data.append([\n",
        "        row['params']['n_estimators'],\n",
        "        row['params']['max_depth'],\n",
        "        row['params']['learning_rate'] * 100,  # Scale for visibility\n",
        "        row['params']['subsample'] * 10,  # Scale for visibility\n",
        "    ])\n",
        "\n",
        "im = ax1.imshow(heatmap_data, aspect='auto', cmap='RdYlGn')\n",
        "ax1.set_yticks(range(len(top15)))\n",
        "ax1.set_yticklabels(top15['name'], fontsize=8)\n",
        "ax1.set_xticks(range(4))\n",
        "ax1.set_xticklabels(['n_est', 'depth', 'LR√ó100', 'sub√ó10'], fontsize=9)\n",
        "ax1.set_title('Top 15: Hyperparameter Patterns', fontweight='bold', fontsize=11)\n",
        "plt.colorbar(im, ax=ax1)\n",
        "\n",
        "# 2. F1 scores distribution\n",
        "ax2 = plt.subplot(3, 4, 2)\n",
        "ax2.hist(df_results['f1_mean'], bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "ax2.axvline(winner['f1_mean'], color='red', linestyle='--', linewidth=2,\n",
        "           label=f\"Winner: {winner['f1_mean']:.4f}\")\n",
        "ax2.set_xlabel('F1 Score', fontweight='bold')\n",
        "ax2.set_ylabel('Count', fontweight='bold')\n",
        "ax2.set_title('F1 Score Distribution (30 configs)', fontweight='bold', fontsize=11)\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Top 10 configs comparison\n",
        "ax3 = plt.subplot(3, 4, 3)\n",
        "top10 = df_results.head(10)\n",
        "bars = ax3.barh(range(10), top10['f1_mean'],\n",
        "               xerr=top10['f1_std'], capsize=5,\n",
        "               color=plt.cm.RdYlGn(np.linspace(0.5, 0.9, 10)), alpha=0.8)\n",
        "ax3.set_yticks(range(10))\n",
        "ax3.set_yticklabels(top10['name'], fontsize=9)\n",
        "ax3.set_xlabel('F1 Score', fontweight='bold')\n",
        "ax3.set_title('Top 10 Configurations', fontweight='bold', fontsize=11)\n",
        "ax3.invert_yaxis()\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "ax3.set_xlim([0.87, 0.92])\n",
        "\n",
        "# Add value labels\n",
        "for i, (bar, val) in enumerate(zip(bars, top10['f1_mean'])):\n",
        "    ax3.text(val, i, f' {val:.4f}', va='center', fontsize=8, fontweight='bold')\n",
        "\n",
        "# 4. CV vs Test comparison\n",
        "ax4 = plt.subplot(3, 4, 4)\n",
        "metrics = ['Accuracy', 'F1', 'ROC-AUC', 'Precision', 'Recall']\n",
        "cv_vals = [winner['accuracy_mean'], winner['f1_mean'], winner['roc_auc_mean'],\n",
        "           winner['precision_mean'], winner['recall_mean']]\n",
        "test_vals = [test_results['accuracy'], test_results['f1'], test_results['roc_auc'],\n",
        "            test_results['precision'], test_results['recall']]\n",
        "\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = ax4.bar(x - width/2, cv_vals, width, label='5-fold CV',\n",
        "               alpha=0.8, color='steelblue')\n",
        "bars2 = ax4.bar(x + width/2, test_vals, width, label='Test Set',\n",
        "               alpha=0.8, color='darkblue')\n",
        "\n",
        "ax4.set_ylabel('Score', fontweight='bold')\n",
        "ax4.set_title('CV vs Test Set Performance', fontweight='bold', fontsize=11)\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels(metrics, rotation=45, ha='right', fontsize=9)\n",
        "ax4.legend()\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "ax4.set_ylim([0.85, 1.0])\n",
        "\n",
        "# 5. ROC Curve\n",
        "ax5 = plt.subplot(3, 4, 5)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "ax5.plot(fpr, tpr, linewidth=2.5, color='darkblue',\n",
        "        label=f'XGBoost (AUC={test_results[\"roc_auc\"]:.4f})')\n",
        "ax5.plot([0, 1], [0, 1], 'k--', alpha=0.3, linewidth=1)\n",
        "ax5.set_xlabel('False Positive Rate', fontweight='bold')\n",
        "ax5.set_ylabel('True Positive Rate', fontweight='bold')\n",
        "ax5.set_title('ROC Curve (Test Set)', fontweight='bold', fontsize=11)\n",
        "ax5.legend(loc='lower right')\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Confusion Matrix\n",
        "ax6 = plt.subplot(3, 4, 6)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax6,\n",
        "           xticklabels=['Human', 'AI'], yticklabels=['Human', 'AI'],\n",
        "           cbar_kws={'label': 'Count'}, annot_kws={'fontsize': 12, 'fontweight': 'bold'})\n",
        "ax6.set_ylabel('True Label', fontweight='bold')\n",
        "ax6.set_xlabel('Predicted Label', fontweight='bold')\n",
        "ax6.set_title('Confusion Matrix', fontweight='bold', fontsize=11)\n",
        "\n",
        "# 7. Feature Importance (Top 20)\n",
        "ax7 = plt.subplot(3, 4, 7)\n",
        "importances = final_model.feature_importances_\n",
        "indices = np.argsort(importances)[-20:]\n",
        "top20_features = [expected_features[i] for i in indices]\n",
        "top20_importance = importances[indices]\n",
        "\n",
        "colors = plt.cm.viridis(np.linspace(0.3, 0.9, 20))\n",
        "ax7.barh(range(20), top20_importance, color=colors, alpha=0.8)\n",
        "ax7.set_yticks(range(20))\n",
        "ax7.set_yticklabels(top20_features, fontsize=8)\n",
        "ax7.set_xlabel('Importance', fontweight='bold')\n",
        "ax7.set_title('Top 20 Feature Importances', fontweight='bold', fontsize=11)\n",
        "ax7.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 8. Prediction confidence distribution\n",
        "ax8 = plt.subplot(3, 4, 8)\n",
        "human_probs = y_proba[y_test == 0]\n",
        "ai_probs = y_proba[y_test == 1]\n",
        "\n",
        "ax8.hist(human_probs, bins=30, alpha=0.6, label='Human texts', color='blue', edgecolor='black')\n",
        "ax8.hist(ai_probs, bins=30, alpha=0.6, label='AI texts', color='red', edgecolor='black')\n",
        "ax8.axvline(0.5, color='black', linestyle='--', linewidth=2, alpha=0.5, label='Decision threshold')\n",
        "ax8.set_xlabel('Predicted Probability (AI)', fontweight='bold')\n",
        "ax8.set_ylabel('Count', fontweight='bold')\n",
        "ax8.set_title('Prediction Confidence Distribution', fontweight='bold', fontsize=11)\n",
        "ax8.legend()\n",
        "ax8.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 9. Parameter sensitivity: n_estimators\n",
        "ax9 = plt.subplot(3, 4, 9)\n",
        "n_est_analysis = df_results[df_results['name'].str.contains('baseline|many_trees|very_many|few')]\n",
        "ax9.scatter([p['n_estimators'] for p in n_est_analysis['params']],\n",
        "           n_est_analysis['f1_mean'], s=100, alpha=0.7, color='steelblue')\n",
        "ax9.set_xlabel('n_estimators', fontweight='bold')\n",
        "ax9.set_ylabel('F1 Score', fontweight='bold')\n",
        "ax9.set_title('Impact of Tree Count', fontweight='bold', fontsize=11)\n",
        "ax9.grid(True, alpha=0.3)\n",
        "\n",
        "# 10. Parameter sensitivity: max_depth\n",
        "ax10 = plt.subplot(3, 4, 10)\n",
        "depth_configs = df_results[df_results['name'].str.contains('shallow|deep|baseline')]\n",
        "depths = [p['max_depth'] for p in depth_configs['params']]\n",
        "ax10.scatter(depths, depth_configs['f1_mean'], s=100, alpha=0.7, color='darkgreen')\n",
        "ax10.set_xlabel('max_depth', fontweight='bold')\n",
        "ax10.set_ylabel('F1 Score', fontweight='bold')\n",
        "ax10.set_title('Impact of Tree Depth', fontweight='bold', fontsize=11)\n",
        "ax10.grid(True, alpha=0.3)\n",
        "\n",
        "# 11. Error rates by class\n",
        "ax11 = plt.subplot(3, 4, 11)\n",
        "error_data = {\n",
        "    'Human‚ÜíAI\\n(False Pos)': human_as_ai / total_human * 100,\n",
        "    'AI‚ÜíHuman\\n(False Neg)': ai_as_human / total_ai * 100\n",
        "}\n",
        "bars = ax11.bar(error_data.keys(), error_data.values(),\n",
        "               color=['lightcoral', 'coral'], alpha=0.7, edgecolor='black', linewidth=2)\n",
        "ax11.set_ylabel('Error Rate (%)', fontweight='bold')\n",
        "ax11.set_title('Error Analysis by Class', fontweight='bold', fontsize=11)\n",
        "ax11.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    ax11.text(bar.get_x() + bar.get_width()/2., height,\n",
        "             f'{height:.2f}%', ha='center', va='bottom',\n",
        "             fontweight='bold', fontsize=11)\n",
        "\n",
        "# 12. Training time vs performance\n",
        "ax12 = plt.subplot(3, 4, 12)\n",
        "times = [r['time_seconds'] for r in results]\n",
        "f1s = [r['f1_mean'] for r in results]\n",
        "scatter = ax12.scatter(times, f1s, s=100, alpha=0.6,\n",
        "                      c=f1s, cmap='RdYlGn', edgecolors='black', linewidth=0.5)\n",
        "ax12.scatter(winner['time_seconds'], winner['f1_mean'],\n",
        "            s=300, marker='*', color='gold', edgecolors='black',\n",
        "            linewidth=2, label='Winner', zorder=5)\n",
        "ax12.set_xlabel('Training Time (seconds)', fontweight='bold')\n",
        "ax12.set_ylabel('F1 Score', fontweight='bold')\n",
        "ax12.set_title('Efficiency: Time vs Performance', fontweight='bold', fontsize=11)\n",
        "ax12.legend()\n",
        "ax12.grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=ax12, label='F1 Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xgboost_comprehensive_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n‚úì Comprehensive visualization saved as 'xgboost_comprehensive_analysis.png'\")"
      ],
      "metadata": {
        "id": "qb1UUVBM4dAD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}