{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "# Import the modules themselves for version checking\n",
    "import xgboost\n",
    "\n",
    "# Import the classifier classes\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"XGBoost version: {xgboost.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3862b73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone_features = [\n",
    "    'trigram_diversity',\n",
    "    'yules_k',\n",
    "    'comma_ratio',\n",
    "    'colon_ratio',\n",
    "    'pos_ratio_NUM',\n",
    "    'verbs_per_100_tok',\n",
    "    'sentence_length_std',\n",
    "    'n_sentences_doc',\n",
    "    'exclamation_ratio',\n",
    "    'token_burstiness'\n",
    "]\n",
    "\n",
    "# Original expected_features with 9 features removed that don't exist\n",
    "expected_features = [\n",
    "    # 'smog_index', 'automated_readability_index',  # REMOVED - not in df\n",
    "    # 'unigram_diversity',  # REMOVED - not in df\n",
    "    'trigram_diversity',\n",
    "    # 'hapax_type_ratio',  # REMOVED - not in df\n",
    "    'yules_k',\n",
    "    # 'mtld',  # REMOVED - not in df\n",
    "    # 'trigram_entropy',  # REMOVED - not in df\n",
    "    'token_burstiness', 'char_trigram_entropy',\n",
    "    'avg_tree_depth', 'max_tree_depth', 'avg_dependency_distance',\n",
    "    # 'right_dependency_ratio',  # REMOVED - not in df\n",
    "    'uppercase_ratio', 'whitespace_ratio', 'unique_char_count',\n",
    "    # 'bits_per_char',  # REMOVED - not in df\n",
    "    'comma_ratio', 'period_ratio', 'question_ratio', 'exclamation_ratio',\n",
    "    'semicolon_ratio', 'colon_ratio', 'quote_ratio',\n",
    "    'sentiment_polarity', 'sentiment_subjectivity', 'sentiment_polarity_variance',\n",
    "    'neutral_sentence_ratio', 'positive_word_ratio', 'negative_word_ratio',\n",
    "    'pos_ratio_DET', 'pos_ratio_ADP', 'pos_ratio_AUX', 'pos_ratio_CCONJ',\n",
    "    'pos_ratio_PART', 'pos_ratio_NUM', 'pos_row_entropy_weighted',\n",
    "    'function_to_content_rate', 'noun_verb_alternation_rate', 'content_function_ratio',\n",
    "    'noun_verb_ratio', 'adj_adv_ratio', 'verbs_per_100_tok', 'nouns_per_100_tok',\n",
    "    'adj_per_100_tok', 'adv_per_100_tok', 'pron_per_100_tok', 'punct_per_100_tok',\n",
    "    'tokens_per_sentence_mean', 'mean_nouns_per_sent', 'mean_verbs_per_sent',\n",
    "    'mean_adjs_per_sent', 'mean_advs_per_sent', 'prop_sents_with_verb',\n",
    "    'unique_upos_per_sent_mean', 'max_runlen_NOUN', 'max_runlen_PUNCT',\n",
    "    'avg_sentence_length', 'sentence_length_std',\n",
    "    # 'n_tokens_doc',  # REMOVED - not in df\n",
    "    'n_sentences_doc'\n",
    "]\n",
    "\n",
    "# Verify all backbone features exist in expected_features\n",
    "missing = set(backbone_features) - set(expected_features)\n",
    "if missing:\n",
    "    print(f\"WARNING: {missing} not in full feature set\")\n",
    "else:\n",
    "    print(f\"✓ All {len(backbone_features)} backbone features found in full feature set\")\n",
    "    print(f\"✓ Full feature set contains {len(expected_features)} features\")\n",
    "    print(f\"\\nFeature sets defined:\")\n",
    "    print(f\"  - Backbone: 10 features (84.8% reduction)\")\n",
    "    print(f\"  - Full: 53 features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99e99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Replace this with your actual data loading\n",
    "# Example:\n",
    "df = pd.read_csv(r'C:\\Users\\marco\\OneDrive\\Desktop\\Tesi_Codice\\Cognitive_TaskGen\\backbone creation\\raid_sample_large_PostPOS_CLEAN.csv')\n",
    "# OR if you're loading from previous notebook variables:\n",
    "# Assuming df already exists with your features and 'is_ai' column\n",
    "\n",
    "# Verify data\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Class distribution:\\n{df['is_ai'].value_counts()}\")\n",
    "print(f\"\\nFeature columns available: {len(df.columns)}\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = df[expected_features].isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"\\nWARNING: Missing values detected:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(\"\\n✓ No missing values in feature set\")\n",
    "\n",
    "# Prepare data matrices\n",
    "X_backbone = df[backbone_features].values\n",
    "X_full = df[expected_features].values\n",
    "y = df['is_ai'].values  # CHANGED FROM 'label' to 'is_ai'\n",
    "\n",
    "print(f\"\\nData prepared:\")\n",
    "print(f\"  Backbone features: {X_backbone.shape}\")\n",
    "print(f\"  Full features: {X_full.shape}\")\n",
    "print(f\"  Labels: {y.shape}\")\n",
    "print(f\"  Class distribution: AI={y.sum()}, Human={len(y)-y.sum()}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
