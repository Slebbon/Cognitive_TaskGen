{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1af94c95",
   "metadata": {},
   "source": [
    " ###  **Importing** + Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323edc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Configuration ─────────────────────────────────────────────────────────\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(notebook_dir, \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from utils.baseline_utils import (\n",
    "    load_data,\n",
    "    process_binary_labels,\n",
    "    process_base_ablation_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80cf5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── If processing TRAIN_FILE ─────────────────────────────────────────────────────────\n",
    "\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "df = train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b188a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── If processing TEST_FILE ─────────────────────────────────────────────────────────\n",
    "train_df, test_df = load_data()\n",
    "\n",
    "df = test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67ec033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import spacy\n",
    "import textstat\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1a223a",
   "metadata": {},
   "source": [
    "### Baseline **Text Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab1fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract features\n",
    "def extract_text_features(text):\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    stopwords = [token for token in doc if token.is_stop]\n",
    "    punctuations = [token for token in doc if token.is_punct]\n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    total_words = len(words)\n",
    "    total_sentences = len(sentences)\n",
    "    \n",
    "    features = {\n",
    "        \"avg_word_length\": sum(len(word) for word in words) / total_words if total_words else 0,\n",
    "        \"type_token_ratio\": len(set(words)) / total_words if total_words else 0,\n",
    "        \"stopword_ratio\": len(stopwords) / len(doc) if len(doc) else 0,\n",
    "        \"punctuation_ratio\": len(punctuations) / len(doc) if len(doc) else 0,\n",
    "        \"avg_sentence_length\": total_words / total_sentences if total_sentences else 0,\n",
    "        \"sentence_length_std\": pd.Series([len(sent) for sent in sentences]).std() if total_sentences > 1 else 0,\n",
    "        \n",
    "        # Readability scores (textstat works on raw string)\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "    }\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e94f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure generation column is string\n",
    "df[\"generation\"] = df[\"generation\"].astype(str)\n",
    "\n",
    "# Apply feature extraction\n",
    "features_df = df[\"generation\"].apply(extract_text_features).apply(pd.Series)\n",
    "\n",
    "# Merge with original data\n",
    "df_features = pd.concat([df, features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b6d8c8",
   "metadata": {},
   "source": [
    "### Baseline **POS-Tags** features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdf0163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ POS features added.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# Define POS tags of interest (coarse-grained)\n",
    "POS_TAGS = [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\", \"PRON\", \"DET\", \"ADP\", \"AUX\", \"CCONJ\", \"PART\", \"NUM\", \"PUNCT\", \"X\"]\n",
    "\n",
    "def extract_pos_features(text):\n",
    "    doc = nlp(text)\n",
    "    total_tokens = len(doc)\n",
    "    \n",
    "    # POS ratios\n",
    "    pos_counts = Counter([token.pos_ for token in doc])\n",
    "    pos_ratios = {f\"pos_ratio_{tag}\": pos_counts.get(tag, 0) / total_tokens if total_tokens else 0 for tag in POS_TAGS}\n",
    "    \n",
    "    # POS bigram transitions\n",
    "    pos_sequence = [token.pos_ for token in doc]\n",
    "    transitions = list(zip(pos_sequence, pos_sequence[1:]))\n",
    "    transition_counts = Counter(transitions)\n",
    "    \n",
    "    # Transition entropy\n",
    "    total_transitions = sum(transition_counts.values())\n",
    "    entropy = -sum(\n",
    "        (count / total_transitions) * math.log2(count / total_transitions)\n",
    "        for count in transition_counts.values()\n",
    "    ) if total_transitions else 0\n",
    "    \n",
    "    pos_ratios[\"pos_transition_entropy\"] = entropy\n",
    "    \n",
    "    return pos_ratios\n",
    "\n",
    "# Apply POS features\n",
    "pos_df = df[\"generation\"].apply(extract_pos_features).apply(pd.Series)\n",
    "df_pos = pd.concat([df, pos_df], axis=1)\n",
    "\n",
    "print(\"✅ POS features added.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ee42e",
   "metadata": {},
   "source": [
    "### **Perplexity**  extraction w/ GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9633e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load pretrained GPT-2\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def compute_perplexity(text, max_length=512):\n",
    "    try:\n",
    "        # Tokenize and manually move tensors to the target device\n",
    "        encodings = tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=max_length\n",
    "        )\n",
    "        encodings = {k: v.to(device) for k, v in encodings.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        return torch.exp(loss).item()\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8183542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "100%|██████████| 2000/2000 [18:50<00:00,  1.77it/s]\n"
     ]
    }
   ],
   "source": [
    "df_pos[\"gpt2_perplexity\"] = df_pos[\"generation\"].progress_apply(compute_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e9d0c",
   "metadata": {},
   "source": [
    "### **Sentence Variance** + **Repetition** + **Burstiness** (Prototype) extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7791e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sentence variance features added.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sentence_variance_features(text):\n",
    "    doc = nlp(text)\n",
    "    sentence_lengths = [len([t for t in sent if not t.is_punct]) for sent in doc.sents]\n",
    "    \n",
    "    n = len(sentence_lengths)\n",
    "    total_words = sum(sentence_lengths)\n",
    "    \n",
    "    avg_len = np.mean(sentence_lengths) if n else 0\n",
    "    std_len = np.std(sentence_lengths) if n else 0\n",
    "\n",
    "    # Normalize for entropy\n",
    "    if total_words > 0:\n",
    "        probs = [l / total_words for l in sentence_lengths if l > 0]\n",
    "        entropy = -sum(p * math.log2(p) for p in probs)\n",
    "    else:\n",
    "        entropy = 0\n",
    "\n",
    "    return {\n",
    "        \"avg_sentence_length_v2\": avg_len,\n",
    "        \"sentence_length_std\": std_len,\n",
    "        \"sentence_length_entropy\": entropy\n",
    "    }\n",
    "\n",
    "sentence_df = df_pos[\"generation\"].apply(sentence_variance_features).apply(pd.Series)\n",
    "df_pos = pd.concat([df_pos, sentence_df], axis=1)\n",
    "\n",
    "print(\"✅ Sentence variance features added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fabda163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Repetition features added.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def repetition_features(text):\n",
    "    doc = nlp(text.lower())  # Lowercased for n-gram counting\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "\n",
    "    ngram_features = {}\n",
    "    for n in [2, 3]:\n",
    "        ngrams = list(zip(*[words[i:] for i in range(n)]))\n",
    "        counts = Counter(ngrams)\n",
    "        total = len(ngrams)\n",
    "        repeats = sum(1 for v in counts.values() if v > 1)\n",
    "\n",
    "        ngram_features[f\"repeated_{n}gram_ratio\"] = repeats / total if total else 0\n",
    "        ngram_features[f\"unique_{n}grams\"] = len(counts)\n",
    "\n",
    "    return ngram_features\n",
    "\n",
    "repetition_df = df_pos[\"generation\"].apply(repetition_features).apply(pd.Series)\n",
    "df_pos = pd.concat([df_pos, repetition_df], axis=1)\n",
    "\n",
    "print(\"✅ Repetition features added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05498ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Burstiness features added.\n"
     ]
    }
   ],
   "source": [
    "def lexical_burstiness(text):\n",
    "    doc = nlp(text.lower())\n",
    "    words = [token.text for token in doc if token.is_alpha]\n",
    "    counts = Counter(words)\n",
    "    \n",
    "    freqs = np.array(list(counts.values()))\n",
    "    return {\n",
    "        \"burstiness_token_var\": np.var(freqs) if len(freqs) > 1 else 0,\n",
    "        \"burstiness_token_std\": np.std(freqs) if len(freqs) > 1 else 0\n",
    "    }\n",
    "\n",
    "burstiness_df = df_pos[\"generation\"].apply(lexical_burstiness).apply(pd.Series)\n",
    "df_pos = pd.concat([df_pos, burstiness_df], axis=1)\n",
    "\n",
    "print(\"✅ Burstiness features added.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d5a524e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined DataFrame shape: (2000, 45)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Find overlapping columns (excluding the index)\n",
    "overlap_cols = df_features.columns.intersection(df_pos.columns)\n",
    "\n",
    "# Step 2: Drop those columns from df_pos\n",
    "df_pos_cleaned = df_pos.drop(columns=overlap_cols)\n",
    "\n",
    "# Step 3: Concatenate horizontally\n",
    "df_all = pd.concat([df_features, df_pos_cleaned], axis=1)\n",
    "\n",
    "print(f\"✅ Combined DataFrame shape: {df_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cd7cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to test_sample_baseline.csv\n"
     ]
    }
   ],
   "source": [
    "output_path = \"test_sample_baseline.csv\"\n",
    "df_all.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
